{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to web-scraping\n",
    "\n",
    "It's 2021. The web is everywhere.\n",
    "\n",
    "* If you want to buy a house, real estate agents have [websites](https://www.wendytlouie.com/) where they list the houses they're currently selling. \n",
    "* If you want to know whether to where a rain jacket or shorts, you check the weather on a [website](https://weather.com/weather/tenday/l/Berkeley+CA+USCA0087:1:US). \n",
    "* If you want to know what's happening in the world, you read the news [online](https://www.sfchronicle.com/). \n",
    "* If you've forgotten which city is the capital of Australia, you check [Wikipedia](https://en.wikipedia.org/wiki/Australia).\n",
    "\n",
    "**The point is this: there is an enormous amount of information (also known as data) on the web.**\n",
    "\n",
    "If we (in our capacities as, for example, data scientists, social scientists, digital humanists, businesses, public servants or members of the public) can get our hands on this information, **we can answer all sorts of interesting questions or solve important problems**.\n",
    "\n",
    "* Maybe you're studying gender bias in student evaluations of professors. One option would be to scrape ratings from [Rate My Professors](https://www.ratemyprofessors.com/) (provided you follow their [terms of service](https://www.ratemyprofessors.com/TermsOfUse_us.jsp#use))\n",
    "* Perhaps you want to build an app that shows users articles relating to their specified interests. You could scrape stories from various news websites and then use NLP methods to decide which articles to show which users.\n",
    "* [Geoff Boeing](https://geoffboeing.com/) and [Paul Waddell](https://ced.berkeley.edu/ced/faculty-staff/paul-waddell) recently published [a great study](https://arxiv.org/pdf/1605.05397.pdf) of the US housing market by scraping millions of Craiglist rental listings. Among other insights, their study shows which metropolitan areas in the US are more or less affordable to renters.\n",
    "\n",
    "This first day's workshop is a one-hour beginner's introduction to web scraping. \n",
    "\n",
    "\n",
    "## Learning Goals\n",
    "*   \n",
    "\n",
    "## Outline\n",
    "\n",
    "* [Structured queries with APIs](#apis)\n",
    "* [URL collection with automated Google search](#URLs)\n",
    "* [Mirroring websites with `wget`](#wget)\n",
    "* [Template code: See Google Places API in action](#Places)\n",
    "\n",
    "## Background\n",
    "\n",
    "We will do some review, but this notebook assumes you have basic familiarity with Python. If you need a beginner's introduction to coding in Python, please walk through the intro to Python notebook at `extra/intro-to-python.ipynb` and/or [this one](https://github.com/lknelson/text-analysis-course/blob/master/scripts/01.25.02_PythonBasics.ipynb) *before* the workshop. \n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "* *Uniform Resource Locator (URL)*: \n",
    "    * The address of information on the web and directions to get there. A URL points to resources--usually the files needed to show a website, but it can also point to files and such.\n",
    "* *Domain name*:\n",
    "    * A website identifier that begins a URL: for instance, in https://www.example.com/ this is everything from `https` to `.com/`.\n",
    "* *web-scraping* (i.e., *screen-scraping*):\n",
    "    * Extracting structured information from the files that make up websites (i.e., what's shown in web browsers), relying on their HTML, CSS, and sometimes JS files. \n",
    "* *Hyper-Text Markup Language (HTML)*: \n",
    "    * The standard markup language for websites, the \"nuts and bolts\" of WHAT a website will display, including text.\n",
    "* *Cascading Style Sheets (CSS)*: \n",
    "    * A technology used to format the layout of a webpage, i.e. HOW to make it pretty. Not usually relevant for web-scraping.\n",
    "* *web-crawling*:\n",
    "    * Finding web pages through links, automated search, etc. Once discovered, pages can be checked (is this website still up?), downloaded, or scraped. \n",
    "* *website mirroring*:\n",
    "    * Creating a complete local copy of the files needed to display and host a website. \n",
    "* *Application Programming Interface (API)*:\n",
    "    * A tool used to access structured data provided by an organization. Examples include Twitter, Reddit, Wikipedia, and the New York Times. When an API is available (not always the case), this is usually the preferred way to access data (over web-scraping).\n",
    "\n",
    "**__________________________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured queries with APIs<a id='apis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's try out the [Google Fact Check API](https://developers.google.com/fact-check/tools/api/), which can be easily explored [in a browser](https://toolbox.google.com/factcheck/explorer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import requests # for downloading\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag # for html scraping\n",
    "import regex as re # Regex module with Unicode support\n",
    "import html5lib # slower but more accurate bs4 parser for messy HTML # lxml faster\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "# Import functions to scrape fact check web pages\n",
    "from scrape_helpers import load_api_key, clean_text, scrape_politifact, scrape_factcheck, scrape_snopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Call API\n",
    "######################################################\n",
    "\n",
    "# Elements in query response: text, claimDate, claimReview[publisher[name], url, textualRating]\n",
    "# Columns in output CSV: (date (DD-MM-YYYY), claim, truth rating, url, source (publisher), fact, explanation\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    page_token = 0\n",
    "    domains = ['covid', 'blm', 'election']\n",
    "    query_sets = [\n",
    "        [\"masks\", \"Chinese bioweapon\", \"China virus\"],\n",
    "        [\"George Floyd\", \"Antifa\", \"Black Lives Matter\"],\n",
    "        [\"Hunter Biden\", \"rigged election\", \"mail-in\", \"election ballots\"],\n",
    "    ]\n",
    "\n",
    "    api_key_fp = \"api_key.txt\"\n",
    "    key = load_api_key(api_key_fp)\n",
    "    endpoint = 'https://factchecktools.googleapis.com'\n",
    "    search = '/v1alpha1/claims:search'\n",
    "\n",
    "    sites = ['politifact.com', 'factcheck.org', 'snopes.com']\n",
    "    site_scrapers = [scrape_politifact, scrape_factcheck, scrape_snopes]\n",
    "    site_switches = ['politifact', 'factcheck.org', 'snopes']\n",
    "\n",
    "\n",
    "    for i in range(0, len(domains)):\n",
    "        domain = domains[i]\n",
    "        queries = query_sets[i]\n",
    "        claims = [] # initialize list of claims\n",
    "\n",
    "        for query in queries:\n",
    "            urls = set() # initialize set of fact check URLs already seen for this query\n",
    "\n",
    "            for site in tqdm(sites, desc='Collecting data for {} via API'.format(query)):\n",
    "                params = {\n",
    "                    'pageToken': page_token,\n",
    "                    'query': query,\n",
    "                    'reviewPublisherSiteFilter': site,\n",
    "                    'key': key\n",
    "                }\n",
    "\n",
    "                nextToken = True\n",
    "                while nextToken:\n",
    "                    url = endpoint + search + '?' + urllib.parse.urlencode(params)\n",
    "                    response = requests.get(url)\n",
    "                    data = response.json()\n",
    "\n",
    "                    if 'claims' in data:\n",
    "                        for claim in data['claims']:\n",
    "                            if not site == 'snopes.com':\n",
    "                                claims.append([claim['claimDate'],\n",
    "                                               claim['text'],\n",
    "                                               claim['claimReview'][0]['textualRating'],\n",
    "                                               claim['claimReview'][0]['url'],\n",
    "                                               claim['claimReview'][0]['publisher']['name']])\n",
    "                            else:\n",
    "                                claims.append([claim['claimReview'][0]['reviewDate'],\n",
    "                                               claim['text'],\n",
    "                                               claim['claimReview'][0]['textualRating'],\n",
    "                                               claim['claimReview'][0]['url'],\n",
    "                                               claim['claimReview'][0]['publisher']['name']\n",
    "                                              .replace('.com', '')])\n",
    "\n",
    "                    if 'nextPageToken' in data:\n",
    "                        params['pageToken'] = data['nextPageToken']\n",
    "                    else:\n",
    "                        nextToken = False\n",
    "\n",
    "            for j in tqdm(range(0, len(claims)), desc='Scraping websites'.format(query)):\n",
    "                claim = claims[j]\n",
    "                switch = site_switches.index(claim[4].lower()) # use fact to get publisher site then index (site name is 5th element)\n",
    "                scraper = site_scrapers[switch] # get scraper using index\n",
    "                claim.extend(scraper(claim[3])) # scrape URL using scraper (URL is 4th element), add to existing claim info\n",
    "                claims[j] = claim # record fact in list\n",
    "\n",
    "            claims # remove duplicates\n",
    "\n",
    "            # Save output for this query\n",
    "            query_string = query.replace(' ', '-')\n",
    "            with open('data/fact_checker_data_{}.csv'.format(query_string), 'w') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(['date', 'claim', 'truth_rating', 'url', 'source', 'fact', 'explanation'])\n",
    "                for claim in claims: # Each claim gets its own column\n",
    "                    if claim[3] not in urls: # don't add if fact check URL already seen\n",
    "                        csv_writer.writerow(claim) # save row\n",
    "                        urls.add(claim[3]) # add to set of urls already saved\n",
    "\n",
    "            print('Saved {} claims for {} query.'.format(str(len(urls)), query))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL collection with automated Google search<a id='URLs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to crawl and/or scrape an online community of websites, there's a good chance may find yourself needing to collect their URLs. If you're lucky, you have comprehensive metadata describing these entities, something like their name and physical address. Your next step in this scenario would be to automate a Google search to collect the best URL matching each entity. \n",
    "\n",
    "How can you scrape URLs from Google? There are two fairly easy ways.\n",
    "\n",
    "First, the **Google Places API**, which is the best option to do this at scale. You would need to apply for an API key from Google: go to the [Google cloud console](https://console.cloud.google.com/), create a project, and request an API key for each service you want to use. Approval may take a few days, but once done there is a [handy Python wrapper](https://github.com/slimkrazy/python-google-places) to make this easy to use in Python. See [Google Web Services](https://developers.google.com/places/web-service/) for general documentation and [Google Developers](https://developers.google.com/places/web-service/details) for details on Place Details requests.\n",
    "\n",
    "Be aware that Google APIs are not a free service, and they may not work at all unless you sign up for billing. However, if you apply for access under an education account or for research purposes, Google offers you credit to start with (200 dollars last I checked). Nonetheless, to avoid excessive charges (I have experience with this!), check what exact requests you're making and set up account alerts before making API calls at scale.\n",
    "\n",
    "The second option is **automated Google search**, which is not nearly as reliable and may get you blocked if used repeatedly. This method tends to get lots of false positives and third-party website aggregators (e.g., yellowpages.com, trulia.com), so using a blacklist to manually filter results is a good idea. Check out [the source code](https://github.com/MarioVilas/googlesearch) and [documentation](https://python-googlesearch.readthedocs.io/en/latest/). _Thanks Mario Vilas for this package!_\n",
    "\n",
    "Because this second option is free and has no waiting period to use, we will practice using this in a nice way. You can see template code for running the Google Places API at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping school URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this works, let's start by searching for the best URL for a charter school in Washington, D.C. Assume we have the name and address of the school.\n",
    "\n",
    "To prevent overwhelming Google search with rapid requests--and likely getting our IP address blocked by Google as a result--let's search only for the first 10 results and include a five-second pause in between each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ccpcs.org/\n",
      "https://www.ccpcs.org/current-families/calendar\n",
      "https://www.ccpcs.org/about/mission-and-history\n",
      "https://www.ccpcs.org/admissions/applying-capital-city\n",
      "https://www.ccpcs.org/program/el-education\n",
      "https://www.ccpcs.org/about/our-staff\n",
      "https://www.facebook.com/CapitalCityPCS/\n",
      "https://www.greatschools.org/washington-dc/washington/282-Capital-City-PCS---Lower-School/\n",
      "https://www.greatschools.org/washington-dc/washington/591-Capital-City-High-School-PCS/\n",
      "https://www.greatschools.org/washington-dc/washington/591-Capital-City-High-School-PCS/#College_readiness\n"
     ]
    }
   ],
   "source": [
    "# Import automated Google search package\n",
    "from googlesearch import search\n",
    "\n",
    "# Define metadata for a single entity: a DC charter school\n",
    "school_name = 'Capital City Public Charter School'\n",
    "school_address = '100 Peabody Street NW, Washington, DC 20011'\n",
    "\n",
    "# Search for first 10 Google results using joined metadata, show each one\n",
    "for url in search(school_name + ' ' + school_address, \\\n",
    "                  stop=10, pause=5.0):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty strong result: the first six matches share the domain of https://www.ccpcs.org/, so this is probably the best match. We identified a URL without even visiting any websites!\n",
    "\n",
    "Notice that results 7-10 are about the right school, but they don't point to it's genuine website--with all its descriptive language, images, and subpages. Even in this case with a strong topline result, we can already get a feel for what websites will pollute our automated searches: Facebook and greatschools.org are a good start to making a blacklist to filter the results. \n",
    "\n",
    "Now let's try something harder to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the first 10 results from Google for Dr. David C. Walker Intermediate School located at 6500 Ih 35 N Ste C, San Antonio, TX 78218. What do you notice about the results? How do they compare to the previous set of results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.niche.com/k12/dr-david-c-walker-intermediate-school-san-antonio-tx/\n",
      "https://www.har.com/school/015806106/dr-david-c-walker-elementary-school\n",
      "https://www.greatschools.org/texas/san-antonio/12035-Dr-David-C-Walker-Intermediate-School/\n",
      "https://www.usnews.com/education/k12/texas/dr-david-c-walker-el-206298\n",
      "https://www.publicschoolreview.com/dr-david-c-walker-elementary-school-profile\n",
      "https://nces.ed.gov/ccd/schoolsearch/school_detail.asp?ID=480006211404\n",
      "https://www.dnb.com/business-directory/company-profiles.school_of_excellence_in_education.8fde8b90005cb3de714dd31c0d8e98f4.html\n",
      "https://www.schooldigger.com/go/TX/schools/0006211404/school.aspx\n",
      "https://elementaryschools.org/directory/tx/cities/san-antonio/dr-david-c-walker-elementary/480006211404/\n",
      "https://closelocation.com/find-school/dr-david-c-walker-elementary-school-school-in-basse-basse-16-11508-1216-80\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "# Define metadata\n",
    "school_name = 'Dr. David C. Walker Intermediate School'\n",
    "school_address = '6500 Ih 35 N Ste C, San Antonio, TX 78218'\n",
    "\n",
    "# Automated search\n",
    "for url in search(school_name + ' ' + school_address, \\\n",
    "                  stop=10, pause=5.0):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are much less clear and organized: Each one points to a different site, and all of them are third parties. Interestingly, the [first result](https://www.niche.com/k12/dr-david-c-walker-intermediate-school-san-antonio-tx/) (with domain of https://www.niche.com) does point to the [official website](https://excellence-sa.org/walker/), but extracting this information systematically would mean web-scraping--which we will get to tomorrow! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping URLs using a blacklist\n",
    "\n",
    "To provide cleaner search results, let's filter out the third-party websites from the previous two examples. \n",
    "\n",
    "Many of these websites can show up with either 'http' or 'https', often with or without a 'www', but usually have a consistent top-level domain (e.g., 'com'). Exact string matchin would fail to capture matches across these variations. Regular expressions could do this, but for now let's just filter out those search results that contain the core of any blacklisted domain name (e.g., niche.com). \n",
    "\n",
    "Let's get the first result for the previous school (Dr. David C. Walker Intermediate School) that doesn't match any blacklisted domains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully collected Google search results.\n",
      "Bad site detected: https://www.niche.com/k12/dr-david-c-walker-intermediate-school-san-antonio-tx/\n",
      "Bad site detected: https://www.har.com/school/015806106/dr-david-c-walker-elementary-school\n",
      "Bad site detected: https://www.greatschools.org/texas/san-antonio/12035-Dr-David-C-Walker-Intermediate-School/\n",
      "Bad site detected: https://www.usnews.com/education/k12/texas/dr-david-c-walker-el-206298\n",
      "Bad site detected: https://www.publicschoolreview.com/dr-david-c-walker-elementary-school-profile\n",
      "Bad site detected: https://nces.ed.gov/ccd/schoolsearch/school_detail.asp?ID=480006211404\n",
      "Bad site detected: https://www.dnb.com/business-directory/company-profiles.school_of_excellence_in_education.8fde8b90005cb3de714dd31c0d8e98f4.html\n",
      "Bad site detected: https://www.schooldigger.com/go/TX/schools/0006211404/school.aspx\n",
      "Bad site detected: https://elementaryschools.org/directory/tx/cities/san-antonio/dr-david-c-walker-elementary/480006211404/\n",
      "Bad site detected: https://closelocation.com/find-school/dr-david-c-walker-elementary-school-school-in-basse-basse-16-11508-1216-80\n",
      "Success! URL obtained by Google search with 10 bad URLs avoided.\n",
      "Quality URL: http://castro.tea.state.tx.us/charter_apps/content/downloads/Renewals/015806_2.pdf\n"
     ]
    }
   ],
   "source": [
    "# Define blacklisted domains to filter out: third-party domains/false positives that we DON'T want to scrape \n",
    "blacklist = ['facebook.com', 'greatschools.org', 'niche.com', 'har.com', 'usnews.com', 'publicschoolreview.com', \n",
    "             'nces.ed.gov', 'dnb.com', 'schooldigger.com', 'elementaryschools.org', 'closelocation.com']\n",
    "\n",
    "# Define search metadata\n",
    "school_name = 'Dr. David C. Walker Intermediate School'\n",
    "school_address = '6500 Ih 35 N Ste C, San Antonio, TX 78218'\n",
    "#school_name = \"River City Scholars Charter Academy\"\n",
    "#school_address = \"944 Evergreen Street, Grand Rapids, MI 49507\"\n",
    "\n",
    "# Collect search results\n",
    "urls = search(school_name + ' ' + school_address, \\\n",
    "              stop=20, pause=5.0) # Expand search range to help avoid blacklisted domains\n",
    "print(\"Successfully collected Google search results.\")\n",
    "\n",
    "# Initialize blacklist match counter: How many blacklisted domains has this search encountered?\n",
    "blacklisted_num = 0 \n",
    "\n",
    "# Loop through google search output to find first good result:\n",
    "for url in urls:\n",
    "    if any(domain in url for domain in blacklist):\n",
    "        print(f'Bad site detected: {url}') \n",
    "        blacklisted_num += 1 # Add one to blacklist match counter\n",
    "    else:\n",
    "        good_url = url\n",
    "        print(\"Success! URL obtained by Google search with \" + str(blacklisted_num) + \" bad URLs avoided.\")\n",
    "        break # Exit for loop after first good url is found\n",
    "        \n",
    "print(f'Quality URL: {good_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of [the \"quality\" URL we landed on](http://castro.tea.state.tx.us/charter_apps/content/downloads/Renewals/015806_2.pdf)? Looks like we need to expand our blacklist!\n",
    "\n",
    "### Challenge\n",
    "\n",
    "Improve our automated searching to get the genuine URL of Dr. David C. Walker Intermediate School. <br/>\n",
    "_Hint_: You could try (A) adding more URLs to the blacklist OR (B) try a simple search but for more URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully collected Google search results.\n",
      "Bad site detected: https://www.niche.com/k12/dr-david-c-walker-intermediate-school-san-antonio-tx/\n",
      "Bad site detected: https://www.har.com/school/015806106/dr-david-c-walker-elementary-school\n",
      "Bad site detected: https://www.greatschools.org/texas/san-antonio/12035-Dr-David-C-Walker-Intermediate-School/\n",
      "Bad site detected: https://www.usnews.com/education/k12/texas/dr-david-c-walker-el-206298\n",
      "Bad site detected: https://www.publicschoolreview.com/dr-david-c-walker-elementary-school-profile\n",
      "Bad site detected: https://nces.ed.gov/ccd/schoolsearch/school_detail.asp?ID=480006211404\n",
      "Bad site detected: https://www.dnb.com/business-directory/company-profiles.school_of_excellence_in_education.8fde8b90005cb3de714dd31c0d8e98f4.html\n",
      "Bad site detected: https://www.schooldigger.com/go/TX/schools/0006211404/school.aspx\n",
      "Bad site detected: https://elementaryschools.org/directory/tx/cities/san-antonio/dr-david-c-walker-elementary/480006211404/\n",
      "Bad site detected: https://closelocation.com/find-school/dr-david-c-walker-elementary-school-school-in-basse-basse-16-11508-1216-80\n",
      "Bad site detected: http://castro.tea.state.tx.us/charter_apps/content/downloads/Renewals/015806_2.pdf\n",
      "Success! URL obtained by Google search with 11 bad URLs avoided.\n",
      "Quality URL: https://excellence-sa.org/walker/\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "\n",
    "# Option A: Define expanded blacklist\n",
    "blacklist = ['facebook.com', 'greatschools.org', 'niche.com', 'har.com', 'usnews.com', 'publicschoolreview.com', \n",
    "             'nces.ed.gov', 'dnb.com', 'schooldigger.com', 'elementaryschools.org', 'closelocation.com', \n",
    "             'castro.tea.state.tx.us']\n",
    "\n",
    "# Collect search results\n",
    "urls = search(school_name + ' ' + school_address, \\\n",
    "              stop=20, pause=5.0) # Expand search range to help avoid blacklisted domains\n",
    "print(\"Successfully collected Google search results.\")\n",
    "\n",
    "# Initialize blacklist match counter\n",
    "blacklisted_num = 0 \n",
    "\n",
    "# Get first good search result:\n",
    "for url in urls:\n",
    "    if any(domain in url for domain in blacklist):\n",
    "        print(f'Bad site detected: {url}') \n",
    "        blacklisted_num += 1 # Add one to blacklist match counter\n",
    "    else:\n",
    "        good_url = url\n",
    "        print(\"Success! URL obtained by Google search with \" + str(blacklisted_num) + \" bad URLs avoided.\")\n",
    "        break # Exit for loop after first good url is found\n",
    "        \n",
    "print(f'Quality URL: {good_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.niche.com/k12/dr-david-c-walker-intermediate-school-san-antonio-tx/\n",
      "https://www.har.com/school/015806106/dr-david-c-walker-elementary-school\n",
      "https://www.greatschools.org/texas/san-antonio/12035-Dr-David-C-Walker-Intermediate-School/\n",
      "https://www.usnews.com/education/k12/texas/dr-david-c-walker-el-206298\n",
      "https://www.publicschoolreview.com/dr-david-c-walker-elementary-school-profile\n",
      "https://nces.ed.gov/ccd/schoolsearch/school_detail.asp?ID=480006211404\n",
      "https://www.dnb.com/business-directory/company-profiles.school_of_excellence_in_education.8fde8b90005cb3de714dd31c0d8e98f4.html\n",
      "https://www.schooldigger.com/go/TX/schools/0006211404/school.aspx\n",
      "https://elementaryschools.org/directory/tx/cities/san-antonio/dr-david-c-walker-elementary/480006211404/\n",
      "https://closelocation.com/find-school/dr-david-c-walker-elementary-school-school-in-basse-basse-16-11508-1216-80\n",
      "http://castro.tea.state.tx.us/charter_apps/content/downloads/Renewals/015806_2.pdf\n",
      "https://excellence-sa.org/walker/\n",
      "https://www.homesnap.com/schools/TX/San_Antonio/Dr_David_C_Walker_Intermediate_School\n",
      "https://texas.hometownlocator.com/schools/profiles,n,dr%20david%20c%20walker%20el,z,78218,t,pb,i,1115128.cfm\n",
      "https://www.hisawyer.com/listings/schools/126444-dr-david-c-walker-elementary\n",
      "https://rehold.com/San+Antonio+TX/CASTLE+VW/6518\n",
      "https://rehold.com/San+Antonio+TX/CASTLE+VW/4737\n",
      "https://www.diedinhouse.com/sitemap.aspx/234%20ashland%20dr%2C%20san%20antonio%2C%20tx\n",
      "https://www.realtytrac.com/property/tx/san-antonio/78218/7702-n-interstate-35/167927542/\n",
      "https://www.sanantonio.gov/Portals/0/Files/health/News/RestaurantReports/RRAPRIL2017.xlsx?ver=2017-05-12-162100-603\n"
     ]
    }
   ],
   "source": [
    "# Option B: Expanded simple search\n",
    "for url in search(school_name + ' ' + school_address, \\\n",
    "                  stop=20, pause=5.0, num=20): # Get first 20 results: stop at 20, and get 20 in first page of results\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirroring websites with `wget`<a id='wget'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wget` is classic (circa 1996, but still updated) [free software](https://www.gnu.org/philosophy/free-sw) in shell for non-interactively downloading web content. It's often used for basic one-time downloads, like `curl` also does for shell or `urllib.urlretrieve` does in-house for Python. But where `wget` really shines is in its extensive customization, including retrying failed connections, following links, and duplicating a remote website's files and structure to the point of having an identical local copy (website mirroring). \n",
    "\n",
    "Let's try using the nice Python wrapper for `wget` to download the MDI News page nested in the McCourt School for Public Policy site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'download.wget'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget \n",
    "wget.download(url='https://mccourt.georgetown.edu/research/mdi-news/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out the contents of this (rather poorly named) file using the Jupyter interface in the previous tab. \n",
    "\n",
    "We got some HTML--cool! But what if we want something clickable and interactive? This is easiest to do with `wget` run via its native shell, rather than this simple Python wrapper--which also doesn't allow for `get`'s more advanced functionality. We can use the helpful `!` prefix to run shell commands straight from this notebook. \n",
    "\n",
    "Let's make a new `wget` request to download a version of the same page that's easier to see in your browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:43:11--  https://mccourt.georgetown.edu/research/mdi-news/\n",
      "Resolving mccourt.georgetown.edu (mccourt.georgetown.edu)... 23.185.0.1, 2620:12a:8001::1, 2620:12a:8000::1\n",
      "Connecting to mccourt.georgetown.edu (mccourt.georgetown.edu)|23.185.0.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 157499 (154K) [text/html]\n",
      "Saving to: ‘index.html.1’\n",
      "\n",
      "index.html.1        100%[===================>] 153.81K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-04-23 14:43:12 (8.80 MB/s) - ‘index.html.1’ saved [157499/157499]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://mccourt.georgetown.edu/research/mdi-news/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your Jupyter browser to check out the results: just click on `index.html` in your current folder (probably this is `day-1/`) to view the page. What do you notice? How does it compare to viewing https://mccourt.georgetown.edu/research/mdi-news/ in your browser? Try clicking the links. Where can you go on the actual page that your local copy can't show you? Do you have local copies of the images?\n",
    "\n",
    "You might have noticed that we only ended up with some HTML--we didn't download any of the files associated with the webpage. So, this isn't a true copy; we couldn't host the page ourselves, analyze its images, or easily use its content for purposes other than viewing. How do we mirror the full site?\n",
    "\n",
    "To do this, we need only the `page-requisites` option, which makes sure to download all the resources needed to render the page in a browser: that means CSS, javascript, image files, etc. To keep from overloading the server, let's pause for a few seconds in between downloads using the `--wait` option. \n",
    "\n",
    "Let's use some other features as well for politeness and subtlety (i.e. to avoid getting blocked). Here is explanation for all of them:\n",
    "\n",
    "```shell\n",
    "--page-requisites             Grabs all of the linked resources necessary to render the page (images, CSS, javascript, etc.)\n",
    "--wait                        Pauses between downloads (in seconds)\n",
    "--tries=3                     Retries failed downloads 3 times\n",
    "--user-agent=Mozilla          Makes wget look like a Mozilla browser by masking its user agent\n",
    "--header=\"Accept:text/html\"   Sends header with each HTML request, looks more browser-ish\n",
    "--no-check-certificate        Doesnt check authenticity of website server (use only with trusted websites!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:16:52--  https://mccourt.georgetown.edu/research/mdi-news/\n",
      "Resolving mccourt.georgetown.edu (mccourt.georgetown.edu)... 23.185.0.1, 2620:12a:8001::1, 2620:12a:8000::1\n",
      "Connecting to mccourt.georgetown.edu (mccourt.georgetown.edu)|23.185.0.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 157499 (154K) [text/html]\n",
      "Saving to: ‘index.html’\n",
      "\n",
      "index.html          100%[===================>] 153.81K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-04-23 14:16:52 (9.30 MB/s) - ‘index.html’ saved [157499/157499]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --page-requisites --wait=2 --tries=3 --user-agent=Mozilla --header=\"Accept:text/html\" --no-check-certificate \\\n",
    "    https://mccourt.georgetown.edu/research/mdi-news/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the results--what's similar and whats different? See `/research/mdi-news/` for the `index.html` (sometimes this is `default.html`) page we saw earlier. \n",
    "\n",
    "`wget` has a rich array of options. Here are some of the most useful ones in addition to those above:\n",
    "\n",
    "```shell\n",
    "--mirror                      Downloads a full website and makes available for local viewing\n",
    "--recursive                   Recursively downloads files and follows links\n",
    "--no-parent \t\t          Does not follow links above hierarchical level of input URL\n",
    "--convert-links \t          Turns links into local links as appropriate\n",
    "--accept                      Download only file suffixes in this list (e.g., .html)\n",
    "--execute robots=off          Turns off automatic robots.txt checking, preventing server privacy exclusions\n",
    "--random-wait                 Randomizes the defined wait period to between .5 and 1.5x that value\n",
    "--background\t\t          For a huge download, put the download in background\n",
    "--spider                      Determines whether the remote file exist at the destination (mimics web spiders)\n",
    "--domains   \t\t          Downloads only only PDF files from specific domains\n",
    "--user --password   \t\t  Downloads files from password protected sites\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Download only `.html` files from https://mccourt.georgetown.edu/research/ and links below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 15:03:17--  https://mccourt.georgetown.edu/research/\n",
      "Resolving mccourt.georgetown.edu (mccourt.georgetown.edu)... 23.185.0.1, 2620:12a:8000::1, 2620:12a:8001::1\n",
      "Connecting to mccourt.georgetown.edu (mccourt.georgetown.edu)|23.185.0.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 157616 (154K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 153.92K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-04-23 15:03:18 (9.33 MB/s) - ‘mccourt.georgetown.edu/research/index.html’ saved [157616/157616]\n",
      "\n",
      "Loading robots.txt; please ignore errors.\n",
      "--2021-04-23 15:03:20--  https://mccourt.georgetown.edu/robots.txt\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 116 [text/plain]\n",
      "Saving to: ‘mccourt.georgetown.edu/robots.txt.tmp’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>]     116  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 15:03:20 (3.37 MB/s) - ‘mccourt.georgetown.edu/robots.txt.tmp’ saved [116/116]\n",
      "\n",
      "--2021-04-23 15:03:22--  https://mccourt.georgetown.edu/research/featured-publications/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154027 (150K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/featured-publications/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 150.42K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2021-04-23 15:03:22 (17.1 MB/s) - ‘mccourt.georgetown.edu/research/featured-publications/index.html’ saved [154027/154027]\n",
      "\n",
      "--2021-04-23 15:03:24--  https://mccourt.georgetown.edu/research/mccourt-centers/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 145651 (142K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/mccourt-centers/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 142.24K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:25 (39.3 MB/s) - ‘mccourt.georgetown.edu/research/mccourt-centers/index.html’ saved [145651/145651]\n",
      "\n",
      "--2021-04-23 15:03:27--  https://mccourt.georgetown.edu/research/research-data-center/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 139956 (137K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 136.68K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:27 (32.8 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/index.html’ saved [139956/139956]\n",
      "\n",
      "--2021-04-23 15:03:29--  https://mccourt.georgetown.edu/research/the-massive-data-institute/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 155731 (152K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 152.08K  --.-KB/s    in 0.005s  \n",
      "\n",
      "2021-04-23 15:03:30 (31.6 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/index.html’ saved [155731/155731]\n",
      "\n",
      "--2021-04-23 15:03:32--  https://mccourt.georgetown.edu/research/faculty-seminars/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 157025 (153K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/faculty-seminars/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 153.34K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:33 (34.5 MB/s) - ‘mccourt.georgetown.edu/research/faculty-seminars/index.html’ saved [157025/157025]\n",
      "\n",
      "--2021-04-23 15:03:35--  https://mccourt.georgetown.edu/research/mccourt-centers/research-center-directors/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 155073 (151K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/mccourt-centers/research-center-directors/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 151.44K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:35 (36.8 MB/s) - ‘mccourt.georgetown.edu/research/mccourt-centers/research-center-directors/index.html’ saved [155073/155073]\n",
      "\n",
      "--2021-04-23 15:03:37--  https://mccourt.georgetown.edu/research/mdi-news/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 157499 (154K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/mdi-news/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 153.81K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:38 (36.8 MB/s) - ‘mccourt.georgetown.edu/research/mdi-news/index.html’ saved [157499/157499]\n",
      "\n",
      "--2021-04-23 15:03:40--  https://mccourt.georgetown.edu/research/the-massive-data-institute/resources/dp-resources/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 143363 (140K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/resources/dp-resources/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 140.00K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:40 (34.1 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/resources/dp-resources/index.html’ saved [143363/143363]\n",
      "\n",
      "--2021-04-23 15:03:42--  https://mccourt.georgetown.edu/research/research-data-center/getting-started/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 144786 (141K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/getting-started/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 141.39K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:43 (38.6 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/getting-started/index.html’ saved [144786/144786]\n",
      "\n",
      "--2021-04-23 15:03:45--  https://mccourt.georgetown.edu/research/research-data-center/administrative-data-metadata/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 139980 (137K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/administrative-data-metadata/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 136.70K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2021-04-23 15:03:45 (43.7 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/administrative-data-metadata/index.html’ saved [139980/139980]\n",
      "\n",
      "--2021-04-23 15:03:47--  https://mccourt.georgetown.edu/research/research-data-center/research/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 140145 (137K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/research/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 136.86K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:48 (35.2 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/research/index.html’ saved [140145/140145]\n",
      "\n",
      "--2021-04-23 15:03:50--  https://mccourt.georgetown.edu/research/research-data-center/data-and-software-available/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 156464 (153K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/data-and-software-available/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 152.80K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:50 (37.1 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/data-and-software-available/index.html’ saved [156464/156464]\n",
      "\n",
      "--2021-04-23 15:03:52--  https://mccourt.georgetown.edu/research/research-data-center/contact-us/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 137306 (134K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/contact-us/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 134.09K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:53 (30.2 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/contact-us/index.html’ saved [137306/137306]\n",
      "\n",
      "--2021-04-23 15:03:55--  https://mccourt.georgetown.edu/research/the-massive-data-institute/supporting-ethical-data-governance/research-data-center/contact-us/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-23 15:03:55 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-23 15:03:57--  https://mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research/\n",
      "Connecting to mccourt.georgetown.edu (mccourt.georgetown.edu)|23.185.0.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 143331 (140K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 139.97K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-04-23 15:03:58 (8.29 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research/index.html’ saved [143331/143331]\n",
      "\n",
      "--2021-04-23 15:04:00--  https://mccourt.georgetown.edu/research/the-massive-data-institute/shape-the-policy-conversation/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 142753 (139K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/shape-the-policy-conversation/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 139.41K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2021-04-23 15:04:00 (17.3 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/shape-the-policy-conversation/index.html’ saved [142753/142753]\n",
      "\n",
      "--2021-04-23 15:04:02--  https://mccourt.georgetown.edu/research/the-massive-data-institute/improving-data-driven-policy-making/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 143621 (140K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/improving-data-driven-policy-making/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 140.25K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:02 (119 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/improving-data-driven-policy-making/index.html’ saved [143621/143621]\n",
      "\n",
      "--2021-04-23 15:04:04--  https://mccourt.georgetown.edu/research/the-massive-data-institute/about-mdi/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 143501 (140K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/about-mdi/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 140.14K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2021-04-23 15:04:05 (64.9 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/about-mdi/index.html’ saved [143501/143501]\n",
      "\n",
      "--2021-04-23 15:04:07--  https://mccourt.georgetown.edu/research/the-massive-data-institute/funding-opportunities/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 152029 (148K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/funding-opportunities/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 148.47K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:07 (132 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/funding-opportunities/index.html’ saved [152029/152029]\n",
      "\n",
      "--2021-04-23 15:04:09--  https://mccourt.georgetown.edu/research/the-massive-data-institute/compute-infrastructure/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 143447 (140K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/compute-infrastructure/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 140.08K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:10 (121 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/compute-infrastructure/index.html’ saved [143447/143447]\n",
      "\n",
      "--2021-04-23 15:04:12--  https://mccourt.georgetown.edu/research/the-massive-data-institute/resources/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 142043 (139K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/resources/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 138.71K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:12 (163 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/resources/index.html’ saved [142043/142043]\n",
      "\n",
      "--2021-04-23 15:04:14--  https://mccourt.georgetown.edu/research/the-massive-data-institute/mdi-data-blending/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 141487 (138K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-data-blending/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 138.17K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:14 (139 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-data-blending/index.html’ saved [141487/141487]\n",
      "\n",
      "--2021-04-23 15:04:16--  https://mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research-and-collaborations/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 147895 (144K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research-and-collaborations/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 144.43K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:17 (129 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research-and-collaborations/index.html’ saved [147895/147895]\n",
      "\n",
      "--2021-04-23 15:04:19--  https://mccourt.georgetown.edu/research/the-massive-data-institute/mdi-conferences-and-panels/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 149002 (146K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-conferences-and-panels/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 145.51K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:19 (166 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-conferences-and-panels/index.html’ saved [149002/149002]\n",
      "\n",
      "FINISHED --2021-04-23 15:04:19--\n",
      "Total wall clock time: 1m 2s\n",
      "Downloaded: 25 files, 3.4M in 0.1s (31.8 MB/s)\n",
      "Converting links in mccourt.georgetown.edu/research/mdi-news/index.html... 17-8\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/administrative-data-metadata/index.html... 28-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research/index.html... 35-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research-and-collaborations/index.html... 32-8\n",
      "Converting links in mccourt.georgetown.edu/research/index.html... 20-12\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/research/index.html... 28-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/improving-data-driven-policy-making/index.html... 32-10\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/index.html... 33-8\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/getting-started/index.html... 28-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/mdi-data-blending/index.html... 31-8\n",
      "Converting links in mccourt.georgetown.edu/research/featured-publications/index.html... 17-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/shape-the-policy-conversation/index.html... 32-8\n",
      "Converting links in mccourt.georgetown.edu/research/mccourt-centers/research-center-directors/index.html... 17-8\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/contact-us/index.html... 28-8\n",
      "Converting links in mccourt.georgetown.edu/research/faculty-seminars/index.html... 33-8\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/data-and-software-available/index.html... 28-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/about-mdi/index.html... 24-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/funding-opportunities/index.html... 35-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/index.html... 24-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/compute-infrastructure/index.html... 32-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/resources/dp-resources/index.html... 23-8\r\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/resources/index.html... 34-8\r\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/mdi-conferences-and-panels/index.html... 32-8\r\n",
      "Converting links in mccourt.georgetown.edu/research/mccourt-centers/index.html... 24-8\r\n",
      "Converted links in 24 files in 0.06 seconds.\r\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "!wget --accept .html --recursive --no-parent --page-requisites --convert-links --wait=2 --tries=3 \\\n",
    "    --user-agent=Mozilla --header=\"Accept:text/html\" --no-check-certificate \\\n",
    "    https://mccourt.georgetown.edu/research/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use advanced options for `wget` (listed above) to mirror a website you use often. Be sure to use a polite `--wait` and avoid downloading anything with massive numbers of links, files, or pages (e.g., don't try YouTube.com or Wikipedia.com). If you want to download a segment or specific page within a website (e.g., a single YouTube channel or Wikipedia page), use the `--recursive` option with `--no-parent` (to follow only links within the input URL).\n",
    "\n",
    "While you let `wget` run, read more about it on its [manual](https://www.gnu.org/software/wget/manual/wget.html) and see other examples of `wget` usage [here](https://gist.github.com/bueckl/bd0a1e7a30bc8e2eeefd) and [here](https://phoenixnap.com/kb/wget-command-with-examples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:39:08--  https://www.gnu.org/software/wget/\n",
      "Resolving www.gnu.org (www.gnu.org)... 209.51.188.148, 2001:470:142:3::a\n",
      "Connecting to www.gnu.org (www.gnu.org)|209.51.188.148|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/index.html’\n",
      "\n",
      "www.gnu.org/softwar     [ <=>                ]  10.46K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:08 (363 KB/s) - ‘www.gnu.org/software/wget/index.html’ saved [10708]\n",
      "\n",
      "Loading robots.txt; please ignore errors.\n",
      "--2021-04-23 14:39:10--  https://www.gnu.org/robots.txt\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1135 (1.1K) [text/plain]\n",
      "Saving to: ‘www.gnu.org/robots.txt’\n",
      "\n",
      "www.gnu.org/robots. 100%[===================>]   1.11K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:10 (29.2 MB/s) - ‘www.gnu.org/robots.txt’ saved [1135/1135]\n",
      "\n",
      "--2021-04-23 14:39:12--  https://www.gnu.org/mini.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3490 (3.4K) [text/css]\n",
      "Saving to: ‘www.gnu.org/mini.css’\n",
      "\n",
      "www.gnu.org/mini.cs 100%[===================>]   3.41K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:12 (52.4 MB/s) - ‘www.gnu.org/mini.css’ saved [3490/3490]\n",
      "\n",
      "--2021-04-23 14:39:14--  https://www.gnu.org/layout.min.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18112 (18K) [text/css]\n",
      "Saving to: ‘www.gnu.org/layout.min.css’\n",
      "\n",
      "www.gnu.org/layout. 100%[===================>]  17.69K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-04-23 14:39:14 (626 KB/s) - ‘www.gnu.org/layout.min.css’ saved [18112/18112]\n",
      "\n",
      "--2021-04-23 14:39:16--  https://www.gnu.org/print.min.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1714 (1.7K) [text/css]\n",
      "Saving to: ‘www.gnu.org/print.min.css’\n",
      "\n",
      "www.gnu.org/print.m 100%[===================>]   1.67K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:17 (42.8 MB/s) - ‘www.gnu.org/print.min.css’ saved [1714/1714]\n",
      "\n",
      "--2021-04-23 14:39:19--  https://www.gnu.org/graphics/heckert_gnu.transp.small.png\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7677 (7.5K) [image/png]\n",
      "Saving to: ‘www.gnu.org/graphics/heckert_gnu.transp.small.png’\n",
      "\n",
      "www.gnu.org/graphic 100%[===================>]   7.50K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:19 (156 MB/s) - ‘www.gnu.org/graphics/heckert_gnu.transp.small.png’ saved [7677/7677]\n",
      "\n",
      "--2021-04-23 14:39:21--  https://www.gnu.org/graphics/icons/search.png\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1136 (1.1K) [image/png]\n",
      "Saving to: ‘www.gnu.org/graphics/icons/search.png’\n",
      "\n",
      "www.gnu.org/graphic 100%[===================>]   1.11K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:21 (28.1 MB/s) - ‘www.gnu.org/graphics/icons/search.png’ saved [1136/1136]\n",
      "\n",
      "--2021-04-23 14:39:23--  https://www.gnu.org/software/wget/manual/\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/index.html’\n",
      "\n",
      "www.gnu.org/softwar     [ <=>                ]   7.92K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:23 (286 KB/s) - ‘www.gnu.org/software/wget/manual/index.html’ saved [8112]\n",
      "\n",
      "--2021-04-23 14:39:25--  https://www.gnu.org/graphics/fsf-logo-notext-small.png\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2494 (2.4K) [image/png]\n",
      "Saving to: ‘www.gnu.org/graphics/fsf-logo-notext-small.png’\n",
      "\n",
      "www.gnu.org/graphic 100%[===================>]   2.44K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:25 (94.5 MB/s) - ‘www.gnu.org/graphics/fsf-logo-notext-small.png’ saved [2494/2494]\n",
      "\n",
      "--2021-04-23 14:39:27--  https://www.gnu.org/software/wget/manual/wget.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 345663 (338K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>] 337.56K  --.-KB/s    in 0.1s    \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:27 (2.83 MB/s) - ‘www.gnu.org/software/wget/manual/wget.html’ saved [345663/345663]\n",
      "\n",
      "--2021-04-23 14:39:29--  https://www.gnu.org/software/wget/manual/html_node/index.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12386 (12K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/index.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  12.10K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:29 (128 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/index.html’ saved [12386/12386]\n",
      "\n",
      "--2021-04-23 14:39:31--  https://www.gnu.org/software/wget/manual/wget.html.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 83121 (81K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.html.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  81.17K  --.-KB/s    in 0.06s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:31 (1.39 MB/s) - ‘www.gnu.org/software/wget/manual/wget.html.gz’ saved [83121/83121]\n",
      "\n",
      "--2021-04-23 14:39:33--  https://www.gnu.org/software/wget/manual/wget.html_node.tar.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 95950 (94K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.html_node.tar.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  93.70K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-04-23 14:39:33 (1.56 MB/s) - ‘www.gnu.org/software/wget/manual/wget.html_node.tar.gz’ saved [95950/95950]\n",
      "\n",
      "--2021-04-23 14:39:35--  https://www.gnu.org/software/wget/manual/wget.info.tar.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 72345 (71K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.info.tar.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  70.65K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-04-23 14:39:35 (1.20 MB/s) - ‘www.gnu.org/software/wget/manual/wget.info.tar.gz’ saved [72345/72345]\n",
      "\n",
      "--2021-04-23 14:39:37--  https://www.gnu.org/software/wget/manual/wget.txt\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 226204 (221K) [text/plain]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.txt’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>] 220.90K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-04-23 14:39:37 (1.91 MB/s) - ‘www.gnu.org/software/wget/manual/wget.txt’ saved [226204/226204]\n",
      "\n",
      "--2021-04-23 14:39:39--  https://www.gnu.org/software/wget/manual/wget.txt.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 70458 (69K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.txt.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  68.81K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-04-23 14:39:39 (1.19 MB/s) - ‘www.gnu.org/software/wget/manual/wget.txt.gz’ saved [70458/70458]\n",
      "\n",
      "--2021-04-23 14:39:41--  https://www.gnu.org/software/wget/manual/wget.dvi.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 118092 (115K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.dvi.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>] 115.32K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2021-04-23 14:39:42 (1.33 MB/s) - ‘www.gnu.org/software/wget/manual/wget.dvi.gz’ saved [118092/118092]\n",
      "\n",
      "--2021-04-23 14:39:44--  https://www.gnu.org/software/wget/manual/wget.pdf\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 454439 (444K) [application/pdf]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.pdf’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>] 443.79K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-04-23 14:39:44 (3.05 MB/s) - ‘www.gnu.org/software/wget/manual/wget.pdf’ saved [454439/454439]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:39:46--  https://www.gnu.org/software/wget/manual/wget.texi.tar.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68053 (66K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.texi.tar.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  66.46K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-04-23 14:39:46 (1.13 MB/s) - ‘www.gnu.org/software/wget/manual/wget.texi.tar.gz’ saved [68053/68053]\n",
      "\n",
      "--2021-04-23 14:39:48--  https://www.gnu.org/software/wget/manual/dir.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-23 14:39:48 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-23 14:39:50--  https://www.gnu.org/software/gnulib/manual.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2083 (2.0K) [text/css]\n",
      "Saving to: ‘www.gnu.org/software/gnulib/manual.css’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   2.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:50 (47.9 MB/s) - ‘www.gnu.org/software/gnulib/manual.css’ saved [2083/2083]\n",
      "\n",
      "--2021-04-23 14:39:52--  https://www.gnu.org/software/wget/manual/https//www.gnu.org/software/wget/\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-23 14:39:52 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-23 14:39:54--  https://www.gnu.org/software/wget/manual/html_node/Concept-Index.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64717 (63K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Concept-Index.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  63.20K  --.-KB/s    in 0.06s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:54 (1.08 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Concept-Index.html’ saved [64717/64717]\n",
      "\n",
      "--2021-04-23 14:39:56--  https://www.gnu.org/software/wget/manual/dir/index.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-23 14:39:56 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-23 14:39:58--  https://www.gnu.org/software/wget/manual/html_node/Overview.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6903 (6.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Overview.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.74K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:58 (146 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Overview.html’ saved [6903/6903]\n",
      "\n",
      "--2021-04-23 14:40:00--  https://www.gnu.org/software/wget/manual/html_node/Invoking.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5662 (5.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Invoking.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   5.53K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:00 (140 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Invoking.html’ saved [5662/5662]\n",
      "\n",
      "--2021-04-23 14:40:02--  https://www.gnu.org/software/wget/manual/html_node/URL-Format.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6894 (6.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/URL-Format.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.73K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:02 (149 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/URL-Format.html’ saved [6894/6894]\n",
      "\n",
      "--2021-04-23 14:40:04--  https://www.gnu.org/software/wget/manual/html_node/Option-Syntax.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6653 (6.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Option-Syntax.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.50K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:04 (140 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Option-Syntax.html’ saved [6653/6653]\n",
      "\n",
      "--2021-04-23 14:40:06--  https://www.gnu.org/software/wget/manual/html_node/Basic-Startup-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4050 (4.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Basic-Startup-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.96K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:06 (85.9 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Basic-Startup-Options.html’ saved [4050/4050]\n",
      "\n",
      "--2021-04-23 14:40:08--  https://www.gnu.org/software/wget/manual/html_node/Logging-and-Input-File-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11411 (11K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Logging-and-Input-File-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  11.14K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:08 (54.6 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Logging-and-Input-File-Options.html’ saved [11411/11411]\n",
      "\n",
      "--2021-04-23 14:40:10--  https://www.gnu.org/software/wget/manual/html_node/Download-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40722 (40K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Download-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  39.77K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:10 (1.33 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Download-Options.html’ saved [40722/40722]\n",
      "\n",
      "--2021-04-23 14:40:12--  https://www.gnu.org/software/wget/manual/html_node/Directory-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6857 (6.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Directory-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.70K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:12 (164 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Directory-Options.html’ saved [6857/6857]\n",
      "\n",
      "--2021-04-23 14:40:14--  https://www.gnu.org/software/wget/manual/html_node/HTTP-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28357 (28K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/HTTP-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  27.69K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:14 (988 KB/s) - ‘www.gnu.org/software/wget/manual/html_node/HTTP-Options.html’ saved [28357/28357]\n",
      "\n",
      "--2021-04-23 14:40:16--  https://www.gnu.org/software/wget/manual/html_node/HTTPS-_0028SSL_002fTLS_0029-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18507 (18K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/HTTPS-_0028SSL_002fTLS_0029-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  18.07K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:16 (640 KB/s) - ‘www.gnu.org/software/wget/manual/html_node/HTTPS-_0028SSL_002fTLS_0029-Options.html’ saved [18507/18507]\n",
      "\n",
      "--2021-04-23 14:40:18--  https://www.gnu.org/software/wget/manual/html_node/FTP-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11483 (11K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/FTP-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  11.21K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:19 (121 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/FTP-Options.html’ saved [11483/11483]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:40:21--  https://www.gnu.org/software/wget/manual/html_node/Recursive-Retrieval-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15938 (16K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Recursive-Retrieval-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  15.56K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:21 (566 KB/s) - ‘www.gnu.org/software/wget/manual/html_node/Recursive-Retrieval-Options.html’ saved [15938/15938]\n",
      "\n",
      "--2021-04-23 14:40:23--  https://www.gnu.org/software/wget/manual/html_node/Recursive-Accept_002fReject-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9306 (9.1K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Recursive-Accept_002fReject-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   9.09K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:23 (64.5 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Recursive-Accept_002fReject-Options.html’ saved [9306/9306]\n",
      "\n",
      "--2021-04-23 14:40:25--  https://www.gnu.org/software/wget/manual/html_node/Exit-Status.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4000 (3.9K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Exit-Status.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.91K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:25 (91.8 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Exit-Status.html’ saved [4000/4000]\n",
      "\n",
      "--2021-04-23 14:40:27--  https://www.gnu.org/software/wget/manual/html_node/Recursive-Download.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6946 (6.8K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Recursive-Download.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.78K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:27 (145 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Recursive-Download.html’ saved [6946/6946]\n",
      "\n",
      "--2021-04-23 14:40:29--  https://www.gnu.org/software/wget/manual/html_node/Following-Links.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4481 (4.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Following-Links.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.38K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:29 (65.0 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Following-Links.html’ saved [4481/4481]\n",
      "\n",
      "--2021-04-23 14:40:31--  https://www.gnu.org/software/wget/manual/html_node/Spanning-Hosts.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5907 (5.8K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Spanning-Hosts.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   5.77K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:31 (130 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Spanning-Hosts.html’ saved [5907/5907]\n",
      "\n",
      "--2021-04-23 14:40:33--  https://www.gnu.org/software/wget/manual/html_node/Types-of-Files.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9740 (9.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Types-of-Files.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   9.51K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:33 (81.6 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Types-of-Files.html’ saved [9740/9740]\n",
      "\n",
      "--2021-04-23 14:40:35--  https://www.gnu.org/software/wget/manual/html_node/Directory_002dBased-Limits.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7714 (7.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Directory_002dBased-Limits.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   7.53K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:35 (78.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Directory_002dBased-Limits.html’ saved [7714/7714]\n",
      "\n",
      "--2021-04-23 14:40:37--  https://www.gnu.org/software/wget/manual/html_node/Relative-Links.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3825 (3.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Relative-Links.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:37 (20.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Relative-Links.html’ saved [3825/3825]\n",
      "\n",
      "--2021-04-23 14:40:39--  https://www.gnu.org/software/wget/manual/html_node/FTP-Links.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3810 (3.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/FTP-Links.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.72K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:39 (30.1 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/FTP-Links.html’ saved [3810/3810]\n",
      "\n",
      "--2021-04-23 14:40:41--  https://www.gnu.org/software/wget/manual/html_node/Time_002dStamping.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5577 (5.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Time_002dStamping.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   5.45K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:41 (140 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Time_002dStamping.html’ saved [5577/5577]\n",
      "\n",
      "--2021-04-23 14:40:43--  https://www.gnu.org/software/wget/manual/html_node/Time_002dStamping-Usage.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5658 (5.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Time_002dStamping-Usage.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   5.53K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:43 (154 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Time_002dStamping-Usage.html’ saved [5658/5658]\n",
      "\n",
      "--2021-04-23 14:40:45--  https://www.gnu.org/software/wget/manual/html_node/HTTP-Time_002dStamping-Internals.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4870 (4.8K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/HTTP-Time_002dStamping-Internals.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.76K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:45 (76.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/HTTP-Time_002dStamping-Internals.html’ saved [4870/4870]\n",
      "\n",
      "--2021-04-23 14:40:47--  https://www.gnu.org/software/wget/manual/html_node/FTP-Time_002dStamping-Internals.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4461 (4.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/FTP-Time_002dStamping-Internals.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.36K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:47 (111 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/FTP-Time_002dStamping-Internals.html’ saved [4461/4461]\n",
      "\n",
      "--2021-04-23 14:40:49--  https://www.gnu.org/software/wget/manual/html_node/Startup-File.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4442 (4.3K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Startup-File.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.34K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:49 (98.8 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Startup-File.html’ saved [4442/4442]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:40:51--  https://www.gnu.org/software/wget/manual/html_node/Wgetrc-Location.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3677 (3.6K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Location.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.59K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:51 (88.9 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Location.html’ saved [3677/3677]\n",
      "\n",
      "--2021-04-23 14:40:53--  https://www.gnu.org/software/wget/manual/html_node/Wgetrc-Syntax.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3731 (3.6K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Syntax.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.64K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:53 (86.0 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Syntax.html’ saved [3731/3731]\n",
      "\n",
      "--2021-04-23 14:40:55--  https://www.gnu.org/software/wget/manual/html_node/Wgetrc-Commands.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27543 (27K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Commands.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  26.90K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:55 (944 KB/s) - ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Commands.html’ saved [27543/27543]\n",
      "\n",
      "--2021-04-23 14:40:57--  https://www.gnu.org/software/wget/manual/html_node/Sample-Wgetrc.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8653 (8.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Sample-Wgetrc.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   8.45K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:57 (58.9 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Sample-Wgetrc.html’ saved [8653/8653]\n",
      "\n",
      "--2021-04-23 14:40:59--  https://www.gnu.org/software/wget/manual/html_node/Examples.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3478 (3.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Examples.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.40K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:59 (74.8 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Examples.html’ saved [3478/3478]\n",
      "\n",
      "--2021-04-23 14:41:01--  https://www.gnu.org/software/wget/manual/html_node/Simple-Usage.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4390 (4.3K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Simple-Usage.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.29K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:01 (96.7 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Simple-Usage.html’ saved [4390/4390]\n",
      "\n",
      "--2021-04-23 14:41:03--  https://www.gnu.org/software/wget/manual/html_node/Advanced-Usage.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7968 (7.8K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Advanced-Usage.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   7.78K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:03 (159 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Advanced-Usage.html’ saved [7968/7968]\n",
      "\n",
      "--2021-04-23 14:41:05--  https://www.gnu.org/software/wget/manual/html_node/Very-Advanced-Usage.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4496 (4.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Very-Advanced-Usage.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.39K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:05 (91.0 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Very-Advanced-Usage.html’ saved [4496/4496]\n",
      "\n",
      "--2021-04-23 14:41:07--  https://www.gnu.org/software/wget/manual/html_node/Various.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4473 (4.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Various.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.37K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:07 (117 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Various.html’ saved [4473/4473]\n",
      "\n",
      "--2021-04-23 14:41:09--  https://www.gnu.org/software/wget/manual/html_node/Proxies.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6815 (6.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Proxies.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.66K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:09 (38.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Proxies.html’ saved [6815/6815]\n",
      "\n",
      "--2021-04-23 14:41:11--  https://www.gnu.org/software/wget/manual/html_node/Distribution.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3075 (3.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Distribution.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.00K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:11 (73.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Distribution.html’ saved [3075/3075]\n",
      "\n",
      "--2021-04-23 14:41:13--  https://www.gnu.org/software/wget/manual/html_node/Web-Site.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3057 (3.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Web-Site.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   2.99K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:13 (79.2 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Web-Site.html’ saved [3057/3057]\n",
      "\n",
      "--2021-04-23 14:41:15--  https://www.gnu.org/software/wget/manual/html_node/Mailing-Lists.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5358 (5.2K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Mailing-Lists.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   5.23K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:16 (110 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Mailing-Lists.html’ saved [5358/5358]\n",
      "\n",
      "--2021-04-23 14:41:18--  https://www.gnu.org/software/wget/manual/html_node/Internet-Relay-Chat.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3083 (3.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Internet-Relay-Chat.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.01K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:18 (68.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Internet-Relay-Chat.html’ saved [3083/3083]\n",
      "\n",
      "--2021-04-23 14:41:20--  https://www.gnu.org/software/wget/manual/html_node/Reporting-Bugs.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6151 (6.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Reporting-Bugs.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.01K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:20 (150 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Reporting-Bugs.html’ saved [6151/6151]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:41:22--  https://www.gnu.org/software/wget/manual/html_node/Portability.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4458 (4.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Portability.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.35K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:22 (114 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Portability.html’ saved [4458/4458]\n",
      "\n",
      "--2021-04-23 14:41:24--  https://www.gnu.org/software/wget/manual/html_node/Signals.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3420 (3.3K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Signals.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.34K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:24 (73.4 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Signals.html’ saved [3420/3420]\n",
      "\n",
      "--2021-04-23 14:41:26--  https://www.gnu.org/software/wget/manual/html_node/Appendices.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3452 (3.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Appendices.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.37K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:26 (79.6 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Appendices.html’ saved [3452/3452]\n",
      "\n",
      "--2021-04-23 14:41:28--  https://www.gnu.org/software/wget/manual/html_node/Robot-Exclusion.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7740 (7.6K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Robot-Exclusion.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   7.56K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:28 (164 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Robot-Exclusion.html’ saved [7740/7740]\n",
      "\n",
      "--2021-04-23 14:41:30--  https://www.gnu.org/software/wget/manual/html_node/Security-Considerations.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3988 (3.9K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Security-Considerations.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.89K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:30 (97.1 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Security-Considerations.html’ saved [3988/3988]\n",
      "\n",
      "--2021-04-23 14:41:32--  https://www.gnu.org/software/wget/manual/html_node/Contributors.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9201 (9.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Contributors.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   8.99K  --.-KB/s    in 0.001s  \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:32 (10.8 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Contributors.html’ saved [9201/9201]\n",
      "\n",
      "--2021-04-23 14:41:34--  https://www.gnu.org/software/wget/manual/html_node/Copying-this-manual.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3127 (3.1K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Copying-this-manual.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.05K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:34 (78.1 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Copying-this-manual.html’ saved [3127/3127]\n",
      "\n",
      "--2021-04-23 14:41:36--  https://www.gnu.org/software/wget/manual/html_node/GNU-Free-Documentation-License.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27757 (27K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/GNU-Free-Documentation-License.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  27.11K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:36 (955 KB/s) - ‘www.gnu.org/software/wget/manual/html_node/GNU-Free-Documentation-License.html’ saved [27757/27757]\n",
      "\n",
      "--2021-04-23 14:41:38--  https://www.gnu.org/style.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4565 (4.5K) [text/css]\n",
      "Saving to: ‘www.gnu.org/style.css’\n",
      "\n",
      "www.gnu.org/style.c 100%[===================>]   4.46K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:41:38 (108 MB/s) - ‘www.gnu.org/style.css’ saved [4565/4565]\n",
      "\n",
      "--2021-04-23 14:41:40--  https://www.gnu.org/software/wget/manual/html_node/https//www.gnu.org/software/wget/\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-23 14:41:40 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-23 14:41:42--  https://www.gnu.org/reset.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2596 (2.5K) [text/css]\n",
      "Saving to: ‘www.gnu.org/reset.css’\n",
      "\n",
      "www.gnu.org/reset.c 100%[===================>]   2.54K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:41:42 (64.8 MB/s) - ‘www.gnu.org/reset.css’ saved [2596/2596]\n",
      "\n",
      "FINISHED --2021-04-23 14:41:42--\n",
      "Total wall clock time: 2m 34s\n",
      "Downloaded: 72 files, 2.0M in 1.1s (1.87 MB/s)\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "!wget --mirror --recursive --no-parent --page-requisites --convert-links --wait=2 --tries=3 \\\n",
    "    --user-agent=Mozilla --header=\"Accept:text/html\" --no-check-certificate \\\n",
    "    https://www.jarenhaber.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template code: See Google Places API in action<a id='Places'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your reference, this is the code you would use to do URL scraping with the Google Places API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from googleplaces import GooglePlaces, types  # Google Places API: 'types' lets us define what kind of entity to look for (e.g., schools)\n",
    "import re\n",
    "\n",
    "# Initialize Google Places API key\n",
    "api_fp = 'define_me.txt' # Replace with API key filepath\n",
    "places_api_key = re.sub(\"\\n\", \"\", open(api_fp).read())\n",
    "google_places = GooglePlaces(places_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Google Places API in action\n",
    "school_name = \"River City Scholars Charter Academy\"\n",
    "school_address = \"944 Evergreen Street, Grand Rapids, MI 49507\"\n",
    "\n",
    "query_result = google_places.nearby_search(\n",
    "        location=school_address, name=school_name,\n",
    "        radius=15000, types=[types.TYPE_SCHOOL], rankby='distance') # Search for schools within 15000 km of input location\n",
    "\n",
    "for place in query_result.places:\n",
    "    print(place.name)\n",
    "    place.get_details()  # makes further API call\n",
    "    print(place.details) # A dict matching the JSON response from Google.\n",
    "    print(place.website)\n",
    "    print(place.formatted_address)\n",
    "\n",
    "# Are there any additional pages of results?\n",
    "if query_result.has_next_page_token:\n",
    "    query_result_next_page = google_places.nearby_search(\n",
    "        pagetoken=query_result.next_page_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look like this:\n",
    "```python\n",
    "River City Scholars Charter Academy\n",
    "http://rivercityscholars.org/\n",
    "944 Evergreen St SE, Grand Rapids, MI 49507, USA\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More robust code with a blacklist\n",
    "query_result = google_places.nearby_search(\n",
    "    location=address, name=school_name,\n",
    "    radius=15000, types=[types.TYPE_SCHOOL], rankby='distance') # search within radius of Google Places API search (in km)\n",
    "        \n",
    "for place in query_result.places:\n",
    "    place.get_details()  # Make further API call to get detailed info on this place\n",
    "    \n",
    "    found_name = place.name  # Compare this name in Places API to school's name on file\n",
    "    found_address = place.formatted_address  # Compare this address in Places API to address on file\n",
    "\n",
    "    url = place.website  # Grab school URL from Google Places API, if it's there\n",
    "    \n",
    "    # Initialize blacklist match counter\n",
    "    blacklisted_num = 0 \n",
    "\n",
    "    if any(domain in url for domain in blacklist):\n",
    "        blacklisted_num += 1    # If this url is in bad_sites_list, add 1 to counter and move on\n",
    "        print(\"URL in Google Places API is a third-party domain. Moving on.\")\n",
    "\n",
    "    else:\n",
    "        good_url = url\n",
    "        print(\"Success! URL obtained from Google Places API with \" + str(blacklisted_num) + \" bad URLs avoided.\")\n",
    "        break # Exit for-loop after finding first good result\n",
    "        \n",
    "print(f'Quality URL: {good_url}') # Show valid URL of the Place discovered in Google Places API"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
