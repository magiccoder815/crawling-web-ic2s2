{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to web-scraping\n",
    "\n",
    "It's 2021. The web is everywhere.\n",
    "\n",
    "* If you want to buy a house, real estate agents have [websites](https://www.wendytlouie.com/) where they list the houses they're currently selling. \n",
    "* If you want to know whether to where a rain jacket or shorts, you check the weather on a [website](https://weather.com/weather/tenday/l/Berkeley+CA+USCA0087:1:US). \n",
    "* If you want to know what's happening in the world, you read the news [online](https://www.sfchronicle.com/). \n",
    "* If you've forgotten which city is the capital of Australia, you check [Wikipedia](https://en.wikipedia.org/wiki/Australia).\n",
    "\n",
    "**The point is this: there is an enormous amount of information (also known as data) on the web.**\n",
    "\n",
    "If we (in our capacities as, for example, data scientists, social scientists, digital humanists, businesses, public servants or members of the public) can get our hands on this information, **we can answer all sorts of interesting questions or solve important problems**.\n",
    "\n",
    "* Maybe you're studying gender bias in student evaluations of professors. One option would be to scrape ratings from [Rate My Professors](https://www.ratemyprofessors.com/) (provided you follow their [terms of service](https://www.ratemyprofessors.com/TermsOfUse_us.jsp#use))\n",
    "* Perhaps you want to build an app that shows users articles relating to their specified interests. You could scrape stories from various news websites and then use NLP methods to decide which articles to show which users.\n",
    "* [Geoff Boeing](https://geoffboeing.com/) and [Paul Waddell](https://ced.berkeley.edu/ced/faculty-staff/paul-waddell) recently published [a great study](https://arxiv.org/pdf/1605.05397.pdf) of the US housing market by scraping millions of Craiglist rental listings. Among other insights, their study shows which metropolitan areas in the US are more or less affordable to renters.\n",
    "\n",
    "This first day's workshop is a one-hour beginner's introduction to web scraping. \n",
    "\n",
    "\n",
    "## Learning Goals\n",
    "*   \n",
    "\n",
    "## Outline\n",
    "\n",
    "* [Structured queries with APIs](#apis)\n",
    "* [Domain collection with automated google search](#domain)\n",
    "* [Mirroring websites with `wget`](#wget)\n",
    "\n",
    "## Background\n",
    "\n",
    "We will do some review, but this notebook assumes you have basic familiarity with Python. If you need a beginner's introduction to coding in Python, please walk through the intro to Python notebook at `extra/intro-to-python.ipynb` and/or [this one](https://github.com/lknelson/text-analysis-course/blob/master/scripts/01.25.02_PythonBasics.ipynb) *before* the workshop. \n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "* *domain*: \n",
    "    * The address of information on the web and directions to get there. Known technically as a URL (Uniform Resource Locator), a domain points to resources--usually the files needed to show a website, but it can also point to files and such. \n",
    "* *web-scraping* (i.e., *screen-scraping*):\n",
    "    * Extracting structured information from the files that make up websites (i.e., what's shown in web browsers), relying on their HTML, CSS, and sometimes JS files. \n",
    "* *Hyper-Text Markup Language (HTML)*: \n",
    "    * The standard markup language for websites, the \"nuts and bolts\" of WHAT a website will display, including text.\n",
    "* *Cascading Style Sheets (CSS)*: \n",
    "    * A technology used to format the layout of a webpage, i.e. HOW to make it pretty. Not usually relevant for web-scraping.\n",
    "* *web-crawling*:\n",
    "    * Finding web pages through links, automated search, etc. Once discovered, pages can be checked (is this website still up?), downloaded, or scraped. \n",
    "* *website mirroring*:\n",
    "    * Creating a complete local copy of the files needed to display and host a website. \n",
    "* *Application Programming Interface (API)*:\n",
    "    * A tool used to access structured data provided by an organization. Examples include Twitter, Reddit, Wikipedia, and the New York Times. When an API is available (not always the case), this is usually the preferred way to access data (over web-scraping).\n",
    "\n",
    "**__________________________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured queries with APIs<a id='apis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's try out the [Google Fact Check API](https://developers.google.com/fact-check/tools/api/), which can be easily explored [in a browser](https://toolbox.google.com/factcheck/explorer). By searching this Google service, the Fact Identifier collects facts relevant to the query input by user (or built in by default, as in the current version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import requests # for downloading\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag # for html scraping\n",
    "import regex as re # Regex module with Unicode support\n",
    "import html5lib # slower but more accurate bs4 parser for messy HTML # lxml faster\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "# Import functions to scrape fact check web pages\n",
    "from scrape_helpers import load_api_key, clean_text, scrape_politifact, scrape_factcheck, scrape_snopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Call API\n",
    "######################################################\n",
    "\n",
    "# Elements in query response: text, claimDate, claimReview[publisher[name], url, textualRating]\n",
    "# Columns in output CSV: (date (DD-MM-YYYY), claim, truth rating, url, source (publisher), fact, explanation\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    page_token = 0\n",
    "    domains = ['covid', 'blm', 'election']\n",
    "    query_sets = [\n",
    "        [\"masks\", \"Chinese bioweapon\", \"China virus\"],\n",
    "        [\"George Floyd\", \"Antifa\", \"Black Lives Matter\"],\n",
    "        [\"Hunter Biden\", \"rigged election\", \"mail-in\", \"election ballots\"],\n",
    "    ]\n",
    "\n",
    "    api_key_fp = \"api_key.txt\"\n",
    "    key = load_api_key(api_key_fp)\n",
    "    endpoint = 'https://factchecktools.googleapis.com'\n",
    "    search = '/v1alpha1/claims:search'\n",
    "\n",
    "    sites = ['politifact.com', 'factcheck.org', 'snopes.com']\n",
    "    site_scrapers = [scrape_politifact, scrape_factcheck, scrape_snopes]\n",
    "    site_switches = ['politifact', 'factcheck.org', 'snopes']\n",
    "\n",
    "\n",
    "    for i in range(0, len(domains)):\n",
    "        domain = domains[i]\n",
    "        queries = query_sets[i]\n",
    "        claims = [] # initialize list of claims\n",
    "\n",
    "        for query in queries:\n",
    "            urls = set() # initialize set of fact check URLs already seen for this query\n",
    "\n",
    "            for site in tqdm(sites, desc='Collecting data for {} via API'.format(query)):\n",
    "                params = {\n",
    "                    'pageToken': page_token,\n",
    "                    'query': query,\n",
    "                    'reviewPublisherSiteFilter': site,\n",
    "                    'key': key\n",
    "                }\n",
    "\n",
    "                nextToken = True\n",
    "                while nextToken:\n",
    "                    url = endpoint + search + '?' + urllib.parse.urlencode(params)\n",
    "                    response = requests.get(url)\n",
    "                    data = response.json()\n",
    "\n",
    "                    if 'claims' in data:\n",
    "                        for claim in data['claims']:\n",
    "                            if not site == 'snopes.com':\n",
    "                                claims.append([claim['claimDate'],\n",
    "                                               claim['text'],\n",
    "                                               claim['claimReview'][0]['textualRating'],\n",
    "                                               claim['claimReview'][0]['url'],\n",
    "                                               claim['claimReview'][0]['publisher']['name']])\n",
    "                            else:\n",
    "                                claims.append([claim['claimReview'][0]['reviewDate'],\n",
    "                                               claim['text'],\n",
    "                                               claim['claimReview'][0]['textualRating'],\n",
    "                                               claim['claimReview'][0]['url'],\n",
    "                                               claim['claimReview'][0]['publisher']['name']\n",
    "                                              .replace('.com', '')])\n",
    "\n",
    "                    if 'nextPageToken' in data:\n",
    "                        params['pageToken'] = data['nextPageToken']\n",
    "                    else:\n",
    "                        nextToken = False\n",
    "\n",
    "            for j in tqdm(range(0, len(claims)), desc='Scraping websites'.format(query)):\n",
    "                claim = claims[j]\n",
    "                switch = site_switches.index(claim[4].lower()) # use fact to get publisher site then index (site name is 5th element)\n",
    "                scraper = site_scrapers[switch] # get scraper using index\n",
    "                claim.extend(scraper(claim[3])) # scrape URL using scraper (URL is 4th element), add to existing claim info\n",
    "                claims[j] = claim # record fact in list\n",
    "\n",
    "            claims # remove duplicates\n",
    "\n",
    "            # Save output for this query\n",
    "            query_string = query.replace(' ', '-')\n",
    "            with open('data/fact_checker_data_{}.csv'.format(query_string), 'w') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(['date', 'claim', 'truth_rating', 'url', 'source', 'fact', 'explanation'])\n",
    "                for claim in claims: # Each claim gets its own column\n",
    "                    if claim[3] not in urls: # don't add if fact check URL already seen\n",
    "                        csv_writer.writerow(claim) # save row\n",
    "                        urls.add(claim[3]) # add to set of urls already saved\n",
    "\n",
    "            print('Saved {} claims for {} query.'.format(str(len(urls)), query))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain collection with automated google search<a id='domain'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses two related functions to scrape the best URL from online sources: \n",
    "> The Google Places API. See the [GitHub page](https://github.com/slimkrazy/python-google-places) for the Python wrapper and sample code, [Google Web Services](https://developers.google.com/places/web-service/) for general documentation, and [here](https://developers.google.com/places/web-service/details) for details on Place Details requests.\n",
    "\n",
    "> The Google Search function (manually filtered). See [here](https://pypi.python.org/pypi/google) for source code and [here](http://pythonhosted.org/google/) for documentation.\n",
    "\n",
    "To get an API key for the Google Places API (or Knowledge Graph API), go to the [Google API Console](http://code.google.com/apis/console).\n",
    "To upgrade your quota limits, sign up for billing--it's free and raises your daily request quota from 1K to 150K (!!).\n",
    "\n",
    "The code below doesn't use Google's Knowledge Graph (KG) Search API because this turns out NOT to reveal websites related to search results (despite these being displayed in the KG cards visible at right in a standard Google search). The KG API is only useful for scraping KG id, description, name, and other basic/ irrelevant info. TO see examples of how the KG API constructs a search URL, etc., (see [here](http://searchengineland.com/cool-tricks-hack-googles-knowledge-graph-results-featuring-donald-trump-268231)).\n",
    "\n",
    "Possibly useful note on debugging: An issue causing the GooglePlaces package to unnecessarily give a \"ValueError\" and stop was resolved in [July 2017](https://github.com/slimkrazy/python-google-places/issues/59). <br>\n",
    "Other instances of this error may occur if Google Places API cannot identify a location as given. Dealing with this is a matter of proper Exception handling (which seems to be working fine below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google # For automated Google searching \n",
    "!pip install https://github.com/slimkrazy/python-google-places/zipball/master # Google Places API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicts_to_csv(list_of_dicts, file_name, header):\n",
    "    '''This helper function writes a list of dictionaries to a csv called file_name, with column names decided by 'header'.'''\n",
    "    \n",
    "    with open(file_name, 'w') as output_file:\n",
    "        print(\"Saving to \" + str(file_name) + \" ...\")\n",
    "        dict_writer = csv.DictWriter(output_file, header)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_left(list_of_dicts, varname):\n",
    "    '''This helper function determines how many dicts in list_of_dicts don't have a valid key/value pair with key varname.'''\n",
    "    \n",
    "    count = 0\n",
    "    for school in list_of_dicts:\n",
    "        if school[varname] == \"\" or school[varname] == None:\n",
    "            count += 1\n",
    "\n",
    "    print(str(count) + \" schools in this data are missing \" + str(varname) + \"s.\")\n",
    "\n",
    "count_left(sample, 'URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Python search environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING KEY PACKAGES\n",
    "from googlesearch import search # automated Google Search package\n",
    "from googleplaces import GooglePlaces, types, lang  # Google Places API\n",
    "\n",
    "import csv, re, os  # Standard packages\n",
    "import pandas as pd  # for working with csv files\n",
    "import urllib, requests  # for scraping\n",
    "from tqdm import tqdm # for progress tracking in for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Google Places API search functionality\n",
    "places_api_key = re.sub(\"\\n\", \"\", open(\"../data/places_api_key.txt\").read())\n",
    "print(places_api_key)\n",
    "\n",
    "google_places = GooglePlaces(places_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a list of sites we DON'T want to spider, \n",
    "# but that an automated Google search might return...\n",
    "# and we might thus accidentally spider unless we filter them out (as below)!\n",
    "\n",
    "bad_sites = []\n",
    "with open('../data/bad_sites.csv', 'r', encoding = 'utf-8') as csvfile:\n",
    "    for row in csvfile:\n",
    "        bad_sites.append(re.sub('\\n', '', row))\n",
    "\n",
    "print(bad_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the Google Places API wrapper at work!\n",
    "school_name = \"River City Scholars Charter Academy\"\n",
    "address = \"944 Evergreen Street, Grand Rapids, MI 49507\"\n",
    "\n",
    "query_result = google_places.nearby_search(\n",
    "        location=address, name=school_name,\n",
    "        radius=15000, types=[types.TYPE_SCHOOL], rankby='distance')\n",
    "\n",
    "for place in query_result.places:\n",
    "    print(place.name)\n",
    "    place.get_details()  # makes further API call\n",
    "    #print(place.details) # A dict matching the JSON response from Google.\n",
    "    print(place.website)\n",
    "    print(place.formatted_address)\n",
    "\n",
    "# Are there any additional pages of results?\n",
    "if query_result.has_next_page_token:\n",
    "    query_result_next_page = google_places.nearby_search(\n",
    "            pagetoken=query_result.next_page_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the google search function:\n",
    "for url in search('DR DAVID C WALKER INT 6500 IH 35 N STE C, SAN ANTONIO, TX 78218', \\\n",
    "                  stop=5, pause=5.0):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []  # make empty list in which to store the dictionaries\n",
    "\n",
    "if os.path.exists('../data/filtered_schools.csv'):  # first, check if file containing search results is available on disk\n",
    "    file_path = '../data/filtered_schools.csv'\n",
    "else:  # use original data if no existing results are available on disk\n",
    "    file_path = '../../data_management/data/charters_unscraped_noURL_2015.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding = 'utf-8') as csvfile: # open file                      \n",
    "    print('Reading in ' + str(file_path) + ' ...')\n",
    "    reader = csv.DictReader(csvfile)  # create a reader\n",
    "    for row in reader:  # loop through rows\n",
    "        sample.append(row)  # append each row to the list\n",
    "\n",
    "print(\"\\nColumns in data: \")\n",
    "print(list(sample[0]))\n",
    "sample = sample[0:5]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new \"URL\" and \"NUM_BAD_URLS\" variables for each school, without overwriting any with data there already:\n",
    "for school in sample:\n",
    "    try:\n",
    "        if len(school[\"URL\"]) > 0:\n",
    "            pass\n",
    "        \n",
    "    except (KeyError, NameError):\n",
    "        school[\"URL\"] = \"\"\n",
    "\n",
    "for school in sample:\n",
    "    try:\n",
    "        if school[\"QUERY_RANKING\"]:\n",
    "            pass\n",
    "        \n",
    "    except (KeyError, NameError):\n",
    "        school[\"QUERY_RANKING\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Take a look at the first entry's contents and the variables list in our sample (a list of dictionaries)\n",
    "print(sample[1][\"SCH_NAME\"], \"\\n\", sample[1][\"ADDRESSES\"], \"\\n\", sample[1][\"NCESSCH\"], \"\\n\")\n",
    "print(sample[1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getURL(school_name, address, bad_sites_list): # manual_url\n",
    "    \n",
    "    '''This function finds the one best URL for a school using two methods:\n",
    "    \n",
    "    1. If a school with this name can be found within 20 km (to account for proximal relocations) in\n",
    "    the Google Maps database (using the Google Places API), AND\n",
    "    if this school has a website on record, then this website is returned.\n",
    "    If no school is found, the school discovered has missing data in Google's database (latitude/longitude, \n",
    "    address, etc.), or the address on record is unreadable, this passes to method #2. \n",
    "    \n",
    "    2. An automated Google search using the school's name + address. This is an essential backup plan to \n",
    "    Google Places API, because sometimes the address on record (courtesy of Dept. of Ed. and our tax dollars) is not \n",
    "    in Google's database. For example, look at: \"3520 Central Pkwy Ste 143 Mezz, Cincinnati, OH 45223\". \n",
    "    No wonder Google Maps can't find this. How could it intelligibly interpret \"Mezz\"?\n",
    "    \n",
    "    Whether using the first or second method, this function excludes URLs with any of the 62 bad_sites defined above, \n",
    "    e.g. trulia.com, greatschools.org, mapquest. It returns the number of excluded URLs (from either method) \n",
    "    and the first non-bad URL discovered.'''\n",
    "    \n",
    "    \n",
    "    ## INITIALIZE\n",
    "    \n",
    "    new_urls = []    # start with empty list\n",
    "    good_url = \"\"    # output goes here\n",
    "    k = 0    # initialize counter for number of URLs skipped\n",
    "    \n",
    "    radsearch = 15000  # define radius of Google Places API search, in km\n",
    "    numgoo = 20  # define number of google results to collect for method #2\n",
    "    wait_time = 20.0  # define length of pause between Google searches (longer is better for big catches like this)\n",
    "    \n",
    "    search_terms = school_name + \" \" + address\n",
    "    print(\"Getting URL for \" + school_name + \", \" + address + \"...\")    # show school name & address\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## FIRST URL-SCRAPE ATTEMPT: GOOGLE PLACES API\n",
    "    # Search for nearest school with this name within radsearch km of this address\n",
    "    \n",
    "    try:\n",
    "        query_result = google_places.nearby_search(\n",
    "            location=address, name=school_name,\n",
    "            radius=radsearch, types=[types.TYPE_SCHOOL], rankby='distance')\n",
    "        \n",
    "        for place in query_result.places:\n",
    "            place.get_details()  # Make further API call to get detailed info on this place\n",
    "\n",
    "            found_name = place.name  # Compare this name in Places API to school's name on file\n",
    "            found_address = place.formatted_address  # Compare this address in Places API to address on file\n",
    "\n",
    "            try: \n",
    "                url = place.website  # Grab school URL from Google Places API, if it's there\n",
    "\n",
    "                if any(domain in url for domain in bad_sites_list):\n",
    "                    k+=1    # If this url is in bad_sites_list, add 1 to counter and move on\n",
    "                    #print(\"  URL in Google Places API is a bad site. Moving on.\")\n",
    "\n",
    "                else:\n",
    "                    good_url = url\n",
    "                    print(\"    Success! URL obtained from Google Places API with \" + str(k) + \" bad URLs avoided.\")\n",
    "                    \n",
    "                    '''\n",
    "                    # For testing/ debugging purposes:\n",
    "                    \n",
    "                    print(\"  VALIDITY CHECK: Is the discovered URL of \" + good_url + \\\n",
    "                          \" consistent with the known URL of \" + manual_url + \" ?\")\n",
    "                    print(\"  Also, is the discovered name + address of \" + found_name + \" \" + found_address + \\\n",
    "                          \" consistent with the known name/address of: \" + search_terms + \" ?\")\n",
    "                    \n",
    "                    if manual_url != \"\":\n",
    "                        if manual_url == good_url:\n",
    "                            print(\"    Awesome! The known and discovered URLs are the SAME!\")\n",
    "                    '''\n",
    "                            \n",
    "                    return(k, good_url)  # Returns valid URL of the Place discovered in Google Places API\n",
    "        \n",
    "            except:  # No URL in the Google database? Then try next API result or move on to Google searching.\n",
    "                print(\"  Error collecting URL from Google Places API. Moving on.\")\n",
    "                pass\n",
    "    \n",
    "    except:\n",
    "        print(\"  Google Places API search failed. Moving on to Google search.\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "    ## SECOND URL-SCRAPE ATTEMPT: FILTERED GOOGLE SEARCH\n",
    "    # Automate Google search and take first result that doesn't have a bad_sites_list element in it.\n",
    "    \n",
    "    \n",
    "    # Loop through google search output to find first good result:\n",
    "    try:\n",
    "        new_urls = list(search(search_terms, stop=numgoo, pause=wait_time))  # Grab first numgoo Google results (URLs)\n",
    "        print(\"  Successfully collected Google search results.\")\n",
    "        \n",
    "        for url in new_urls:\n",
    "            if any(domain in url for domain in bad_sites_list):\n",
    "                k+=1    # If this url is in bad_sites_list, add 1 to counter and move on\n",
    "                #print(\"  Bad site detected. Moving on.\")\n",
    "            else:\n",
    "                good_url = url\n",
    "                print(\"    Success! URL obtained by Google search with \" + str(k) + \" bad URLs avoided.\")\n",
    "                break    # Exit for loop after first good url is found\n",
    "                \n",
    "    \n",
    "    except:\n",
    "        print(\"  Problem with collecting Google search results. Try this by hand instead.\")\n",
    "            \n",
    "        \n",
    "    '''\n",
    "    # For testing/ debugging purposes:\n",
    "    \n",
    "    if k>2:  # Print warning messages depending on number of bad sites preceding good_url\n",
    "        print(\"  WARNING!! CHECK THIS URL!: \" + good_url + \\\n",
    "              \"\\n\" + str(k) + \" bad Google results have been omitted.\")\n",
    "    if k>1:\n",
    "        print(str(k) + \" bad Google results have been omitted. Check this URL!\")\n",
    "    elif k>0:\n",
    "        print(str(k) + \" bad Google result has been omitted. Check this URL!\")\n",
    "    else: \n",
    "        print(\"  No bad sites detected. Reliable URL!\")\n",
    "    \n",
    "    if manual_url != \"\":\n",
    "        if manual_url == good_url:\n",
    "            print(\"    Awesome! The known and discovered URLs are the SAME!\")\n",
    "    '''\n",
    "    \n",
    "    if good_url == \"\":\n",
    "        print(\"  WARNING! No good URL found via API or google search.\\n\")\n",
    "    \n",
    "    return(k + 1, good_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numschools = 0  # initialize scraping counter\n",
    "\n",
    "keys = sample[0].keys()  # define keys for writing function\n",
    "fname = \"../data/final_schools.csv\"  # define file name for writing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for school in sample[:2]:\n",
    "    print(school[\"URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to call the above function and actually scrape these things!\n",
    "# \n",
    "for school in tqdm(sample): # loop through list of schools (sample)\n",
    "    if school[\"URL\"] == \"\":  # if URL is missing, fill that in by scraping\n",
    "        numschools += 1\n",
    "        school[\"QUERY_RANKING\"], school[\"URL\"] = getURL(school[\"SCH_NAME\"], school[\"ADDRESSES\"], bad_sites) # school[\"MANUAL_URL\"]\n",
    "    \n",
    "    else:\n",
    "        if school[\"URL\"]:\n",
    "            pass  # If URL exists, don't bother scraping it again\n",
    "\n",
    "        else:  # If URL hasn't been defined, then scrape it!\n",
    "            numschools += 1\n",
    "            school[\"QUERY_RANKING\"], school[\"URL\"] = \"\", \"\" # start with empty strings\n",
    "            school[\"QUERY_RANKING\"], school[\"URL\"] = getURL(school[\"SCH_NAME\"], school[\"ADDRESSES\"], bad_sites) # school[\"MANUAL_URL\"]\n",
    "\n",
    "print(\"\\n\\nURLs discovered for \" + str(numschools) + \" schools.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summer 2017, the above approach works to get a good URL for 6,677 out of the 6,752 schools in this data set. Not bad! <br>\n",
    "For some reason, the Google search algorithm (method #2) is less likely to work after passing from the Google Places API. <br>\n",
    "To fill in for the remaining 75, let's skip the function's layers of code and just call the google search function by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for school in sample:\n",
    "    school[\"SEARCH\"] = school[\"SCH_NAME\"] + \" \" + school[\"ADDRESSES\"]\n",
    "    if school[\"URL\"] == \"\":\n",
    "        k = 0  # initialize counter for number of URLs skipped\n",
    "        school[\"QUERY_RANKING\"] = \"\"\n",
    "\n",
    "        \n",
    "        print(\"Scraping URL for \" + school[\"SEARCH\"] + \"...\")\n",
    "        urls_list = list(search(school[\"SEARCH\"], stop=20, pause=10.0))\n",
    "        print(\"  URLs list collected successfully!\")\n",
    "\n",
    "        for url in urls_list:\n",
    "            if any(domain in url for domain in bad_sites):\n",
    "                k+=1    # If this url is in bad_sites_list, add 1 to counter and move on\n",
    "                # print(\"  Bad site detected. Moving on.\")\n",
    "            else:\n",
    "                good_url = url\n",
    "                print(\"    Success! URL obtained by Google search with \" + str(k) + \" bad URLs avoided.\")\n",
    "\n",
    "                school[\"URL\"] = good_url\n",
    "                school[\"QUERY_RANKING\"] = k + 1\n",
    "                \n",
    "                count_left(sample, 'URL')\n",
    "                dicts_to_csv(sample, fname, keys)\n",
    "                print()\n",
    "                break    # Exit for loop after first good url is found                               \n",
    "                                           \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "count_left(sample, 'URL')\n",
    "dicts_to_csv(sample, fname, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample to file (can continue to load and add to it):\n",
    "count_left(sample, 'URL')\n",
    "dicts_to_csv(sample, fname, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK OUT RESULTS\n",
    "# TO DO: Make a histogram of 'NUM_BAD_URLS'\n",
    "# systematic way to look at problem URLs (with k > 0)?\n",
    "\n",
    "f = 0\n",
    "for school in sample:\n",
    "    if int(school['NUM_BAD_URLS']) > 14:\n",
    "        print(school[\"SEARCH\"], \"\\n\", school[\"URL\"], \"\\n\")\n",
    "        f += 1\n",
    "\n",
    "print(str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirroring websites with `wget`<a id='wget'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wget` is classic (circa 1996, but still updated) [free software](https://www.gnu.org/philosophy/free-sw) in shell for non-interactively downloading web content. It's often used for basic one-time downloads, like `curl` also does for shell or `urllib.urlretrieve` does in-house for Python. But where `wget` really shines is in its extensive customization, including retrying failed connections, following links, and duplicating a remote website's files and structure to the point of having an identical local copy (website mirroring). \n",
    "\n",
    "Let's try using the nice Python wrapper for `wget` to download the MDI News page nested in the McCourt School for Public Policy site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'download.wget'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget \n",
    "wget.download(url='https://mccourt.georgetown.edu/research/mdi-news/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out the contents of this (rather poorly named) file using the Jupyter interface in the previous tab. \n",
    "\n",
    "We got some HTML--cool! But what if we want something clickable and interactive? This is easiest to do with `wget` run via its native shell, rather than this simple Python wrapper--which also doesn't allow for `get`'s more advanced functionality. We can use the helpful `!` prefix to run shell commands straight from this notebook. \n",
    "\n",
    "Let's make a new `wget` request to download a version of the same page that's easier to see in your browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:43:11--  https://mccourt.georgetown.edu/research/mdi-news/\n",
      "Resolving mccourt.georgetown.edu (mccourt.georgetown.edu)... 23.185.0.1, 2620:12a:8001::1, 2620:12a:8000::1\n",
      "Connecting to mccourt.georgetown.edu (mccourt.georgetown.edu)|23.185.0.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 157499 (154K) [text/html]\n",
      "Saving to: ‘index.html.1’\n",
      "\n",
      "index.html.1        100%[===================>] 153.81K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-04-23 14:43:12 (8.80 MB/s) - ‘index.html.1’ saved [157499/157499]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://mccourt.georgetown.edu/research/mdi-news/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your Jupyter browser to check out the results: just click on `index.html` in your current folder (probably this is `day-1/`) to view the page. What do you notice? How does it compare to viewing https://mccourt.georgetown.edu/research/mdi-news/ in your browser? Try clicking the links. Where can you go on the actual page that your local copy can't show you? Do you have local copies of the images?\n",
    "\n",
    "You might have noticed that we only ended up with some HTML--we didn't download any of the files associated with the webpage. So, this isn't a true copy; we couldn't host the page ourselves, analyze its images, or easily use its content for purposes other than viewing. How do we mirror the full site?\n",
    "\n",
    "To do this, we need only the `page-requisites` option, which makes sure to download all the resources needed to render the page in a browser: that means CSS, javascript, image files, etc. To keep from overloading the server, let's pause for a few seconds in between downloads using the `--wait` option. \n",
    "\n",
    "Let's use some other features as well for politeness and subtlety (i.e. to avoid getting blocked). Here is explanation for all of them:\n",
    "\n",
    "```shell\n",
    "--page-requisites             Grabs all of the linked resources necessary to render the page (images, CSS, javascript, etc.)\n",
    "--wait                        Pauses between downloads (in seconds)\n",
    "--tries=3                     Retries failed downloads 3 times\n",
    "--user-agent=Mozilla          Makes wget look like a Mozilla browser by masking its user agent\n",
    "--header=\"Accept:text/html\"   Sends header with each HTML request, looks more browser-ish\n",
    "--no-check-certificate        Doesnt check authenticity of website server (use only with trusted websites!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:16:52--  https://mccourt.georgetown.edu/research/mdi-news/\n",
      "Resolving mccourt.georgetown.edu (mccourt.georgetown.edu)... 23.185.0.1, 2620:12a:8001::1, 2620:12a:8000::1\n",
      "Connecting to mccourt.georgetown.edu (mccourt.georgetown.edu)|23.185.0.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 157499 (154K) [text/html]\n",
      "Saving to: ‘index.html’\n",
      "\n",
      "index.html          100%[===================>] 153.81K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-04-23 14:16:52 (9.30 MB/s) - ‘index.html’ saved [157499/157499]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --page-requisites --wait=2 --tries=3 --user-agent=Mozilla --header=\"Accept:text/html\" --no-check-certificate \\\n",
    "    https://mccourt.georgetown.edu/research/mdi-news/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the results--what's similar and whats different? See `/research/mdi-news/` for the `index.html` (sometimes this is `default.html`) page we saw earlier. \n",
    "\n",
    "`wget` has a rich array of options. Here are some of the most useful ones in addition to those above:\n",
    "\n",
    "```shell\n",
    "--mirror                      Downloads a full website and makes available for local viewing\n",
    "--recursive                   Recursively downloads files and follows links\n",
    "--no-parent \t\t          Does not follow links above hierarchical level of input URL\n",
    "--convert-links \t          Turns links into local links as appropriate\n",
    "--accept                      Download only file suffixes in this list (e.g., .html)\n",
    "--execute robots=off          Turns off automatic robots.txt checking, preventing server privacy exclusions\n",
    "--random-wait                 Randomizes the defined wait period to between .5 and 1.5x that value\n",
    "--background\t\t          For a huge download, put the download in background\n",
    "--spider                      Determines whether the remote file exist at the destination (mimics web spiders)\n",
    "--domains   \t\t          Downloads only only PDF files from specific domains\n",
    "--user --password   \t\t  Downloads files from password protected sites\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Download only `.html` files from https://mccourt.georgetown.edu/research/ and links below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 15:03:17--  https://mccourt.georgetown.edu/research/\n",
      "Resolving mccourt.georgetown.edu (mccourt.georgetown.edu)... 23.185.0.1, 2620:12a:8000::1, 2620:12a:8001::1\n",
      "Connecting to mccourt.georgetown.edu (mccourt.georgetown.edu)|23.185.0.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 157616 (154K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 153.92K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-04-23 15:03:18 (9.33 MB/s) - ‘mccourt.georgetown.edu/research/index.html’ saved [157616/157616]\n",
      "\n",
      "Loading robots.txt; please ignore errors.\n",
      "--2021-04-23 15:03:20--  https://mccourt.georgetown.edu/robots.txt\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 116 [text/plain]\n",
      "Saving to: ‘mccourt.georgetown.edu/robots.txt.tmp’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>]     116  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 15:03:20 (3.37 MB/s) - ‘mccourt.georgetown.edu/robots.txt.tmp’ saved [116/116]\n",
      "\n",
      "--2021-04-23 15:03:22--  https://mccourt.georgetown.edu/research/featured-publications/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154027 (150K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/featured-publications/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 150.42K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2021-04-23 15:03:22 (17.1 MB/s) - ‘mccourt.georgetown.edu/research/featured-publications/index.html’ saved [154027/154027]\n",
      "\n",
      "--2021-04-23 15:03:24--  https://mccourt.georgetown.edu/research/mccourt-centers/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 145651 (142K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/mccourt-centers/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 142.24K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:25 (39.3 MB/s) - ‘mccourt.georgetown.edu/research/mccourt-centers/index.html’ saved [145651/145651]\n",
      "\n",
      "--2021-04-23 15:03:27--  https://mccourt.georgetown.edu/research/research-data-center/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 139956 (137K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 136.68K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:27 (32.8 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/index.html’ saved [139956/139956]\n",
      "\n",
      "--2021-04-23 15:03:29--  https://mccourt.georgetown.edu/research/the-massive-data-institute/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 155731 (152K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 152.08K  --.-KB/s    in 0.005s  \n",
      "\n",
      "2021-04-23 15:03:30 (31.6 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/index.html’ saved [155731/155731]\n",
      "\n",
      "--2021-04-23 15:03:32--  https://mccourt.georgetown.edu/research/faculty-seminars/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 157025 (153K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/faculty-seminars/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 153.34K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:33 (34.5 MB/s) - ‘mccourt.georgetown.edu/research/faculty-seminars/index.html’ saved [157025/157025]\n",
      "\n",
      "--2021-04-23 15:03:35--  https://mccourt.georgetown.edu/research/mccourt-centers/research-center-directors/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 155073 (151K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/mccourt-centers/research-center-directors/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 151.44K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:35 (36.8 MB/s) - ‘mccourt.georgetown.edu/research/mccourt-centers/research-center-directors/index.html’ saved [155073/155073]\n",
      "\n",
      "--2021-04-23 15:03:37--  https://mccourt.georgetown.edu/research/mdi-news/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 157499 (154K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/mdi-news/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 153.81K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:38 (36.8 MB/s) - ‘mccourt.georgetown.edu/research/mdi-news/index.html’ saved [157499/157499]\n",
      "\n",
      "--2021-04-23 15:03:40--  https://mccourt.georgetown.edu/research/the-massive-data-institute/resources/dp-resources/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 143363 (140K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/resources/dp-resources/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 140.00K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:40 (34.1 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/resources/dp-resources/index.html’ saved [143363/143363]\n",
      "\n",
      "--2021-04-23 15:03:42--  https://mccourt.georgetown.edu/research/research-data-center/getting-started/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 144786 (141K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/getting-started/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 141.39K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:43 (38.6 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/getting-started/index.html’ saved [144786/144786]\n",
      "\n",
      "--2021-04-23 15:03:45--  https://mccourt.georgetown.edu/research/research-data-center/administrative-data-metadata/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 139980 (137K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/administrative-data-metadata/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 136.70K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2021-04-23 15:03:45 (43.7 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/administrative-data-metadata/index.html’ saved [139980/139980]\n",
      "\n",
      "--2021-04-23 15:03:47--  https://mccourt.georgetown.edu/research/research-data-center/research/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 140145 (137K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/research/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 136.86K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:48 (35.2 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/research/index.html’ saved [140145/140145]\n",
      "\n",
      "--2021-04-23 15:03:50--  https://mccourt.georgetown.edu/research/research-data-center/data-and-software-available/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 156464 (153K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/data-and-software-available/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 152.80K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:50 (37.1 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/data-and-software-available/index.html’ saved [156464/156464]\n",
      "\n",
      "--2021-04-23 15:03:52--  https://mccourt.georgetown.edu/research/research-data-center/contact-us/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 137306 (134K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/research-data-center/contact-us/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 134.09K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-23 15:03:53 (30.2 MB/s) - ‘mccourt.georgetown.edu/research/research-data-center/contact-us/index.html’ saved [137306/137306]\n",
      "\n",
      "--2021-04-23 15:03:55--  https://mccourt.georgetown.edu/research/the-massive-data-institute/supporting-ethical-data-governance/research-data-center/contact-us/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-23 15:03:55 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-23 15:03:57--  https://mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research/\n",
      "Connecting to mccourt.georgetown.edu (mccourt.georgetown.edu)|23.185.0.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 143331 (140K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 139.97K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-04-23 15:03:58 (8.29 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research/index.html’ saved [143331/143331]\n",
      "\n",
      "--2021-04-23 15:04:00--  https://mccourt.georgetown.edu/research/the-massive-data-institute/shape-the-policy-conversation/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 142753 (139K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/shape-the-policy-conversation/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 139.41K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2021-04-23 15:04:00 (17.3 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/shape-the-policy-conversation/index.html’ saved [142753/142753]\n",
      "\n",
      "--2021-04-23 15:04:02--  https://mccourt.georgetown.edu/research/the-massive-data-institute/improving-data-driven-policy-making/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 143621 (140K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/improving-data-driven-policy-making/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 140.25K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:02 (119 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/improving-data-driven-policy-making/index.html’ saved [143621/143621]\n",
      "\n",
      "--2021-04-23 15:04:04--  https://mccourt.georgetown.edu/research/the-massive-data-institute/about-mdi/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 143501 (140K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/about-mdi/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 140.14K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2021-04-23 15:04:05 (64.9 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/about-mdi/index.html’ saved [143501/143501]\n",
      "\n",
      "--2021-04-23 15:04:07--  https://mccourt.georgetown.edu/research/the-massive-data-institute/funding-opportunities/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 152029 (148K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/funding-opportunities/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 148.47K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:07 (132 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/funding-opportunities/index.html’ saved [152029/152029]\n",
      "\n",
      "--2021-04-23 15:04:09--  https://mccourt.georgetown.edu/research/the-massive-data-institute/compute-infrastructure/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 143447 (140K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/compute-infrastructure/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 140.08K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:10 (121 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/compute-infrastructure/index.html’ saved [143447/143447]\n",
      "\n",
      "--2021-04-23 15:04:12--  https://mccourt.georgetown.edu/research/the-massive-data-institute/resources/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 142043 (139K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/resources/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 138.71K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:12 (163 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/resources/index.html’ saved [142043/142043]\n",
      "\n",
      "--2021-04-23 15:04:14--  https://mccourt.georgetown.edu/research/the-massive-data-institute/mdi-data-blending/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 141487 (138K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-data-blending/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 138.17K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:14 (139 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-data-blending/index.html’ saved [141487/141487]\n",
      "\n",
      "--2021-04-23 15:04:16--  https://mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research-and-collaborations/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 147895 (144K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research-and-collaborations/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 144.43K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:17 (129 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research-and-collaborations/index.html’ saved [147895/147895]\n",
      "\n",
      "--2021-04-23 15:04:19--  https://mccourt.georgetown.edu/research/the-massive-data-institute/mdi-conferences-and-panels/\n",
      "Reusing existing connection to mccourt.georgetown.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 149002 (146K) [text/html]\n",
      "Saving to: ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-conferences-and-panels/index.html’\n",
      "\n",
      "mccourt.georgetown. 100%[===================>] 145.51K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-04-23 15:04:19 (166 MB/s) - ‘mccourt.georgetown.edu/research/the-massive-data-institute/mdi-conferences-and-panels/index.html’ saved [149002/149002]\n",
      "\n",
      "FINISHED --2021-04-23 15:04:19--\n",
      "Total wall clock time: 1m 2s\n",
      "Downloaded: 25 files, 3.4M in 0.1s (31.8 MB/s)\n",
      "Converting links in mccourt.georgetown.edu/research/mdi-news/index.html... 17-8\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/administrative-data-metadata/index.html... 28-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research/index.html... 35-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/mdi-research-and-collaborations/index.html... 32-8\n",
      "Converting links in mccourt.georgetown.edu/research/index.html... 20-12\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/research/index.html... 28-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/improving-data-driven-policy-making/index.html... 32-10\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/index.html... 33-8\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/getting-started/index.html... 28-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/mdi-data-blending/index.html... 31-8\n",
      "Converting links in mccourt.georgetown.edu/research/featured-publications/index.html... 17-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/shape-the-policy-conversation/index.html... 32-8\n",
      "Converting links in mccourt.georgetown.edu/research/mccourt-centers/research-center-directors/index.html... 17-8\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/contact-us/index.html... 28-8\n",
      "Converting links in mccourt.georgetown.edu/research/faculty-seminars/index.html... 33-8\n",
      "Converting links in mccourt.georgetown.edu/research/research-data-center/data-and-software-available/index.html... 28-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/about-mdi/index.html... 24-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/funding-opportunities/index.html... 35-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/index.html... 24-8\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/compute-infrastructure/index.html... 32-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/resources/dp-resources/index.html... 23-8\r\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/resources/index.html... 34-8\r\n",
      "Converting links in mccourt.georgetown.edu/research/the-massive-data-institute/mdi-conferences-and-panels/index.html... 32-8\r\n",
      "Converting links in mccourt.georgetown.edu/research/mccourt-centers/index.html... 24-8\r\n",
      "Converted links in 24 files in 0.06 seconds.\r\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "!wget --accept .html --recursive --no-parent --page-requisites --convert-links --wait=2 --tries=3 \\\n",
    "    --user-agent=Mozilla --header=\"Accept:text/html\" --no-check-certificate \\\n",
    "    https://mccourt.georgetown.edu/research/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use advanced options for `wget` (listed above) to mirror a website you use often. Be sure to use a polite `--wait` and avoid downloading anything with massive numbers of links, files, or pages (e.g., don't try YouTube.com or Wikipedia.com). If you want to download a segment or specific page within a website (e.g., a single YouTube channel or Wikipedia page), use the `--recursive` option with `--no-parent` (to follow only links within the input URL).\n",
    "\n",
    "While you let `wget` run, read more about it on its [manual](https://www.gnu.org/software/wget/manual/wget.html) and see other examples of `wget` usage [here](https://gist.github.com/bueckl/bd0a1e7a30bc8e2eeefd) and [here](https://phoenixnap.com/kb/wget-command-with-examples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:39:08--  https://www.gnu.org/software/wget/\n",
      "Resolving www.gnu.org (www.gnu.org)... 209.51.188.148, 2001:470:142:3::a\n",
      "Connecting to www.gnu.org (www.gnu.org)|209.51.188.148|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/index.html’\n",
      "\n",
      "www.gnu.org/softwar     [ <=>                ]  10.46K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:08 (363 KB/s) - ‘www.gnu.org/software/wget/index.html’ saved [10708]\n",
      "\n",
      "Loading robots.txt; please ignore errors.\n",
      "--2021-04-23 14:39:10--  https://www.gnu.org/robots.txt\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1135 (1.1K) [text/plain]\n",
      "Saving to: ‘www.gnu.org/robots.txt’\n",
      "\n",
      "www.gnu.org/robots. 100%[===================>]   1.11K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:10 (29.2 MB/s) - ‘www.gnu.org/robots.txt’ saved [1135/1135]\n",
      "\n",
      "--2021-04-23 14:39:12--  https://www.gnu.org/mini.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3490 (3.4K) [text/css]\n",
      "Saving to: ‘www.gnu.org/mini.css’\n",
      "\n",
      "www.gnu.org/mini.cs 100%[===================>]   3.41K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:12 (52.4 MB/s) - ‘www.gnu.org/mini.css’ saved [3490/3490]\n",
      "\n",
      "--2021-04-23 14:39:14--  https://www.gnu.org/layout.min.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18112 (18K) [text/css]\n",
      "Saving to: ‘www.gnu.org/layout.min.css’\n",
      "\n",
      "www.gnu.org/layout. 100%[===================>]  17.69K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-04-23 14:39:14 (626 KB/s) - ‘www.gnu.org/layout.min.css’ saved [18112/18112]\n",
      "\n",
      "--2021-04-23 14:39:16--  https://www.gnu.org/print.min.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1714 (1.7K) [text/css]\n",
      "Saving to: ‘www.gnu.org/print.min.css’\n",
      "\n",
      "www.gnu.org/print.m 100%[===================>]   1.67K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:17 (42.8 MB/s) - ‘www.gnu.org/print.min.css’ saved [1714/1714]\n",
      "\n",
      "--2021-04-23 14:39:19--  https://www.gnu.org/graphics/heckert_gnu.transp.small.png\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7677 (7.5K) [image/png]\n",
      "Saving to: ‘www.gnu.org/graphics/heckert_gnu.transp.small.png’\n",
      "\n",
      "www.gnu.org/graphic 100%[===================>]   7.50K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:19 (156 MB/s) - ‘www.gnu.org/graphics/heckert_gnu.transp.small.png’ saved [7677/7677]\n",
      "\n",
      "--2021-04-23 14:39:21--  https://www.gnu.org/graphics/icons/search.png\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1136 (1.1K) [image/png]\n",
      "Saving to: ‘www.gnu.org/graphics/icons/search.png’\n",
      "\n",
      "www.gnu.org/graphic 100%[===================>]   1.11K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:21 (28.1 MB/s) - ‘www.gnu.org/graphics/icons/search.png’ saved [1136/1136]\n",
      "\n",
      "--2021-04-23 14:39:23--  https://www.gnu.org/software/wget/manual/\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/index.html’\n",
      "\n",
      "www.gnu.org/softwar     [ <=>                ]   7.92K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:23 (286 KB/s) - ‘www.gnu.org/software/wget/manual/index.html’ saved [8112]\n",
      "\n",
      "--2021-04-23 14:39:25--  https://www.gnu.org/graphics/fsf-logo-notext-small.png\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2494 (2.4K) [image/png]\n",
      "Saving to: ‘www.gnu.org/graphics/fsf-logo-notext-small.png’\n",
      "\n",
      "www.gnu.org/graphic 100%[===================>]   2.44K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:25 (94.5 MB/s) - ‘www.gnu.org/graphics/fsf-logo-notext-small.png’ saved [2494/2494]\n",
      "\n",
      "--2021-04-23 14:39:27--  https://www.gnu.org/software/wget/manual/wget.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 345663 (338K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>] 337.56K  --.-KB/s    in 0.1s    \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:27 (2.83 MB/s) - ‘www.gnu.org/software/wget/manual/wget.html’ saved [345663/345663]\n",
      "\n",
      "--2021-04-23 14:39:29--  https://www.gnu.org/software/wget/manual/html_node/index.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12386 (12K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/index.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  12.10K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:29 (128 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/index.html’ saved [12386/12386]\n",
      "\n",
      "--2021-04-23 14:39:31--  https://www.gnu.org/software/wget/manual/wget.html.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 83121 (81K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.html.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  81.17K  --.-KB/s    in 0.06s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:31 (1.39 MB/s) - ‘www.gnu.org/software/wget/manual/wget.html.gz’ saved [83121/83121]\n",
      "\n",
      "--2021-04-23 14:39:33--  https://www.gnu.org/software/wget/manual/wget.html_node.tar.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 95950 (94K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.html_node.tar.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  93.70K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-04-23 14:39:33 (1.56 MB/s) - ‘www.gnu.org/software/wget/manual/wget.html_node.tar.gz’ saved [95950/95950]\n",
      "\n",
      "--2021-04-23 14:39:35--  https://www.gnu.org/software/wget/manual/wget.info.tar.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 72345 (71K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.info.tar.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  70.65K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-04-23 14:39:35 (1.20 MB/s) - ‘www.gnu.org/software/wget/manual/wget.info.tar.gz’ saved [72345/72345]\n",
      "\n",
      "--2021-04-23 14:39:37--  https://www.gnu.org/software/wget/manual/wget.txt\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 226204 (221K) [text/plain]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.txt’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>] 220.90K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-04-23 14:39:37 (1.91 MB/s) - ‘www.gnu.org/software/wget/manual/wget.txt’ saved [226204/226204]\n",
      "\n",
      "--2021-04-23 14:39:39--  https://www.gnu.org/software/wget/manual/wget.txt.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 70458 (69K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.txt.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  68.81K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-04-23 14:39:39 (1.19 MB/s) - ‘www.gnu.org/software/wget/manual/wget.txt.gz’ saved [70458/70458]\n",
      "\n",
      "--2021-04-23 14:39:41--  https://www.gnu.org/software/wget/manual/wget.dvi.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 118092 (115K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.dvi.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>] 115.32K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2021-04-23 14:39:42 (1.33 MB/s) - ‘www.gnu.org/software/wget/manual/wget.dvi.gz’ saved [118092/118092]\n",
      "\n",
      "--2021-04-23 14:39:44--  https://www.gnu.org/software/wget/manual/wget.pdf\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 454439 (444K) [application/pdf]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.pdf’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>] 443.79K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-04-23 14:39:44 (3.05 MB/s) - ‘www.gnu.org/software/wget/manual/wget.pdf’ saved [454439/454439]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:39:46--  https://www.gnu.org/software/wget/manual/wget.texi.tar.gz\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68053 (66K) [application/x-gzip]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/wget.texi.tar.gz’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  66.46K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-04-23 14:39:46 (1.13 MB/s) - ‘www.gnu.org/software/wget/manual/wget.texi.tar.gz’ saved [68053/68053]\n",
      "\n",
      "--2021-04-23 14:39:48--  https://www.gnu.org/software/wget/manual/dir.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-23 14:39:48 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-23 14:39:50--  https://www.gnu.org/software/gnulib/manual.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2083 (2.0K) [text/css]\n",
      "Saving to: ‘www.gnu.org/software/gnulib/manual.css’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   2.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:39:50 (47.9 MB/s) - ‘www.gnu.org/software/gnulib/manual.css’ saved [2083/2083]\n",
      "\n",
      "--2021-04-23 14:39:52--  https://www.gnu.org/software/wget/manual/https//www.gnu.org/software/wget/\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-23 14:39:52 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-23 14:39:54--  https://www.gnu.org/software/wget/manual/html_node/Concept-Index.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64717 (63K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Concept-Index.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  63.20K  --.-KB/s    in 0.06s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:54 (1.08 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Concept-Index.html’ saved [64717/64717]\n",
      "\n",
      "--2021-04-23 14:39:56--  https://www.gnu.org/software/wget/manual/dir/index.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-23 14:39:56 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-23 14:39:58--  https://www.gnu.org/software/wget/manual/html_node/Overview.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6903 (6.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Overview.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.74K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:39:58 (146 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Overview.html’ saved [6903/6903]\n",
      "\n",
      "--2021-04-23 14:40:00--  https://www.gnu.org/software/wget/manual/html_node/Invoking.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5662 (5.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Invoking.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   5.53K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:00 (140 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Invoking.html’ saved [5662/5662]\n",
      "\n",
      "--2021-04-23 14:40:02--  https://www.gnu.org/software/wget/manual/html_node/URL-Format.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6894 (6.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/URL-Format.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.73K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:02 (149 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/URL-Format.html’ saved [6894/6894]\n",
      "\n",
      "--2021-04-23 14:40:04--  https://www.gnu.org/software/wget/manual/html_node/Option-Syntax.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6653 (6.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Option-Syntax.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.50K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:04 (140 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Option-Syntax.html’ saved [6653/6653]\n",
      "\n",
      "--2021-04-23 14:40:06--  https://www.gnu.org/software/wget/manual/html_node/Basic-Startup-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4050 (4.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Basic-Startup-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.96K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:06 (85.9 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Basic-Startup-Options.html’ saved [4050/4050]\n",
      "\n",
      "--2021-04-23 14:40:08--  https://www.gnu.org/software/wget/manual/html_node/Logging-and-Input-File-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11411 (11K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Logging-and-Input-File-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  11.14K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:08 (54.6 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Logging-and-Input-File-Options.html’ saved [11411/11411]\n",
      "\n",
      "--2021-04-23 14:40:10--  https://www.gnu.org/software/wget/manual/html_node/Download-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40722 (40K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Download-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  39.77K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:10 (1.33 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Download-Options.html’ saved [40722/40722]\n",
      "\n",
      "--2021-04-23 14:40:12--  https://www.gnu.org/software/wget/manual/html_node/Directory-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6857 (6.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Directory-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.70K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:12 (164 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Directory-Options.html’ saved [6857/6857]\n",
      "\n",
      "--2021-04-23 14:40:14--  https://www.gnu.org/software/wget/manual/html_node/HTTP-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28357 (28K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/HTTP-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  27.69K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:14 (988 KB/s) - ‘www.gnu.org/software/wget/manual/html_node/HTTP-Options.html’ saved [28357/28357]\n",
      "\n",
      "--2021-04-23 14:40:16--  https://www.gnu.org/software/wget/manual/html_node/HTTPS-_0028SSL_002fTLS_0029-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18507 (18K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/HTTPS-_0028SSL_002fTLS_0029-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  18.07K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:16 (640 KB/s) - ‘www.gnu.org/software/wget/manual/html_node/HTTPS-_0028SSL_002fTLS_0029-Options.html’ saved [18507/18507]\n",
      "\n",
      "--2021-04-23 14:40:18--  https://www.gnu.org/software/wget/manual/html_node/FTP-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11483 (11K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/FTP-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  11.21K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:19 (121 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/FTP-Options.html’ saved [11483/11483]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:40:21--  https://www.gnu.org/software/wget/manual/html_node/Recursive-Retrieval-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15938 (16K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Recursive-Retrieval-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  15.56K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:21 (566 KB/s) - ‘www.gnu.org/software/wget/manual/html_node/Recursive-Retrieval-Options.html’ saved [15938/15938]\n",
      "\n",
      "--2021-04-23 14:40:23--  https://www.gnu.org/software/wget/manual/html_node/Recursive-Accept_002fReject-Options.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9306 (9.1K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Recursive-Accept_002fReject-Options.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   9.09K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:23 (64.5 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Recursive-Accept_002fReject-Options.html’ saved [9306/9306]\n",
      "\n",
      "--2021-04-23 14:40:25--  https://www.gnu.org/software/wget/manual/html_node/Exit-Status.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4000 (3.9K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Exit-Status.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.91K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:25 (91.8 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Exit-Status.html’ saved [4000/4000]\n",
      "\n",
      "--2021-04-23 14:40:27--  https://www.gnu.org/software/wget/manual/html_node/Recursive-Download.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6946 (6.8K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Recursive-Download.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.78K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:27 (145 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Recursive-Download.html’ saved [6946/6946]\n",
      "\n",
      "--2021-04-23 14:40:29--  https://www.gnu.org/software/wget/manual/html_node/Following-Links.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4481 (4.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Following-Links.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.38K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:29 (65.0 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Following-Links.html’ saved [4481/4481]\n",
      "\n",
      "--2021-04-23 14:40:31--  https://www.gnu.org/software/wget/manual/html_node/Spanning-Hosts.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5907 (5.8K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Spanning-Hosts.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   5.77K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:31 (130 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Spanning-Hosts.html’ saved [5907/5907]\n",
      "\n",
      "--2021-04-23 14:40:33--  https://www.gnu.org/software/wget/manual/html_node/Types-of-Files.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9740 (9.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Types-of-Files.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   9.51K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:33 (81.6 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Types-of-Files.html’ saved [9740/9740]\n",
      "\n",
      "--2021-04-23 14:40:35--  https://www.gnu.org/software/wget/manual/html_node/Directory_002dBased-Limits.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7714 (7.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Directory_002dBased-Limits.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   7.53K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:35 (78.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Directory_002dBased-Limits.html’ saved [7714/7714]\n",
      "\n",
      "--2021-04-23 14:40:37--  https://www.gnu.org/software/wget/manual/html_node/Relative-Links.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3825 (3.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Relative-Links.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:37 (20.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Relative-Links.html’ saved [3825/3825]\n",
      "\n",
      "--2021-04-23 14:40:39--  https://www.gnu.org/software/wget/manual/html_node/FTP-Links.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3810 (3.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/FTP-Links.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.72K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:39 (30.1 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/FTP-Links.html’ saved [3810/3810]\n",
      "\n",
      "--2021-04-23 14:40:41--  https://www.gnu.org/software/wget/manual/html_node/Time_002dStamping.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5577 (5.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Time_002dStamping.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   5.45K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:41 (140 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Time_002dStamping.html’ saved [5577/5577]\n",
      "\n",
      "--2021-04-23 14:40:43--  https://www.gnu.org/software/wget/manual/html_node/Time_002dStamping-Usage.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5658 (5.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Time_002dStamping-Usage.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   5.53K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:43 (154 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Time_002dStamping-Usage.html’ saved [5658/5658]\n",
      "\n",
      "--2021-04-23 14:40:45--  https://www.gnu.org/software/wget/manual/html_node/HTTP-Time_002dStamping-Internals.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4870 (4.8K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/HTTP-Time_002dStamping-Internals.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.76K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:45 (76.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/HTTP-Time_002dStamping-Internals.html’ saved [4870/4870]\n",
      "\n",
      "--2021-04-23 14:40:47--  https://www.gnu.org/software/wget/manual/html_node/FTP-Time_002dStamping-Internals.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4461 (4.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/FTP-Time_002dStamping-Internals.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.36K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:47 (111 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/FTP-Time_002dStamping-Internals.html’ saved [4461/4461]\n",
      "\n",
      "--2021-04-23 14:40:49--  https://www.gnu.org/software/wget/manual/html_node/Startup-File.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4442 (4.3K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Startup-File.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.34K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:49 (98.8 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Startup-File.html’ saved [4442/4442]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:40:51--  https://www.gnu.org/software/wget/manual/html_node/Wgetrc-Location.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3677 (3.6K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Location.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.59K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:51 (88.9 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Location.html’ saved [3677/3677]\n",
      "\n",
      "--2021-04-23 14:40:53--  https://www.gnu.org/software/wget/manual/html_node/Wgetrc-Syntax.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3731 (3.6K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Syntax.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.64K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:53 (86.0 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Syntax.html’ saved [3731/3731]\n",
      "\n",
      "--2021-04-23 14:40:55--  https://www.gnu.org/software/wget/manual/html_node/Wgetrc-Commands.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27543 (27K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Commands.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  26.90K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:55 (944 KB/s) - ‘www.gnu.org/software/wget/manual/html_node/Wgetrc-Commands.html’ saved [27543/27543]\n",
      "\n",
      "--2021-04-23 14:40:57--  https://www.gnu.org/software/wget/manual/html_node/Sample-Wgetrc.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8653 (8.5K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Sample-Wgetrc.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   8.45K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:57 (58.9 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Sample-Wgetrc.html’ saved [8653/8653]\n",
      "\n",
      "--2021-04-23 14:40:59--  https://www.gnu.org/software/wget/manual/html_node/Examples.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3478 (3.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Examples.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.40K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:40:59 (74.8 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Examples.html’ saved [3478/3478]\n",
      "\n",
      "--2021-04-23 14:41:01--  https://www.gnu.org/software/wget/manual/html_node/Simple-Usage.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4390 (4.3K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Simple-Usage.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.29K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:01 (96.7 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Simple-Usage.html’ saved [4390/4390]\n",
      "\n",
      "--2021-04-23 14:41:03--  https://www.gnu.org/software/wget/manual/html_node/Advanced-Usage.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7968 (7.8K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Advanced-Usage.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   7.78K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:03 (159 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Advanced-Usage.html’ saved [7968/7968]\n",
      "\n",
      "--2021-04-23 14:41:05--  https://www.gnu.org/software/wget/manual/html_node/Very-Advanced-Usage.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4496 (4.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Very-Advanced-Usage.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.39K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:05 (91.0 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Very-Advanced-Usage.html’ saved [4496/4496]\n",
      "\n",
      "--2021-04-23 14:41:07--  https://www.gnu.org/software/wget/manual/html_node/Various.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4473 (4.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Various.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.37K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:07 (117 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Various.html’ saved [4473/4473]\n",
      "\n",
      "--2021-04-23 14:41:09--  https://www.gnu.org/software/wget/manual/html_node/Proxies.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6815 (6.7K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Proxies.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.66K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:09 (38.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Proxies.html’ saved [6815/6815]\n",
      "\n",
      "--2021-04-23 14:41:11--  https://www.gnu.org/software/wget/manual/html_node/Distribution.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3075 (3.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Distribution.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.00K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:11 (73.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Distribution.html’ saved [3075/3075]\n",
      "\n",
      "--2021-04-23 14:41:13--  https://www.gnu.org/software/wget/manual/html_node/Web-Site.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3057 (3.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Web-Site.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   2.99K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:13 (79.2 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Web-Site.html’ saved [3057/3057]\n",
      "\n",
      "--2021-04-23 14:41:15--  https://www.gnu.org/software/wget/manual/html_node/Mailing-Lists.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5358 (5.2K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Mailing-Lists.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   5.23K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:16 (110 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Mailing-Lists.html’ saved [5358/5358]\n",
      "\n",
      "--2021-04-23 14:41:18--  https://www.gnu.org/software/wget/manual/html_node/Internet-Relay-Chat.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3083 (3.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Internet-Relay-Chat.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.01K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:18 (68.3 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Internet-Relay-Chat.html’ saved [3083/3083]\n",
      "\n",
      "--2021-04-23 14:41:20--  https://www.gnu.org/software/wget/manual/html_node/Reporting-Bugs.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6151 (6.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Reporting-Bugs.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   6.01K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:20 (150 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Reporting-Bugs.html’ saved [6151/6151]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 14:41:22--  https://www.gnu.org/software/wget/manual/html_node/Portability.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4458 (4.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Portability.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   4.35K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:22 (114 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Portability.html’ saved [4458/4458]\n",
      "\n",
      "--2021-04-23 14:41:24--  https://www.gnu.org/software/wget/manual/html_node/Signals.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3420 (3.3K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Signals.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.34K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:24 (73.4 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Signals.html’ saved [3420/3420]\n",
      "\n",
      "--2021-04-23 14:41:26--  https://www.gnu.org/software/wget/manual/html_node/Appendices.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3452 (3.4K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Appendices.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.37K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:26 (79.6 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Appendices.html’ saved [3452/3452]\n",
      "\n",
      "--2021-04-23 14:41:28--  https://www.gnu.org/software/wget/manual/html_node/Robot-Exclusion.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7740 (7.6K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Robot-Exclusion.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   7.56K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:28 (164 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Robot-Exclusion.html’ saved [7740/7740]\n",
      "\n",
      "--2021-04-23 14:41:30--  https://www.gnu.org/software/wget/manual/html_node/Security-Considerations.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3988 (3.9K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Security-Considerations.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.89K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:30 (97.1 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Security-Considerations.html’ saved [3988/3988]\n",
      "\n",
      "--2021-04-23 14:41:32--  https://www.gnu.org/software/wget/manual/html_node/Contributors.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9201 (9.0K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Contributors.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   8.99K  --.-KB/s    in 0.001s  \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:32 (10.8 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Contributors.html’ saved [9201/9201]\n",
      "\n",
      "--2021-04-23 14:41:34--  https://www.gnu.org/software/wget/manual/html_node/Copying-this-manual.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3127 (3.1K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/Copying-this-manual.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]   3.05K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:34 (78.1 MB/s) - ‘www.gnu.org/software/wget/manual/html_node/Copying-this-manual.html’ saved [3127/3127]\n",
      "\n",
      "--2021-04-23 14:41:36--  https://www.gnu.org/software/wget/manual/html_node/GNU-Free-Documentation-License.html\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27757 (27K) [text/html]\n",
      "Saving to: ‘www.gnu.org/software/wget/manual/html_node/GNU-Free-Documentation-License.html’\n",
      "\n",
      "www.gnu.org/softwar 100%[===================>]  27.11K  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2021-04-23 14:41:36 (955 KB/s) - ‘www.gnu.org/software/wget/manual/html_node/GNU-Free-Documentation-License.html’ saved [27757/27757]\n",
      "\n",
      "--2021-04-23 14:41:38--  https://www.gnu.org/style.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4565 (4.5K) [text/css]\n",
      "Saving to: ‘www.gnu.org/style.css’\n",
      "\n",
      "www.gnu.org/style.c 100%[===================>]   4.46K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:41:38 (108 MB/s) - ‘www.gnu.org/style.css’ saved [4565/4565]\n",
      "\n",
      "--2021-04-23 14:41:40--  https://www.gnu.org/software/wget/manual/html_node/https//www.gnu.org/software/wget/\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-04-23 14:41:40 ERROR 404: Not Found.\n",
      "\n",
      "--2021-04-23 14:41:42--  https://www.gnu.org/reset.css\n",
      "Reusing existing connection to www.gnu.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2596 (2.5K) [text/css]\n",
      "Saving to: ‘www.gnu.org/reset.css’\n",
      "\n",
      "www.gnu.org/reset.c 100%[===================>]   2.54K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 14:41:42 (64.8 MB/s) - ‘www.gnu.org/reset.css’ saved [2596/2596]\n",
      "\n",
      "FINISHED --2021-04-23 14:41:42--\n",
      "Total wall clock time: 2m 34s\n",
      "Downloaded: 72 files, 2.0M in 1.1s (1.87 MB/s)\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "!wget --mirror --recursive --no-parent --page-requisites --convert-links --wait=2 --tries=3 \\\n",
    "    --user-agent=Mozilla --header=\"Accept:text/html\" --no-check-certificate \\\n",
    "    https://www.jarenhaber.com/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
