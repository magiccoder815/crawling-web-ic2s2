{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and crawling the web\n",
    "\n",
    "This second day's workshop gives you practice scraping and crawling with modern Python tools. \n",
    "\n",
    "To review from part 1, web-scraping means “programatically going over a collection of web pages and extracting data”. Scraping is a powerful tool for working with data on the web, but it depends on knowing where the information you want is located. If you don't have specific target URLs, then you can often programmatically search for them by following links around the internet to locate content (often called *web-crawling*) or scraping them, e.g. with automated search (like we practiced in part 1). Once you have the pages you want, then you extract and parse information from these pages (often called *web-scraping*). These two steps often happen together and recursively: you crawl some stuff, but upon scraping it you realize you got the wrong websites, so you go back to crawling, which changes your scraping approach, and so on.\n",
    "\n",
    "We will start with an essential step in any web-scraping or web-crawling pipeline: parsing HTML (with `BeautifulSoup`). Then we will dig into `Scrapy`, a flexible and powerful tool for crawling and scraping heterogeneous websites (what I call _broad crawling_).\n",
    "_narrow crawling_ (focused, precise scraping of a few websites)\n",
    "\n",
    "### Standing on the shoulders of... spiders?\n",
    "\n",
    "You can build a scraper from scratch using low-level modules or libraries, but then you have to deal with some potential headaches as your scraper grows more complex. For example, you'll need to handle concurrency so you can crawl more than one page at a time. You'll probably want to figure out how to transform your scraped data into different formats like CSV, XML, or JSON. And you'll sometimes have to deal with sites that require specific settings and access patterns.\n",
    "\n",
    "You'll have better luck if you build your scraper on top of an existing library that handles those issues for you. For this tutorial, we will build some intuition for web-scraping by working with low-level approaches, using the `Requests` and `BeautifulSoup` libraries to make requests and parse the result. Then we will build a scraper with *Scrapy*,which is one of the most popular, flexible, and powerful Python scraping libraries. Scrapy takes a \"batteries included\" approach to scraping, meaning that it handles a lot of the common functionality that all scrapers need. This prevents you from reinventing the wheel--or worse, the flat tire!\n",
    "\n",
    "To learn about crawling with Scrapy, we will explore [quotes.toscrape.com](quotes.toscrape.com), a scraping-friendly website that lists quotes from famous authors. By the end of today, you’ll have a fully functional web scraper that walks through a series of pages and extracts data from each page. The scraper will be easily expandable so you can tinker around with it and use it as a foundation for your own projects scraping data from the web.\n",
    "\n",
    "You can also read more about the [basics of scrapy](https://docs.scrapy.org/en/latest/intro/overview.html), its [architecture](https://docs.scrapy.org/en/latest/topics/architecture.html), or see [the FAQ](https://docs.scrapy.org/en/latest/faq.html). And if you need a refresher on scraping with Beautiful Soup, here's a [good tutorial](https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3).\n",
    "\n",
    "### What kind of crawling?\n",
    "\n",
    "A flexible tool like Scrapy can be used in many different ways depending on the task at hand.\n",
    "\n",
    "What I call _narrow crawling_ means focusing on a limited set of pre-defined domains--that is, studying their HTML and CSS structures and exploiting these to extract specific information repeatedly. This maximizes precision in scraping while sacrificing _extensibility_: the ability to incorporate new domains or be resilient to changes in website structure. This is what people usually mean when they say \"web-scraping\". It may or may not expand beyond the initial set of websites, but it may crawl more websites within this set (_vertical crawling_).\n",
    "\n",
    "What I call _broad crawling_ makes the opposite tradeoff, collecting information on a range of websites and promoting flexibility in its scraping algorithm (way of extracting website information) at the expense of generally less clean output. It may identify websites to scrape by google search (what do people click on most?), network analysis (what websites tend to link to one another?), or _link extraction_: finding all within-domain links on a given webpage, then all within-domain links on its children links, and so on to a specified depth. The messier output from broad crawling can present challenges for data cleaning and analysis (remember \"garbage in, garbage out\"?), but this depends on the application.\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    "* [Parsing HTML](#parsing)\n",
    "    * [Pretty parsing with `BeautifulSoup`](#BS)\n",
    "    * [Getting human-readable text](#readable)\n",
    "* [Crawling broadly with `Scrapy`](#scrapy)\n",
    "    * [A simple (narrow) spider](#simple)\n",
    "    * [Link extraction in a (broad) spider](#linkextraction)\n",
    "\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "* *relative link*: \n",
    "    * A link that builds on a given domain name. For instance, `/pennsylvania/` as a link from `https://www.politifact.com` points to `https://www.politifact.com/pennsylvania/`.\n",
    "* *absolute link*: \n",
    "    * A link that includes the complete domain name and can be accessed from anywhere, e.g. `https://www.politifact.com/pennsylvania/`.\n",
    "* *narrow crawling (less extensible)*: \n",
    "    * Scraping a limited set of pre-defined domains: studying their HTML and CSS structures and exploiting these to extract specific information repeatedly. This maximizes precision in scraping while sacrificing extensibility (ability to incorporate new domains or changes in website structure). What people usually mean when they say \"web-scraping\". \n",
    "* *broad crawling (more extensible)*: \n",
    "    * Collecting information on a range of websites and promoting flexibility in its scraping algorithm (way of extracting website information) at the expense of generally less clean output. It may identify websites to scrape by google search (what do people click on most?), network analysis (what websites tend to link to one another?), or link extraction.\n",
    "* *extensibility*:\n",
    "    * Ability for a scraping approach to incorporate new domains or be resilient to changes in website structure. Generally higher for broad crawls than narrow crawls, at the expense of precision. \n",
    "* *link extraction*:\n",
    "    * Finding all within-domain links on a given webpage, then all within-domain links on its children links, and so on to a specified depth. \n",
    "* *horizontal crawling*: \n",
    "    * Crawling on the same hierarchical level as the input domain, such as going from the first to the second page of google results.\n",
    "* *vertical crawling*:\n",
    "    * Crawling at a higher or lower level from the input domain, such as navigating to the \"About Us\" page directly linked from a home page. \n",
    "    \n",
    "### Credits\n",
    "Part of this notebook was borrowed from [the official Scrapy tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html), part came from [this great book on Scrapy](https://learning.oreilly.com/library/view/learning-scrapy/9781784399788/) (you may have University access [here](https://www.safaribooksonline.com/library/view/temporary-access/)), part was inspired by [Geoff Bacon's web-scraping workshop](https://github.com/TextXD/introduction-to-web-scraping), and part was written by me ([Jaren Haber](https://www.jarenhaber.com/)). The first three are great resources for further exploration and learning!\n",
    "\n",
    "**__________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML <a id='parsing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step in web scraping is parsing HTML. This is where things can get a little tricky.\n",
    "\n",
    "Let's start by looking more closely at HTML. Use your browser developer tools (e.g., in Chrome, right click > `Inspect`) to inspect the HTML of [the page listing all Fall 2021 Sociology courses at Georgetown University](https://myaccess.georgetown.edu/pls/bninbp/bwckgens.p_proc_term_date?p_term=202130&p_calling_proc=bwckschd.p_disp_dyn_sched#_ga=2.223705375.587656937.1619556624-282868439.1588700423) in your browser (select \"Sociology\" from the list then click \"Get Courses\"), and find the part of the HTML where the course headings are listed. There's a lot of other stuff in the file that we don't care too much about. You could try `Crtl-F`ing for the name of a course you see on the webpage.\n",
    "\n",
    "You should see something like this:\n",
    "\n",
    "```\n",
    "<tbody>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38298\">Introduction to Sociology - 38298 - SOCI 001 - 01</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38299\">Introduction to Sociology - 38299 - SOCI 001 - 02</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38300\">Introduction to Sociology - 38300 - SOCI 001 - 03</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38301\">Introduction to Sociology - 38301 - SOCI 001 - 04</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38302\">Introduction to Sociology - 38302 - SOCI 001 - 05</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=40419\">Sociology of Health/Illness - 40419 - SOCI 109 - 01</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=36639\">Race, Society &amp; Cinema - 36639 - SOCI 133 - 01</a></th></tr>\n",
    "```\n",
    "\n",
    "This is HTML. HTML uses \"tags\", code that surrounds the raw text which indicates the structure of the content. The tags are enclosed in `<` and `>` symbols. The `<li>` says \"this is a new thing in a list and `</li>` says \"that's the end of that new thing in the list\". Similarly, the `<a ...>` and the `</a>` say, \"everything between us is a hyperlink\". And likewise, the `<tr>`and the `</tr>` enclose a table row, while the `<th>`and the `</th>` enclose row cells that contain column headers.\n",
    "\n",
    "In this HTML file, each course title is listed with `<th>...</th>` and is also linked to its own page using `<a>...</a>`. In our browser, if we click on the name of the department, it takes us to detailed information for that class, including Registration Availability. You'll see inside the `<a>` bit, there's a `href=...`. That tells us the (relative) location of the page it's linked to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty parsing with `BeautifulSoup` <a id='BS'></a>\n",
    "\n",
    "Armed with this knowledge of HTML, let's try getting the HTML and parsing the fact checking page we saw earlier. We will use `requests` to get the HTML and its text, then `BeautifulSoup` to parse the result. (Check out [the `BeautifulSoup` docs](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) for lots of tips and tricks!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup for parsing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define URL to scrape\n",
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "\n",
    "# Scrape HTML\n",
    "html = requests.get(url)\n",
    "\n",
    "# Convert HTML into soup object\n",
    "soup = BeautifulSoup(html.text) # use default 'html.parser' ('lxml' is faster though)\n",
    "\n",
    "# See pretty formatting in soup object\n",
    "print(soup.prettify()[:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty! Much more so than the plain ol' `requests.get().text` block we saw earlier. But this is just the beginning of what `BeautifulSoup` can do. It can also find specific tags, like paragraphs (via `<p>`), headers (via `h1`, `h2`, etc.), and hyperlinks (via `<a>` and their `href` elements).\n",
    "\n",
    "In most cases, the `<p>` tag is the most useful for extracting readable text from a webpage. Let's get the first 10 paragraph tags from this claim review page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in soup.find_all('p')[:10]: # first 10 paragraphs via <p> tag\n",
    "    print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Find all the links in the above claim review page using the `<a>` tags and their `href` elements. Print every 10th link. What do you notice about where these links point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see lots of relative links (e.g., `/pennsylvania/`), places where the `href` seems to point nowhere (e.g., `#`), and communication shortcuts (e.g., `https://twitter.com/share?text=PolitiFact - Citizens United calls...`). This could be cleaned up by appending relative links to the domain name (`https://www.politifact.com/`) and keeping only URLs (and nothing after)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes these tags aren't very useful--in fact, they can get in the way of extracting only visible or human-readable text from the HTML. This too can be accomplished with `BeautifulSoup`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting human-readable text <a id='readable'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally we want to learn about websites via their tags: What the headers say, which paragraph comes first, where the links or images are, etc. Other times tags (such as scripts or styles) only introduce extraneous characters and nonsense words, and we want to ignore the tags themselves or even the text they enclose. \n",
    "\n",
    "The simplest way to do this is with the `get_text()` method in `BeautifulSoup`, which returns all the text in a document or beneath a tag, as a single Unicode string. You might have noticed that the `<p>` tags got in the way in our above example. Let's try that again and this time, we will remove the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in soup.find_all('p')[:10]: # first 10 paragraphs via <p> tag\n",
    "    print(paragraph.get_text().strip()) # extract text and strip trailing spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also easy to call the first element of the soup object matching a given tag, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p.get_text() # Get text of first paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is `extract()`, which can be used to surgically remove a tag or string from the soup tree, storing it for safe keeping. Let's extract the first 5 links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = [] # initialize list of extracted links\n",
    "\n",
    "for link in soup.find_all('a')[:5]: # get first ten <a> tags\n",
    "    extracted.append(link.extract()) # extract the link\n",
    "    \n",
    "print('Extracted links:', extracted)\n",
    "print()\n",
    "\n",
    "# What are the first 10 links now the the previous 10 were removed? \n",
    "for link in soup.find_all('a')[:5]: \n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we don't want to keep the tag at all? In this case, we would use `decompose()`, which obliterates a useless tag (and frees up memory). Unlike with `extract()`, with `decompose()` you don't need to assign the junk tag to anything to clear it--the method does this automatically. \n",
    "\n",
    "Let's try the above code again, this time with `decompose()` and `get_text()` to clean up the display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "for link in soup.find_all('a')[:5]: # get first ten <a> tags\n",
    "    link.decompose() # obliterate this link\n",
    "    \n",
    "# What are the first 10 links now the the previous 10 were removed? \n",
    "for link in soup.find_all('a')[:5]: \n",
    "    print(link.get_text().strip()) # get text and clean spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all websites use the `<p>` tag to indicate the important, human-readable text. Sometimes we need to approach HTML parsing from the other end: By finding and removing all non-informative tags. Let's use `BeautifulSoup` to build such a method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use `decompose()` to remove from the soup all tags showing anything other than human-readable text. Below is a list of such junk tags to use as a blacklist. \n",
    "\n",
    "```\n",
    "\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\", \"kbd\", \n",
    "\"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\", \"span\", \"sub\", \"sup\", \"head\", \n",
    "\"title\", \"[document]\", \"script\", \"style\", \"meta\", \"noscript\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that word boundaries get clobbered when you call `get_text()`. This is because the default setting for this method is `strip=True`, which tells `BeautifulSoup` to strip whitespaces (of any kind) from the beginning and end of each bit of text. Using `strip=False` leads to lots of extra whitespaces--usually, newlines--which requires some regular expressions to clean up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Using the above tags blacklist and `decompose()` as before, this time use the `strip=False` parameter when calling `get_text()` to avoid combining words across whitespace boundaries. Instead, use regular expressions to clean up extra whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "You might have noticed that when we scraped HTML above from [this claim review by PolitiFact](https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/), we got headers and tags like this:\n",
    "```html\n",
    "<p>Misinformation isn't going away just because it's a new year. Support trusted, factual information with a tax deductible contribution to PolitiFact.</p>\n",
    "<p>\n",
    "<a class=\"m-disruptor-content__link\" href=\"/membership/\">More Info</a>\n",
    "</p>\n",
    "<p class=\"c-image__caption-inner copy-xs\">\n",
    "The White House infrastructure plan has $111 billion to improve water and sewer systems. (Shutterstock)\n",
    "</p>\n",
    "```\n",
    "Use what you now know about identifying HTML, removing tags, and cleaning spacing to scrape a clean explanation from the body of this article. \n",
    "\n",
    "_Hint:_ Use your browser to inspect this website's HTML and identify any unique types and/or classes that enclose the explanation (and nothing else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the output from this focused, site-specific scraping approach with that from the blacklist method above. <br/>\n",
    "**Which method gives the cleaner output? Which method is more extensible?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling broadly with `Scrapy` <a id='scrapy'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a _Twisted_ application, Scrapy is event-driven, asynchronous, and is virtually multi-threaded (while using only one thread). While other programs cause _blocks_ when they access files or the web, spawn new processes, or do system operations, Scrapy instead waits until a resource is available, solves the immediate problem, and then calls another task. In short, Scrapy is fast, flexible, and scalable. It offers one of the most user-friendly ways to write crawling programs that can move across heterogeneous swaths of the internet, download stuff, and not break. \n",
    "\n",
    "To grasp the intuition behind Scrapy, imagine a bank where tellers (threads) are available to see customers (processes), who need to fill out forms before they're done. Such a situation could be configured in these ways:\n",
    "\n",
    "- _Blocking_ operation with a _single_ thread: Here there is 1 teller trying to help 5 customers. When customer 1 needs time to fill out a form, then teller 1 is occupied waiting for customer 1--and all the other customers are stuck in line.\n",
    "- _Blocking_ operation with _multiple_ threads: Now there are still 5 customers, but there are 3 tellers. When customer 1 needs time to fill out a form, then teller 1 is occupied. Customer 2 may have access to teller 2 and customer 3 to teller 3, but then all the tellers are monopolized while people fill out forms, which means customers 4 and 5 are still stuck waiting in line. \n",
    "- _Non-blocking_ operation with a _single_ thread: Here again we have 1 teller and 5 customers. When customer 1 needs time to fill out a form, they stand aside so the single teller can help customer 2. When customer 1 is finished, they wait until customer 2 is done or has something to do, then customer 1 is called back to continue being helped. If customers 1 and 2 both have forms to complete, they can do that on the side and the single teller can see customer 3, and so on. \n",
    "\n",
    "You can see this last situation is way more efficient than the previous two. This is the Scrapy default; when _multiple_ threads are available for a _non-blocking_ operation (like when multiple spiders work together), this is even better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start scraping, you will have to set up a new Scrapy project. Enter a directory where you’d like to store your code and run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ scrapy startproject schools\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a `schools` directory with the following contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "schools/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "    tutorial/             # project's Python module, you'll import your code from here\n",
    "        __init__.py\n",
    "        items.py          # project items definition file\n",
    "        middlewares.py    # project middlewares file\n",
    "        pipelines.py      # project pipelines file\n",
    "        settings.py       # project settings file\n",
    "        spiders/          # a directory where you'll later put your spiders\n",
    "            __init__.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple (narrow) spider <a id='simple'> </a>\n",
    "\n",
    "Spiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass `scrapy.Spider` and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data.\n",
    "\n",
    "This is the code for our first Spider. Save it in a file named `simple_spider.py` under the `schools/spiders` directory in your project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import scrapy\n",
    "\n",
    "class SimpleSpider(scrapy.Spider):\n",
    "    name = \"simple\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our Spider subclasses `scrapy.Spider` and defines some attributes and methods:\n",
    "\n",
    "-`name`: identifies the Spider. It must be unique within a project, that is, you can’t set the same name for different Spiders.\n",
    "\n",
    "-`start_urls`: a list of URLs to provide the initial requests for the crawler. Armed with this list alone, the spider will download HTML from the webpages specified, much as a web browser does. But it won't extract anything from the pages--that's why we need to define the `parse()` method.\n",
    "\n",
    "-`parse()`: a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of `TextResponse` that holds the page content and has further helpful methods to handle it.\n",
    "\n",
    "The `parse()` method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (`Request`) from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put our spider to work, go to the project’s top level directory and run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ scrapy crawl simple\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command runs the spider with name `simple` that we’ve just added, that will send some requests for the `quotes.toscrape.com` domain. You will get an output similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "...\n",
    "2019-03-19 15:58:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2019-03-19 15:58:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] INFO: Spider opened\n",
    "2019-03-19 15:58:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2019-03-19 15:58:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
    "2019-03-19 15:58:49 [simple] DEBUG: Saved file quotes-1.html\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2019-03-19 15:58:50 [simple] DEBUG: Saved file quotes-2.html\n",
    "2019-03-19 15:58:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2019-03-19 15:58:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 678,\n",
    " 'downloader/request_count': 3,\n",
    " ...}\n",
    "2019-03-19 15:58:50 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the files in the current directory. You should notice that two new files have been created: `quotes-1.html` and `quotes-2.html`, with the content for the respective URLs, as our `parse` method instructs.\n",
    "\n",
    "How did this work? Scrapy schedules the `scrapy.Request` objects returned by the `start_requests` method of the Spider. Upon receiving a response for each one, it instantiates `Response` objects and calls the callback method associated with the request (in this case, the `parse` method) passing the response as argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Modify and run the spider script above to scrape this short list of `start_urls`: \n",
    "```python\n",
    "['http://www.baylessk12.org/', 'https://crcc.doniphanr1.k12.mo.us/', 'https://www.hazelwoodschools.org/southeastmiddle']\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data\n",
    "The best way to learn how to extract data with Scrapy is trying selectors using the shell [Scrapy shell](https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell). Remember to always enclose URLs in quotes (double quotes for Windows) when running Scrapy shell from command-line, otherwise urls containing arguments (ie. `&` character) will not work. Run:\n",
    "\n",
    "```shell\n",
    "$ scrapy shell 'http://quotes.toscrape.com'\n",
    "```\n",
    "You will see something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "2019-03-19 20:00:05 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: tutorial)\n",
    "2019-03-19 20:00:05 [scrapy.utils.log] INFO: Versions: lxml 4.3.1.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 17.9.0, Python 3.6.7 (default, Oct 22 2018, 11:32:17) - [GCC 8.2.0], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Linux-4.15.0-45-generic-x86_64-with-Ubuntu-18.04-bionic\n",
    "2019-03-19 20:00:05 [scrapy.crawler] INFO: Overridden settings: {...}\n",
    "2019-03-19 20:00:05 [scrapy.extensions.telnet] INFO: Telnet Password: 030319d194e7f6b0\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage']\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    "...]\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    "...]\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2019-03-19 20:00:05 [scrapy.core.engine] INFO: Spider opened\n",
    "2019-03-19 20:00:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2019-03-19 20:00:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f07993c02b0>\n",
    "... \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the shell, you can try selecting elements using CSS with the response object:\n",
    "```shell\n",
    ">>> response.css('title')\n",
    "```\n",
    "The result of running `response.css('title')` is a list-like object called SelectorList, which represents a list of Selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data.\n",
    "\n",
    "To extract the text from the title above, you can do:\n",
    "```shell\n",
    ">>> response.css('title::text').getall()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to note here: one is that we’ve added `::text` to the CSS query, to mean we want to select only the text elements directly inside `<title>` element. If we don’t specify `::text`, we’d get the full title element, including its tags:\n",
    "```shell\n",
    ">>> response.css('title').getall()\n",
    "```\n",
    "The other thing is that the result of calling `.getall()` is a list: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do:\n",
    "```shell\n",
    ">>> response.css('title::text').get()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the `getall()` and `get()` methods, you can also use the `re()` method to extract using regular expressions:\n",
    "```shell\n",
    ">>> response.css('title::text').re(r'Quotes.*')\n",
    ">>> response.css('title::text').re(r'Q\\w+')\n",
    ">>> response.css('title::text').re(r'(\\w+) to (\\w+)')\n",
    "```\n",
    "In order to find the proper CSS selectors to use, you can use your browser developer tools (e.g., in Chrome, right click > `Inspect`) to inspect the HTML and come up with a selector (for more info, see [Using your browser’s Developer Tools for scraping](https://docs.scrapy.org/en/latest/topics/developer-tools.html)). You can also try opening the response page from the shell in your web browser using `view(response)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Inspect [quotes.toscrape.com](quotes.toscrape.com) for the selectors associated with quotes. Use this information to display the text of one of the quotes in the scrapy shell. <br>\n",
    "**Hint 1:** If you need help getting a better sense of website structure, use the HTML tree below (under \"Extracting quotes and authors\") as a visual guide.<br>\n",
    "**Hint 2:** You can subset within selectors by using periods and spaces. For instance, the following produces a SelectorList for the class2 of each type2 within the class1 of each type1:\n",
    "```shell\n",
    "response.css('type1.class1 type2.class2')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your solution here: \n",
    "\n",
    "```shell\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting quotes and authors\n",
    "Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page.\n",
    "\n",
    "Each quote in http://quotes.toscrape.com is represented by HTML elements that look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<div class=\"quote\">\n",
    "    <span class=\"text\">“The world as we have created it is a process of our\n",
    "    thinking. It cannot be changed without changing our thinking.”</span>\n",
    "    <span>\n",
    "        by <small class=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "    </span>\n",
    "    <div class=\"tags\">\n",
    "        Tags:\n",
    "        <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "        <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "        <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "        <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "    </div>\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we extract the data we want? To start, we get a list of selectors for the quote HTML elements with:\n",
    "\n",
    "```shell\n",
    ">>> response.css(\"div.quote\")\n",
    "```\n",
    "\n",
    "Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:\n",
    "```shell\n",
    ">>> quote = response.css(\"div.quote\")[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s extract title, author and the tags from that quote using the quote object we just created:\n",
    "\n",
    "```shell\n",
    ">>> title = quote.css(\"span.text::text\").get()\n",
    ">>> title\n",
    ">>> author = quote.css(\"small.author::text\").get()\n",
    ">>> author\n",
    "```\n",
    "\n",
    "Given that the tags are a list of strings, we can use the .getall() method to get all of them:\n",
    "```shell\n",
    ">>> tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    ">>> tags\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary. Copy and paste each of these subsequent lines into scrapy shell (or type them in via split-screen):\n",
    "```shell\n",
    ">>> for quote in response.css(\"div.quote\"):\n",
    "...     text = quote.css(\"span.text::text\").get()\n",
    "...     author = quote.css(\"small.author::text\").get()\n",
    "...     tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "...     print(dict(text=text, author=author, tags=tags))\n",
    ">>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data in our spider\n",
    "Let’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider.\n",
    "\n",
    "A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the `yield` Python keyword in the callback, as you can see below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class SimpleSpider(scrapy.Spider):\n",
    "    name = \"simple\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this spider, it will output the extracted data with the log:\n",
    "```shell\n",
    "2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
    "{'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}\n",
    "2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
    "{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the scraped data\n",
    "The simplest way to store the scraped data is by using Feed exports, with the following command:\n",
    "```shell\n",
    "$ scrapy crawl quotes -o quotes.json\n",
    "```\n",
    "\n",
    "That will generate an quotes.json file containing all scraped items, serialized in JSON.\n",
    "\n",
    "For historic reasons, Scrapy appends to a given file instead of overwriting its contents. If you run this command twice without removing the file before the second time, you’ll end up with a long JSON file--actually, a broken JSON file, which cannot be read.\n",
    "\n",
    "You can also use other formats, like JSON Lines:\n",
    "```shell\n",
    "$ scrapy crawl quotes -o quotes.jl\n",
    "```\n",
    "\n",
    "The JSON Lines format is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory.\n",
    "\n",
    "In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in `schools/pipelines.py`. Though you don’t need to implement any item pipelines if you just want to store the scraped items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link extraction in a (broad) spider <a id='linkextraction'> </a>\n",
    "\n",
    "Let’s say, instead of just scraping the stuff from the first two pages from http://quotes.toscrape.com, you want quotes from all the pages in the website. We could do this by adding more URLs to the `start_urls` field, but this sounds like work. We could also read them in via a text file, but that doesn't sound any easier.\n",
    "\n",
    "The best way would be to find the URLs of within-domain links from the website itself, extract data from these pages, go one level still further down, and so on. We could do this by identifying and extracting links using their CSS selectors, and this would let us scrape all the quotes across all the pages across this website (there are steps for this in [the `Scrapy` tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)). But CSS selectors are site-specific, so what about other websites? Could we use `Scrapy` to crawl all the links reliably across different kinds of websites?\n",
    "\n",
    "Yes! In fact, this kind of _broad crawling_ is (arguably) where `Scrapy` shines the most. Crawlers typically mix _horizontal crawling_, crawling pages at the same hierarchical level (for example, from `school.com/page1` to `school.com/page2`), with _vertical crawling_, moving from a higher hierarchical level (for example, `school.com/main`) to a lower one (for example, `school.com/main/about_us`). `Scrapy` makes doing both of these easy by providing the `CrawlSpider` class, from which we can borrow using the `genspider` command:\n",
    "\n",
    "```shell\n",
    "$ scrapy genspider -t crawl broad http://quotes.toscrape.com  # borrows from `crawl` spider template\n",
    "Created spider 'broad' using template 'crawl' in module:\n",
    "  schools.spiders.broad\n",
    "```\n",
    "Go ahead and execute this command in your terminal, and check out the resulting file in `schools/spiders/broad.py`. It should look like this:\n",
    "\n",
    "```python\n",
    "class BroadSpider(CrawlSpider):\n",
    "    name = 'broad'\n",
    "    allowed_domains = ['http://quotes.toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        item = {}\n",
    "        ...\n",
    "        return item\n",
    "```\n",
    "It's worth explaining this a bit. `CrawlSpider` provides an implementation of the `parse()` method that uses the `rules` variable to allow easy two-direction (both horizontal and vertical) crawling. For instance, `Scrapy` by default avoids duplicate requests. \n",
    "\n",
    "Unless a callback is set, a Rule will follow the extracted URLs, which means that it will scan target pages for extra links and follow them. If a callback is set, the Rule won't follow the links from target pages. If you would like it to follow links, you should either return/yield them from your callback method, or set the follow argument of Rule() to true (which is what we will do). \n",
    "\n",
    "How does this spider find links to follow? As their name implies, LinkExtractors are specialized in extracting links, so by default, they are looking for the `a` (and `area`) `href` HTML tags or attributes. Links are ordered using by  \"last in, first out\", meaning that it scrapes a page and all its sublinks before visiting the next page.\n",
    "\n",
    "In a moment, we will adapt this template to scrape from all the pages on our sample website. But first, we need to update our `items.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining items\n",
    "\n",
    "Within the project directory, there’s an `items.py` file. Items add structure to our scraping results and are used by spiders.\n",
    "\n",
    "Here you can add class fields such as url, images, or locations. These fields can be filled by pipelines (a more advanced topic).\n",
    "\n",
    "Add to this file the fields for text, author, and tags. We will use these for our quotes spider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from scrapy.item import Item, Field\n",
    "\n",
    "class SchoolsItem(Item):\n",
    "    text = Field()\n",
    "    author = Field()\n",
    "    tags = Field()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item properties can then be set with the response we get from parsing. \n",
    "\n",
    "```python\n",
    "from schools.items import SchoolsItem\n",
    "...\n",
    "def parse_item(self, response):\n",
    "    item = SchoolsItem()      \n",
    "    item['text'] = ...\n",
    "    item['author'] = ...\n",
    "    item['tags'] = ...\n",
    "    \n",
    "    yield item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Adapt the `CrawlSpider` in `broad.py` to scrape the text, author, and tag for each quote across all the page on `http://quotes.toscrape.com`. Assign the `text`, `author`, and `tags` fields to Items, then yield the Items. Edit the spider script first, then run it via your terminal, then check the output to make sure.\n",
    "\n",
    "```python\n",
    "# Your solution here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use what you learned above about removing tags with a blacklist to rewrite the `parse_item()` function in the `BroadSpider()` so that it doesn't depend on website structure (HTML, CSS, XPath, etc.). In other words, write a truly broad crawler that only returns text.\n",
    "\n",
    "Make sure to clean up the spacing: convert multiple newlines into a single one or a space, depending on the output format you want. \n",
    "\n",
    "_Hint:_ Check your output--is it missing anything important? Consider removing specific tags from the blacklist. \n",
    "\n",
    "\n",
    "```python\n",
    "# Your solution here\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
