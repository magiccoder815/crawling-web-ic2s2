{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and crawling the web\n",
    "\n",
    "This second day's workshop gives you practice scraping and crawling with modern Python tools. \n",
    "\n",
    "To review from yesterday, web-scraping means “programatically going over a collection of web pages and extracting data”. Scraping is a powerful tool for working with data on the web, and it has two core steps. First, you find web pages and download them via web requests (often called *web-crawling*). Then you extract and parse information from these pages (often called *web-scraping*). These two steps often happen together and recursively: you crawl some stuff, but upon scraping it you realize you got the wrong websites, so you go back to crawling, which changes your scraping approach, and so on.\n",
    "\n",
    "We will start today with the essential tools for web-scraping or _narrow crawling_ (focused, precise scraping of a few websites): making requests (with `Requests`) and parsing HTML (with `BeautifulSoup`). Then we will dig into `Scrapy`, a flexible and powerful tool for crawling and scraping heterogeneous websites (what I call _broad crawling_).\n",
    "\n",
    "### Standing on the shoulders of... spiders?\n",
    "\n",
    "You can build a scraper from scratch using low-level modules or libraries, but then you have to deal with some potential headaches as your scraper grows more complex. For example, you'll need to handle concurrency so you can crawl more than one page at a time. You'll probably want to figure out how to transform your scraped data into different formats like CSV, XML, or JSON. And you'll sometimes have to deal with sites that require specific settings and access patterns.\n",
    "\n",
    "You'll have better luck if you build your scraper on top of an existing library that handles those issues for you. For this tutorial, we will build some intuition for web-scraping by working with low-level approaches, using the `Requests` and `BeautifulSoup` libraries to make requests and parse the result. Then we will build a scraper with *Scrapy*,which is one of the most popular, flexible, and powerful Python scraping libraries. Scrapy takes a \"batteries included\" approach to scraping, meaning that it handles a lot of the common functionality that all scrapers need. This prevents you from reinventing the wheel--or worse, the flat tire!\n",
    "\n",
    "To learn about crawling with Scrapy, we will explore [quotes.toscrape.com](quotes.toscrape.com), a scraping-friendly website that lists quotes from famous authors. By the end of today, you’ll have a fully functional web scraper that walks through a series of pages and extracts data from each page. The scraper will be easily expandable so you can tinker around with it and use it as a foundation for your own projects scraping data from the web.\n",
    "\n",
    "You can also read more about the [basics of scrapy](https://docs.scrapy.org/en/latest/intro/overview.html), its [architecture](https://docs.scrapy.org/en/latest/topics/architecture.html), or see [the FAQ](https://docs.scrapy.org/en/latest/faq.html). And if you need a refresher on scraping with Beautiful Soup, here's a [good tutorial](https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3).\n",
    "\n",
    "### What kind of crawling?\n",
    "\n",
    "A flexible tool like Scrapy can be used in many different ways depending on the task at hand.\n",
    "\n",
    "What I call _narrow crawling_ means focusing on a limited set of pre-defined domains--that is, studying their HTML and CSS structures and exploiting these to extract specific information repeatedly. This maximizes precision in scraping while sacrificing _extensibility_: the ability to incorporate new domains or be resilient to changes in website structure. This is what people usually mean when they say \"web-scraping\". It may or may not expand beyond the initial set of websites, but it may crawl more websites within this set (_vertical crawling_).\n",
    "\n",
    "What I call _broad crawling_ makes the opposite tradeoff, collecting information on a range of websites and promoting flexibility in its scraping algorithm (way of extracting website information) at the expense of generally less clean output. It may identify websites to scrape by google search (what do people click on most?), network analysis (what websites tend to link to one another?), or _link extraction_: finding all within-domain links on a given webpage, then all within-domain links on its children links, and so on to a specified depth. The messier output from broad crawling can present challenges for data cleaning and analysis (remember \"garbage in, garbage out\"?), but this depends on the application.\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    "* [Narrow crawling/scraping](#narrow)\n",
    "    - [Making `Requests`](#request)\n",
    "    - [Parsing HTML](#parsing)\n",
    "       - [Pretty parsing with `BeautifulSoup`](#BS)\n",
    "       - [Getting human-readable text](#readable)\n",
    "* [Crawling broadly with `Scrapy`](#scrapy)\n",
    "    - [A simple (narrow) spider](#simple)\n",
    "    - [Link extraction in a (broad) spider](#linkextraction)\n",
    "* [Scrapy template: A Recursive Text Spider](#recursive)\n",
    "\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "* *relative link*: \n",
    "    * A link that builds on a given domain name. For instance, `/pennsylvania/` as a link from `https://www.politifact.com` points to `https://www.politifact.com/pennsylvania/`.\n",
    "* *absolute link*: \n",
    "    * A link that includes the complete domain name and can be accessed from anywhere, e.g. `https://www.politifact.com/pennsylvania/`.\n",
    "* *narrow crawling (less extensible)*: \n",
    "    * Scraping a limited set of pre-defined domains: studying their HTML and CSS structures and exploiting these to extract specific information repeatedly. This maximizes precision in scraping while sacrificing extensibility (ability to incorporate new domains or changes in website structure). What people usually mean when they say \"web-scraping\". \n",
    "* *broad crawling (more extensible)*: \n",
    "    * Collecting information on a range of websites and promoting flexibility in its scraping algorithm (way of extracting website information) at the expense of generally less clean output. It may identify websites to scrape by google search (what do people click on most?), network analysis (what websites tend to link to one another?), or link extraction.\n",
    "* *extensibility*:\n",
    "    * Ability for a scraping approach to incorporate new domains or be resilient to changes in website structure. Generally higher for broad crawls than narrow crawls, at the expense of precision. \n",
    "* *link extraction*:\n",
    "    * Finding all within-domain links on a given webpage, then all within-domain links on its children links, and so on to a specified depth. \n",
    "* *horizontal crawling*: \n",
    "    * Crawling on the same hierarchical level as the input domain, such as going from the first to the second page of google results.\n",
    "* *vertical crawling*:\n",
    "    * Crawling at a higher or lower level from the input domain, such as navigating to the \"About Us\" page directly linked from a home page. \n",
    "    \n",
    "### Credits\n",
    "Part of this notebook was borrowed from [the official Scrapy tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html), part came from [this great book on Scrapy](https://learning.oreilly.com/library/view/learning-scrapy/9781784399788/) (you may have University access [here](https://www.safaribooksonline.com/library/view/temporary-access/)), part was inspired by [Geoff Bacon's web-scraping workshop](https://github.com/TextXD/introduction-to-web-scraping), and part was written by me ([Jaren Haber](https://www.jarenhaber.com/)). The first three are great resources for further exploration and learning!\n",
    "\n",
    "**__________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow crawling/scraping <a id='narrow'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making `Requests` <a id='request'></a>\n",
    "\n",
    "The first step in web-scraping is getting the HTML of the website we want to scrape. The [requests](http://docs.python-requests.org/en/master/) library is the easiest way to do this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Patawomeck'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it looks like everything worked! Let's see our beautiful HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, that's weird. Doesn't look like HTML to me.\n",
    "\n",
    "What the `requests.get` function returned (and the thing in our `response` variable) was a Response object. It itself isn't the HTML that we wanted, but rather a collection of metadata about the request/response interaction between your computer and the Wikipedia server.\n",
    "\n",
    "For example, it knows whether the response was successful or not (`response.ok`), how long the whole interaction took (`response.elapsed`), what time the request took place (`response.headers['Date']`) and a whole bunch of other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tue, 27 Apr 2021 05:06:05 GMT'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, what we really care about is the HTML content. We can get that from the `Response` object with `response.text`. What we get back is a string of HTML, exactly the contents of the HTML file at the URL that we requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Patawomeck - Wikipedia</title>\n",
      "<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"73e60c54-72d5-408e-9160-a0ae2e2c0194\",\"wgCSPNonce\":!1,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"Patawomeck\",\"wgTitle\":\"Patawomeck\",\"wgCurRevisionId\":995861621,\"wgRevisionId\":995861621,\"wgArticleId\":13441152,\"wgIsArticle\":!0,\"wgIsRedirect\":!1,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Webarchive template wayback links\",\"\\\"Related ethnic groups\\\" needing confirmation\",\"Articles using infobox ethnic group with image parameters\",\"All articles with specifically m\n"
     ]
    }
   ],
   "source": [
    "html = response.text\n",
    "print(html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Get the HTML for [this claim review by fact checking site PolitiFact](https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/). \n",
    "Print out the first 1000 characters and compare it to the HTML you see when you view the source HTML in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!DOCTYPE html>\\n<html lang=\"en-US\" dir=\"ltr\">\\n<head>\\n<meta charset=\"utf-8\">\\n<meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n<title>PolitiFact | Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.</title>\\n<meta name=\"description\" content=\"Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal. Senate Minority Leader \" />\\n<meta property=\"og:url\" content=\"https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/\" />\\n<meta property=\"og:image\" content=\"https://static.politifact.com/politifact/rulings/meter-mostly-false.jpg\" />\\n<meta property=\"og:image:secure_url\" content=\"https://static.politifact.com/politifact/rulings/meter-mostly-false.jpg\" />\\n<meta property=\"og:title\" content=\"PolitiFact - Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.\" />\\n<meta propert'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "html[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML <a id='parsing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step in web scraping is parsing HTML. This is where things can get a little tricky.\n",
    "\n",
    "Let's start by looking more closely at HTML. Use your browser developer tools (e.g., in Chrome, right click > `Inspect`) to inspect the HTML of [the page listing all Fall 2021 Sociology courses at Georgetown University](https://myaccess.georgetown.edu/pls/bninbp/bwckgens.p_proc_term_date?p_term=202130&p_calling_proc=bwckschd.p_disp_dyn_sched#_ga=2.223705375.587656937.1619556624-282868439.1588700423) in your browser (select \"Sociology\" from the list then click \"Get Courses\"), and find the part of the HTML where the course headings are listed. There's a lot of other stuff in the file that we don't care too much about. You could try `Crtl-F`ing for the name of a course you see on the webpage.\n",
    "\n",
    "You should see something like this:\n",
    "\n",
    "```\n",
    "<tbody>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38298\">Introduction to Sociology - 38298 - SOCI 001 - 01</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38299\">Introduction to Sociology - 38299 - SOCI 001 - 02</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38300\">Introduction to Sociology - 38300 - SOCI 001 - 03</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38301\">Introduction to Sociology - 38301 - SOCI 001 - 04</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38302\">Introduction to Sociology - 38302 - SOCI 001 - 05</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=40419\">Sociology of Health/Illness - 40419 - SOCI 109 - 01</a></th></tr>\n",
    "<tr><th class=\"ddtitle\" scope=\"colgroup\"><a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=36639\">Race, Society &amp; Cinema - 36639 - SOCI 133 - 01</a></th></tr>\n",
    "```\n",
    "\n",
    "This is HTML. HTML uses \"tags\", code that surrounds the raw text which indicates the structure of the content. The tags are enclosed in `<` and `>` symbols. The `<li>` says \"this is a new thing in a list and `</li>` says \"that's the end of that new thing in the list\". Similarly, the `<a ...>` and the `</a>` say, \"everything between us is a hyperlink\". And likewise, the `<tr>`and the `</tr>` enclose a table row, while the `<th>`and the `</th>` enclose row cells that contain column headers.\n",
    "\n",
    "In this HTML file, each course title is listed with `<th>...</th>` and is also linked to its own page using `<a>...</a>`. In our browser, if we click on the name of the department, it takes us to detailed information for that class, including Registration Availability. You'll see inside the `<a>` bit, there's a `href=...`. That tells us the (relative) location of the page it's linked to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty parsing with `BeautifulSoup` <a id='BS'></a>\n",
    "\n",
    "Armed with this knowledge of HTML, let's try getting the HTML and parsing the fact checking page we saw earlier. We will use `requests` to get the HTML and its text, then `BeautifulSoup` to parse the result. (Check out [the `BeautifulSoup` docs](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) for lots of tips and tricks!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html dir=\"ltr\" lang=\"en-US\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"ie=edge\" http-equiv=\"x-ua-compatible\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   PolitiFact | Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.\n",
      "  </title>\n",
      "  <meta content=\"Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal. Senate Minority Leader \" name=\"description\"/>\n",
      "  <meta content=\"https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/\" property=\"og:url\"/>\n",
      "  <meta content=\"https://static.politifact.com/politifact/rulings/meter-mostly-false.jpg\" property=\"og:image\"/>\n",
      "  <meta content=\"https://static.politifact.com/politifact/rulings/meter-mostly-false.jpg\" property=\"og:image:secure_url\"/>\n",
      "  <meta content=\"PolitiFact - Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.\" property=\"og:title\"/>\n",
      "  <meta content=\"website\" property=\"og:type\"/>\n",
      "  <meta content=\"600\" property=\"og:image:width\"/>\n",
      "  <meta content=\"600\" property=\"og:image:height\"/>\n",
      "  <meta content=\"@politifact\" property=\"og\n"
     ]
    }
   ],
   "source": [
    "# Import BeautifulSoup for parsing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define URL to scrape\n",
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "\n",
    "# Scrape HTML\n",
    "html = requests.get(url)\n",
    "\n",
    "# Convert HTML into soup object\n",
    "soup = BeautifulSoup(html.text) # use default 'html.parser' ('lxml' is faster though)\n",
    "\n",
    "# See pretty formatting in soup object\n",
    "print(soup.prettify()[:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty! Much more so than the plain ol' `requests.get().text` block we saw earlier. But this is just the beginning of what `BeautifulSoup` can do. It can also find specific tags, like paragraphs (via `<p>`), headers (via `h1`, `h2`, etc.), and hyperlinks (via `<a>` and their `href` elements).\n",
    "\n",
    "In most cases, the `<p>` tag is the most useful for extracting readable text from a webpage. Let's get the first 10 paragraph tags from this claim review page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Misinformation isn't going away just because it's a new year. Support trusted, factual information with a tax deductible contribution to PolitiFact.</p>\n",
      "<p>\n",
      "<a class=\"m-disruptor-content__link\" href=\"/membership/\">More Info</a>\n",
      "</p>\n",
      "<p class=\"c-image__caption-inner copy-xs\">\n",
      "The White House infrastructure plan has $111 billion to improve water and sewer systems. (Shutterstock)\n",
      "</p>\n",
      "<p>The White House infrastructure plan would cost about $2.3 trillion. A Green New Deal-type plan would cost $9.5 trillion.</p>\n",
      "<p>The Green New Deal included broader social economic goals, such as a guaranteed livable wage, affordable higher education and universal health care.</p>\n",
      "<p>Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal.</p>\n",
      "<p>Senate Minority Leader Mitch McConnell said that as written, the $2.3 trillion American Jobs Plan released March 31 was a nonstarter. The conservative PAC Citizens United put Biden’s plan in the same boat as the <a href=\"https://www.congress.gov/bill/116th-congress/house-resolution/109\">Green New Deal</a>, a sweeping environmental and social justice agenda that Republicans have condemned.</p>\n",
      "<p>\"Does this sound like an infrastructure bill to you?\" the group <a href=\"https://twitter.com/Citizens_United/status/1377308915227107336\">tweeted March 31</a>, with a link to a <a href=\"https://www.nytimes.com/live/2021/03/31/us/biden-news-today/biden-introduces-his-infrastructure-plan-calling-it-a-once-in-a-generation-investment-in-america\">New York Times</a> article about the proposal. \"It's not. It's the Green New Deal. \"</p>\n",
      "<p dir=\"ltr\" lang=\"en\">Does this sound like an infrastructure bill to you? It's not. It's the Green New Deal. <br/><br/>\"It is the first step in a two-part agenda to overhaul American capitalism, fight climate change and attempt to improve the productivity of the economy.\"<a href=\"https://t.co/ajIoRCttgl\">https://t.co/ajIoRCttgl</a></p>\n",
      "<p>The Times article described Biden’s plan as the first step in a legislative package that aimed to boost productivity, fight climate change and \"overhaul American capitalism.\"</p>\n"
     ]
    }
   ],
   "source": [
    "for paragraph in soup.find_all('p')[:10]: # first 10 paragraphs via <p> tag\n",
    "    print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Find all the links in the above claim review page using the `<a>` tags and their `href` elements. Print every 10th link. What do you notice about where these links point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "/pennsylvania/\n",
      "/health-check/\n",
      "/personalities/kamala-harris/\n",
      "/personalities/rush-limbaugh/\n",
      "/truth-o-meter/promises/trumpometer/?ruling=true\n",
      "https://twitter.com/Politifact/\n",
      "/staff/jon-greenberg/\n",
      "https://www.congress.gov/bill/116th-congress/house-resolution/109\n",
      "https://twitter.com/Citizens_United/status/1377308915227107336\n",
      "#\n",
      "#\n",
      "#\n",
      "/personalities/joe-biden/\n",
      "/personalities/facebook-posts/\n",
      "/factchecks/list/\n",
      "/personalities/nancy-pelosi/\n",
      "/texas/\n",
      "/corrections-and-updates/\n",
      "https://twitter.com/share?text=PolitiFact - Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.&url=https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "# Your solution here\n",
    "\n",
    "for link in soup.find_all('a')[::10]: # every 10th element\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see lots of relative links (e.g., `/pennsylvania/`), places where the `href` seems to point nowhere (e.g., `#`), and communication shortcuts (e.g., `https://twitter.com/share?text=PolitiFact - Citizens United calls...`). This could be cleaned up by appending relative links to the domain name (`https://www.politifact.com/`) and keeping only URLs (and nothing after)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes these tags aren't very useful--in fact, they can get in the way of extracting only visible or human-readable text from the HTML. This too can be accomplished with `BeautifulSoup`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting human-readable text <a id='readable'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally we want to learn about websites via their tags: What the headers say, which paragraph comes first, where the links or images are, etc. Other times tags (such as scripts or styles) only introduce extraneous characters and nonsense words, and we want to ignore the tags themselves or even the text they enclose. \n",
    "\n",
    "The simplest way to do this is with the `get_text()` method in `BeautifulSoup`, which returns all the text in a document or beneath a tag, as a single Unicode string. You might have noticed that the `<p>` tags got in the way in our above example. Let's try that again and this time, we will remove the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misinformation isn't going away just because it's a new year. Support trusted, factual information with a tax deductible contribution to PolitiFact.\n",
      "More Info\n",
      "The White House infrastructure plan has $111 billion to improve water and sewer systems. (Shutterstock)\n",
      "The White House infrastructure plan would cost about $2.3 trillion. A Green New Deal-type plan would cost $9.5 trillion.\n",
      "The Green New Deal included broader social economic goals, such as a guaranteed livable wage, affordable higher education and universal health care.\n",
      "Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal.\n",
      "Senate Minority Leader Mitch McConnell said that as written, the $2.3 trillion American Jobs Plan released March 31 was a nonstarter. The conservative PAC Citizens United put Biden’s plan in the same boat as the Green New Deal, a sweeping environmental and social justice agenda that Republicans have condemned.\n",
      "\"Does this sound like an infrastructure bill to you?\" the group tweeted March 31, with a link to a New York Times article about the proposal. \"It's not. It's the Green New Deal. \"\n",
      "Does this sound like an infrastructure bill to you? It's not. It's the Green New Deal. \"It is the first step in a two-part agenda to overhaul American capitalism, fight climate change and attempt to improve the productivity of the economy.\"https://t.co/ajIoRCttgl\n",
      "The Times article described Biden’s plan as the first step in a legislative package that aimed to boost productivity, fight climate change and \"overhaul American capitalism.\"\n"
     ]
    }
   ],
   "source": [
    "for paragraph in soup.find_all('p')[:10]: # first 10 paragraphs via <p> tag\n",
    "    print(paragraph.get_text().strip()) # extract text and strip trailing spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also easy to call the first element of the soup object matching a given tag, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Misinformation isn't going away just because it's a new year. Support trusted, factual information with a tax deductible contribution to PolitiFact.\""
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.get_text() # Get text of first paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is `extract()`, which can be used to surgically remove a tag or string from the soup tree, storing it for safe keeping. Let's extract the first 5 links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted links: [<a href=\"/\">\n",
      "<span class=\"m-branding\">\n",
      "<span class=\"m-branding__logo\">\n",
      "<svg class=\"c-icon\">\n",
      "<use xlink:href=\"#svg_logo-plain\"></use>\n",
      "</svg>\n",
      "</span>\n",
      "<span class=\"m-branding__subline\">\n",
      "<span class=\"m-branding__claim\">\n",
      "The Poynter Institute\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</a>, <a class=\"c-burger\" data-menu-toggle=\"\" href=\"#\">\n",
      "<span class=\"c-burger__lines\"></span>\n",
      "<span class=\"c-burger__value\">Menu</span>\n",
      "</a>, <a class=\"c-button c-button--small show-for-large\" href=\"/membership/\">\n",
      "Donate\n",
      "</a>, <a href=\"/california/\">\n",
      "California\n",
      "</a>, <a href=\"/florida/\">\n",
      "Florida\n",
      "</a>]\n",
      "\n",
      "<a href=\"/illinois/\">\n",
      "Illinois\n",
      "</a>\n",
      "<a href=\"/iowa/\">\n",
      "Iowa\n",
      "</a>\n",
      "<a href=\"/missouri/\">\n",
      "Missouri\n",
      "</a>\n",
      "<a href=\"/new-york/\">\n",
      "New York\n",
      "</a>\n",
      "<a href=\"/north-carolina/\">\n",
      "North Carolina\n",
      "</a>\n"
     ]
    }
   ],
   "source": [
    "extracted = [] # initialize list of extracted links\n",
    "\n",
    "for link in soup.find_all('a')[:5]: # get first ten <a> tags\n",
    "    extracted.append(link.extract()) # extract the link\n",
    "    \n",
    "print('Extracted links:', extracted)\n",
    "print()\n",
    "\n",
    "# What are the first 10 links now the the previous 10 were removed? \n",
    "for link in soup.find_all('a')[:5]: \n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we don't want to keep the tag at all? In this case, we would use `decompose()`, which obliterates a useless tag (and frees up memory). Unlike with `extract()`, with `decompose()` you don't need to assign the junk tag to anything to clear it--the method does this automatically. \n",
    "\n",
    "Let's try the above code again, this time with `decompose()` and `get_text()` to clean up the display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illinois\n",
      "Iowa\n",
      "Missouri\n",
      "New York\n",
      "North Carolina\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "for link in soup.find_all('a')[:5]: # get first ten <a> tags\n",
    "    link.decompose() # obliterate this link\n",
    "    \n",
    "# What are the first 10 links now the the previous 10 were removed? \n",
    "for link in soup.find_all('a')[:5]: \n",
    "    print(link.get_text().strip()) # get text and clean spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all websites use the `<p>` tag to indicate the important, human-readable text. Sometimes we need to approach HTML parsing from the other end: By finding and removing all non-informative tags. Let's use `BeautifulSoup` to build such a method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use `decompose()` to remove from the soup all tags showing anything other than human-readable text. Below is a list of such junk tags to use as a blacklist. \n",
    "\n",
    "```\n",
    "\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\", \"kbd\", \n",
    "\"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\", \"span\", \"sub\", \"sup\", \"head\", \n",
    "\"title\", \"[document]\", \"script\", \"style\", \"meta\", \"noscript\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " March 31, 2021 in a tweet:Says Joe Biden’s infrastructure plan “is the Green New Deal.”The White House infrastructure plan has $111 billion to improve water and sewer systems. (Shutterstock)ByJon GreenbergCitizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.If Your Time is shortThe White House infrastructure plan would cost about $2.3 trillion. A Green New Deal-type plan would cost $9.5 trillion.The Green New Deal included broader social economic goals, such as a guaranteed livable wage, affordable higher education and universal health care.See the sources for this fact-checkRepublican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal.Senate Minority Leader Mitch McConnell said that as written, the $2.3 trillion American Jobs Plan released March 31 was a nonstarter. The conservative PAC Citizens United put Biden’s plan in the same boat as theGreen New Deal, a sweeping environmental and social justice agenda that Republicans have condemned.\"Does this sound like an infrastructure bill to you?\" the grouptweeted March 31, with a link to aNew York Timesarticle about the proposal. \"It's not. It's the Green New Deal. \"Does this sound like an infrastructure bill to you? It's not. It's the Green New Deal.\"It is the first step in a two-part agenda to overhaul American capitalism, fight climate change and attempt to improve the productivity of the economy.\"https://t.co/ajIoRCttgl— Citizens United (@Citizens_United)March 31, 2021The Times article described Biden’s plan as the first step in a legislative package that aimed to boost productivity, fight climate change and \"overhaul American capitalism.\"During the presidential campaign,Biden said he does not supportthe Green New Deal. His current proposal is a blend of money for traditional brick-and-mortar infrastructure — roads, water supplies, broadband, etc. — clean energy and improved manufacturing, and social service infrastructure, meaning caregivers for seniors a\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "\n",
    "# Define inline tags for cleaning out HTML\n",
    "tags_blacklist = [\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\", \"kbd\", \n",
    "                  \"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\", \"span\", \"sub\", \"sup\", \"head\", \n",
    "                  \"title\", \"[document]\", \"script\", \"style\", \"meta\", \"noscript\"]\n",
    "\n",
    "# Get HTML and then soup\n",
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "# Remove non-visible tags from soup with two for-loops:\n",
    "for tag in tags_blacklist:\n",
    "    for elem in soup(tag):\n",
    "        elem.decompose()\n",
    "        \n",
    "# Show result\n",
    "visible = soup.get_text(strip=True)\n",
    "print(visible[1000:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that word boundaries get clobbered when you call `get_text()`. This is because the default setting for this method is `strip=True`, which tells `BeautifulSoup` to strip whitespaces (of any kind) from the beginning and end of each bit of text. Using `strip=False` leads to lots of extra whitespaces--usually, newlines--which requires some regular expressions to clean up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Using the above tags blacklist and `decompose()` as before, this time use the `strip=False` parameter when calling `get_text()` to avoid combining words across whitespace boundaries. Instead, use regular expressions to clean up extra whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Donate\n",
      "State Editions\n",
      "California\n",
      "Florida\n",
      "Illinois\n",
      "Iowa\n",
      "Missouri\n",
      "New York\n",
      "North Carolina\n",
      "Pennsylvania\n",
      "Texas\n",
      "Virginia\n",
      "West Virginia\n",
      "Vermont\n",
      "Wisconsin\n",
      "Michigan\n",
      "Issues\n",
      "All Issues\n",
      "Online hoaxes\n",
      "Coronavirus\n",
      "Health Care\n",
      "Immigration\n",
      "Taxes\n",
      "Marijuana\n",
      "Environment\n",
      "Crime\n",
      "Guns\n",
      "Foreign Policy\n",
      "People\n",
      "All People\n",
      "Joe Biden\n",
      "Kamala Harris\n",
      "Charles Schumer\n",
      "Mitch McConnell\n",
      "Bernie Sanders\n",
      "Nancy Pelosi\n",
      "Donald Trump\n",
      "Media\n",
      "PunditFact\n",
      "Tucker Carlson\n",
      "Sean Hannity\n",
      "Rachel Maddow\n",
      "Rush Limbaugh\n",
      "Bloggers\n",
      "Campaigns\n",
      "2020 Elections\n",
      "Truth-o-Meter\n",
      "True\n",
      "Mostly True\n",
      "Half True\n",
      "Mostly False\n",
      "False\n",
      " \n",
      "Pants on Fire\n",
      "Promises\n",
      "Biden Promise Tracker\n",
      "Trump-O-Meter\n",
      "Obameter\n",
      "Latest Promises\n",
      "About Us\n",
      "Our Process\n",
      "Our Staff\n",
      "Who pays for Politifact?\n",
      "Advertise with Us\n",
      "Suggest a Fact-check\n",
      "Corrections and Updates\n",
      "Donate\n",
      "Follow us\n",
      " \n",
      "The Facts Newsletter\n",
      "Sign up\n",
      "Stand up for the facts!\n",
      "Misinformation isn't going away just because it's a new year. Support trusted, factual information with a tax deductible contribution to PolitiFact.\n",
      "More Info\n",
      "I would like to contribute\n",
      "One Time\n",
      "Monthly\n",
      "Yearly\n",
      "Join Now\n",
      "Citizens United\n",
      "stated on March 31, 2021 in a tweet:\n",
      "Says Joe Biden’s infrastructure plan “is the Green New Deal.”\n",
      "The White House infrastructure plan has $111 billion to improve water and sewer systems. (Shutterstock)\n",
      "By Jon Greenberg\n",
      "Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.\n",
      "If Your Time is short\n",
      "The White House infrastructure plan would cost about $2.3 trillion. A Green New Deal-type plan would cost $9.5 trillion.\n",
      "The Green New Deal included broader social economic goals, such as a guaranteed livable wage, affordable higher education and universal health care.\n",
      "See the sources for this fact-check\n",
      "Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal.\n",
      "Senate Minority Leader Mitch McConnell said that as written, the $2.3 trillion American Jobs Plan released March 31 was a nonstarter. The conservative PAC Citizens United put Biden’s plan in the same boat as the Green New Deal, a sweeping environmental and social justice agenda that Republicans have condemned.\n",
      "\"Does this sound like an infrastructure bill to you?\" the group tweeted March 31, with a link to a New York Times article about the proposal. \"It's not. It's the Green New Deal. \"\n",
      "Does this sound like an infrastructure bill to you? It's not. It's the Green New Deal. \"It is the first step in a two-part agenda to overhaul American capitalism, fight climate change and attempt to improve the productivity of the economy.\"https://t.co/ajIoRCttgl— Citizens United (@Citizens_United) March 31, 2021 \n",
      "The Times article described Biden’s plan as the first step in a legislative package that aimed to boost productivity, fight climate change and \"overhaul American capitalism.\"\n",
      "During the presidential campaign, Biden said he does not support the Green New Deal. His current proposal is a blend of money for traditional brick-and-mortar infrastructure — roads, water supplies, broadband, etc. — clean energy and improved manufacturing, and social service infrastructure, meaning caregivers for seniors and the disabled.\n",
      "We called and emailed Citizens United to ask on what basis they equated the White House initiative with the Green New Deal. We did not hear back.\n",
      "There’s overlap between the two, but also considerable differences.\n",
      "Differences in size and scope\n",
      "The Green New Deal was a House resolution introduced by Rep. Alexandria Ocasio-Cortez, D-N.Y., that laid out a set of climate, economic and social goals. Ocasio-Cortez criticized the Biden plan, saying \"This is not nearly enough.\"\n",
      "The 2019 resolution was not a line-by-line spending plan. But the Green New Deal inspired the crafting of the THRIVE Act, legislation due to be introduced in April by a group of Democrats in Congress to advance the goals of the resolution.\n",
      "Featured Fact-check\n",
      "Facebook posts\n",
      "stated on March 29, 2021 in a Facebook post\n",
      "Georgia’s new voting law “makes it a JAIL-TIME CRIME to drop off grandma’s absentee ballot in a dropbox.”\n",
      "By Amy Sherman • March 30, 2021\n",
      "The THRIVE Act does have a proposed budget. The Green New Deal Network, a group of advocacy groups, summarized it in a spreadsheet, and it comes out to much more than the American Jobs Plan. \n",
      "Over a 10-year span, it would spend about $9.5 trillion, or about $7 trillion more than Biden’s proposal. Two of the largest differences are in the areas of clean energy and agriculture.\n",
      "The THRIVE Act would spend about 10 times more than the American Jobs Plan on clean energy. For agriculture, the White House has no spending line, while the THRIVE Act has $1.6 trillion to support farming that uses less fossil fuels and locks more carbon in the soil. (The recently passed stimulus law provided about $5.6 billion in aid to farmers and rural communities.)\n",
      "Here is our summary of how the two spending plans compare across some major categories.\n",
      "The American Jobs Plan also includes about $480 billion to boost manufacturing and research and development, some of which might boost clean energy. The THRIVE Act folds money for those activities into other line items, primarily its investments in clean energy.\n",
      "Ryan Schleeter, spokesman for Greenpeace USA, a Green New Deal Network member, said it is misleading to equate Biden’s proposal with the Green New Deal.\n",
      "\"The American Jobs Plan is similar in intent to the THRIVE Act, but far narrower in scope and scale,\" Schleeter said.\n",
      "Among the broader items on the original Green New Deal agenda were a guaranteed livable wage, affordable higher education and universal health care. The American Jobs Plan does not include those elements.\n",
      "Our ruling\n",
      "Citizens United said that Biden’s infrastructure proposal is the Green New Deal.\n",
      "The two plans share some common approaches, but a spending plan inspired by the Green New Deal is about four times larger than the Biden plan. The Green New Deal also advocates broader social goals that are absent from the White House infrastructure proposal.\n",
      "We rate this claim Mostly False.\n",
      " \n",
      "Our Sources\n",
      "Citizens United, tweet, March 31, 2021\n",
      "New York Times, Biden, in Pennsylvania, Details $2 Trillion Infrastructure Plan, March 31, 2021\n",
      "Alexandria Ocasio-Cortez, tweet, March 31, 2021\n",
      "Sierra Club, White House BBB package and THRIVE, March 31, 2021\n",
      "White House, Fact sheet: The American Jobs Plan, March 31, 2021\n",
      "Green New Deal Network, THRIVE Act, accessed April 2, 2021\n",
      "PolitiFact, 7 questions about the Green New Deal, Feb. 12, 2019\n",
      "Green New Deal Resolution, Feb. 7, 2019\n",
      "U.S. Department of Agriculture, United States Department of Agriculture Provisions in H.R. 1319, the American Rescue Plan, March 10, 2021\n",
      "Email exchange, Ryan Schleeter, spokesman, Greenpeace USA, April 1, 2021\n",
      "Email exchange, Ben Beachy, director, Living Economy, Sierra Club, April 1, 2021\n",
      " \n",
      "Read About Our Process\n",
      "The Principles of the Truth-O-Meter\n",
      "Browse the Truth-O-Meter\n",
      "More by Jon Greenberg\n",
      "Instagram posts\n",
      "stated on April 8, 2021 an Instagram post:\n",
      "Says Officer Derek Chauvin’s knee was “not on neck of George Floyd.”\n",
      "By Jon Greenberg • April 9, 2021\n",
      "Tim Scott\n",
      "stated on April 6, 2021 a tweet:\n",
      "Says Georgia “has more day-of voting rights than Colorado.”\n",
      "By Jon Greenberg • April 7, 2021\n",
      "Citizens United\n",
      "stated on March 31, 2021 a tweet:\n",
      "Says Joe Biden’s infrastructure plan “is the Green New Deal.”\n",
      "By Jon Greenberg • April 2, 2021\n",
      "Instagram posts\n",
      "stated on March 30, 2021 an Instagram post:\n",
      "Calls COVID-19 vaccine a “non-FDA-approved experimental agent.”\n",
      "By Jon Greenberg • March 31, 2021\n",
      "Donald Trump\n",
      "stated on March 29, 2021 a statement given to the press:\n",
      "Says Deborah Birx “traveled a great distance to see her family for Thanksgiving, only to have them call the police and turn her in. She then ... resigned.”\n",
      "By Jon Greenberg • March 30, 2021\n",
      "Cathy McMorris Rodgers\n",
      "stated on March 18, 2021 a House hearing:\n",
      "The Democratic clean energy plan is pro-China, because \"90% of the solar panels, 80% of the wind machines, 90% of the rare earth minerals … are in Asia or in China.”\n",
      "By Jon Greenberg • March 29, 2021\n",
      "Viral image\n",
      "stated on March 22, 2021 a tweet:\n",
      "These 11 mass shootings involved an AR-15-style weapon.\n",
      "By Jon Greenberg • March 24, 2021\n",
      "Kevin McCarthy\n",
      "stated on March 14, 2021 an interview on Fox News:\n",
      "“Democrats have accomplished a lot in just 2 months ... 1 Million energy jobs destroyed.”\n",
      "By Jon Greenberg • March 17, 2021\n",
      "Kevin McCarthy\n",
      "stated on March 14, 2021 a tweet:\n",
      "The COVID economic relief package “included $600 million for San Francisco, part of which goes to cover the tab for free alcohol and marijuana for the homeless.\"\n",
      "By Jon Greenberg • March 16, 2021\n",
      "Bloggers\n",
      "stated on March 8, 2021 a post on Trending Politics:\n",
      "“Schumer and Pelosi sneak funding into COVID bill.”\n",
      "By Jon Greenberg • March 11, 2021\n",
      "Tom Cotton\n",
      "stated on March 6, 2021 a tweet:\n",
      "“Senate Democrats just voted to give stimulus checks to criminals in prison.”\n",
      "By Jon Greenberg • March 9, 2021\n",
      "Marco Rubio\n",
      "stated on March 6, 2021 a tweet:\n",
      "In rejecting his amendment, “Democrats voted to send reopening money to schools that refuse to reopen.”\n",
      "By Jon Greenberg • March 9, 2021\n",
      "Rick Scott\n",
      "stated on March 1, 2021 an interview with CBS News:\n",
      "The $1.9 trillion relief package is “about a bridge for Chuck Schumer, a tunnel for Nancy Pelosi.”\n",
      "By Jon Greenberg • March 2, 2021\n",
      "Ted Budd\n",
      "stated on February 24, 2021 a speech:\n",
      "“There’s about 9% of (the American Rescue Plan) actually going to COVID, meaning 91% of it is not even COVID-related.”\n",
      "By Jon Greenberg • March 1, 2021\n",
      "Mitch McConnell\n",
      "stated on February 24, 2021 a Senate floor speech:\n",
      "In the American Rescue Plan, “just about 1% of the money is for vaccines.”\n",
      "By Jon Greenberg • February 26, 2021\n",
      "Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.\n",
      "Joe Biden\n",
      "stated on April 21, 2021 in remarks:\n",
      "Says his administration’s efforts have led to more school reopenings.\n",
      "By Amy Sherman • April 27, 2021\n",
      "Instagram posts\n",
      "stated on April 15, 2021 in an Instagram post:\n",
      "Mariah Carey faked getting her COVID-19 vaccine because the needle can’t be seen coming out of her arm.\n",
      "By Samantha Putterman • April 27, 2021\n",
      "Donald Trump\n",
      "stated on April 19, 2021 in an interview on Fox News:\n",
      "“Human trafficking and drugs” at the Mexico border have “doubled, tripled and quadrupled” since Joe Biden became president.\n",
      "By Tom Kertscher • April 27, 2021\n",
      "Facebook posts\n",
      "stated on April 25, 2021 in a Facebook post:\n",
      "“The Biden Administration is buying Kamala Harris’ book with our taxpayer dollars.”\n",
      "By Ciara O'Rourke • April 27, 2021\n",
      "Viral image\n",
      "stated on April 23, 2021 in a Facebook post:\n",
      "Photo shows Facebook’s “community standards team.”\n",
      "By Ciara O'Rourke • April 27, 2021\n",
      "Facebook posts\n",
      "stated on April 18, 2021 in a Facebook post:\n",
      "COVID-19 “has killed less people than the damn flu.”\n",
      "By Ciara O'Rourke • April 27, 2021\n",
      "Viral image\n",
      "stated on April 26, 2021 in a Facebook post:\n",
      "Says the Memphis Police Department “will no longer respond” to crimes including larceny, breaking and entering, and motor vehicle theft.\n",
      "By Ciara O'Rourke • April 27, 2021\n",
      "Ted Cruz\n",
      "stated on April 22, 2021 in a press conference:\n",
      "“You didn't see Republicans when we had control of the Senate try to rig the game.”\n",
      "By Louis Jacobson • April 27, 2021\n",
      "Van Wanggaard\n",
      "stated on April 19, 2021 in Statement:\n",
      "Foxconn “is the largest taxpayer in Racine County.”\n",
      "By Haley BeMiller • April 27, 2021\n",
      "Fox News Channel\n",
      "stated on April 23, 2021 in a TV graphic:\n",
      "\"Biden's climate requirements\" will \"cut 90% of red meat from diet\" to a \"max 4 lbs per year\" and \"one burger per month.\"\n",
      "By Bill McCarthy • April 26, 2021\n",
      "Load more\n",
      "Support independent fact-checking.\n",
      "Become a member!\n",
      "In a world of wild talk and fake news, help us stand up for the facts.\n",
      "Sign me up\n",
      "Offices\n",
      "District of Columbia\n",
      "Florida\n",
      "People\n",
      "All People\n",
      "Joe Biden\n",
      "Kamala Harris\n",
      "Charles Schumer\n",
      "Mitch McConnell\n",
      "Bernie Sanders\n",
      "Nancy Pelosi\n",
      "Donald Trump\n",
      " \n",
      "State Editions\n",
      "California\n",
      "Florida\n",
      "Illinois\n",
      "Iowa\n",
      "Missouri\n",
      "New York\n",
      "North Carolina\n",
      "Pennsylvania\n",
      "Texas\n",
      "Virginia\n",
      "West Virginia\n",
      "Vermont\n",
      "Wisconsin\n",
      "Michigan\n",
      "About Us\n",
      "Our Process\n",
      "Our Staff\n",
      "Who pays for PolitiFact?\n",
      "Advertise with Us\n",
      "Corrections and Updates\n",
      "RSS Feeds\n",
      "Recent Articles and Fact-checks\n",
      "Recent Fact-checks\n",
      "Suggest a Fact-check\n",
      "Follow us\n",
      "The Facts Newsletter\n",
      "Sign up\n",
      "Terms & Conditions\n",
      "Privacy Policy\n",
      "Copyright\n",
      "©All Rights Reserved Poynter Institute 2020, a 501(c)(3) nonprofit organization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "\n",
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "# Faster way to remove non-visible tags from soup:\n",
    "[s.decompose() for s in soup(tags_blacklist)]\n",
    "\n",
    "# Don't strip spaces in-between elements, to avoid clobbering word boundaries\n",
    "visible = soup.get_text(strip=False)\n",
    "\n",
    "import re\n",
    "#visible = re.sub(r\"\\n+\", \"\\n\", visible) # This works, but less extensible than below\n",
    "\n",
    "import regex # better unicode support than Python's built-in re package\n",
    "\n",
    "# Use regex to replace all consecutive spaces (including in unicode), tabs, or \"|\"s with a single space\n",
    "visible = regex.sub(r\"[ \\t\\h\\|]+\", \" \", visible)\n",
    "# Replace any consecutive linebreaks with a single space\n",
    "visible = regex.sub(r\"[\\n\\r\\f\\v]+\", \"\\n\", visible)\n",
    "\n",
    "print(visible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "You might have noticed that when we scraped HTML above from [this claim review by PolitiFact](https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/), we got headers and tags like this:\n",
    "```html\n",
    "<p>Misinformation isn't going away just because it's a new year. Support trusted, factual information with a tax deductible contribution to PolitiFact.</p>\n",
    "<p>\n",
    "<a class=\"m-disruptor-content__link\" href=\"/membership/\">More Info</a>\n",
    "</p>\n",
    "<p class=\"c-image__caption-inner copy-xs\">\n",
    "The White House infrastructure plan has $111 billion to improve water and sewer systems. (Shutterstock)\n",
    "</p>\n",
    "```\n",
    "Use what you now know about identifying HTML, removing tags, and cleaning spacing to scrape a clean explanation from the body of this article. \n",
    "\n",
    "_Hint:_ Use your browser to inspect this website's HTML and identify any unique types and/or classes that enclose the explanation (and nothing else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal.\n",
      "Senate Minority Leader Mitch McConnell said that as written, the $2.3 trillion American Jobs Plan released March 31 was a nonstarter. The conservative PAC Citizens United put Biden’s plan in the same boat as the Green New Deal, a sweeping environmental and social justice agenda that Republicans have condemned.\n",
      "\"Does this sound like an infrastructure bill to you?\" the group tweeted March 31, with a link to a New York Times article about the proposal. \"It's not. It's the Green New Deal. \"\n",
      "Does this sound like an infrastructure bill to you? It's not. It's the Green New Deal. \"It is the first step in a two-part agenda to overhaul American capitalism, fight climate change and attempt to improve the productivity of the economy.\"https://t.co/ajIoRCttgl— Citizens United (@Citizens_United) March 31, 2021 \n",
      "The Times article described Biden’s plan as the first step in a legislative package that aimed to boost productivity, fight climate change and \"overhaul American capitalism.\"\n",
      "During the presidential campaign, Biden said he does not support the Green New Deal. His current proposal is a blend of money for traditional brick-and-mortar infrastructure — roads, water supplies, broadband, etc. — clean energy and improved manufacturing, and social service infrastructure, meaning caregivers for seniors and the disabled.\n",
      "We called and emailed Citizens United to ask on what basis they equated the White House initiative with the Green New Deal. We did not hear back.\n",
      "There’s overlap between the two, but also considerable differences.\n",
      "Differences in size and scope\n",
      "The Green New Deal was a House resolution introduced by Rep. Alexandria Ocasio-Cortez, D-N.Y., that laid out a set of climate, economic and social goals. Ocasio-Cortez criticized the Biden plan, saying \"This is not nearly enough.\"\n",
      "The 2019 resolution was not a line-by-line spending plan. But the Green New Deal inspired the crafting of the THRIVE Act, legislation due to be introduced in April by a group of Democrats in Congress to advance the goals of the resolution.\n",
      "Featured Fact-check\n",
      "Facebook posts\n",
      "stated on March 29, 2021 in a Facebook post\n",
      "Georgia’s new voting law “makes it a JAIL-TIME CRIME to drop off grandma’s absentee ballot in a dropbox.”\n",
      " \n",
      "By Amy Sherman • March 30, 2021\n",
      "The THRIVE Act does have a proposed budget. The Green New Deal Network, a group of advocacy groups, summarized it in a spreadsheet, and it comes out to much more than the American Jobs Plan. \n",
      "Over a 10-year span, it would spend about $9.5 trillion, or about $7 trillion more than Biden’s proposal. Two of the largest differences are in the areas of clean energy and agriculture.\n",
      "The THRIVE Act would spend about 10 times more than the American Jobs Plan on clean energy. For agriculture, the White House has no spending line, while the THRIVE Act has $1.6 trillion to support farming that uses less fossil fuels and locks more carbon in the soil. (The recently passed stimulus law provided about $5.6 billion in aid to farmers and rural communities.)\n",
      "Here is our summary of how the two spending plans compare across some major categories.\n",
      "!function(e,i,n,s){var t=\"InfogramEmbeds\",d=e.getElementsByTagName(\"script\")[0];if(window[t]&&window[t].initialized)window[t].process&&window[t].process();else if(!e.getElementById(n)){var o=e.createElement(\"script\");o.async=1,o.id=n,o.src=\"https://e.infogram.com/js/dist/embed-loader-min.js\",d.parentNode.insertBefore(o,d)}}(document,0,\"infogram-async\");\n",
      "The American Jobs Plan also includes about $480 billion to boost manufacturing and research and development, some of which might boost clean energy. The THRIVE Act folds money for those activities into other line items, primarily its investments in clean energy.\n",
      "Ryan Schleeter, spokesman for Greenpeace USA, a Green New Deal Network member, said it is misleading to equate Biden’s proposal with the Green New Deal.\n",
      "\"The American Jobs Plan is similar in intent to the THRIVE Act, but far narrower in scope and scale,\" Schleeter said.\n",
      "Among the broader items on the original Green New Deal agenda were a guaranteed livable wage, affordable higher education and universal health care. The American Jobs Plan does not include those elements.\n",
      "Our ruling\n",
      "Citizens United said that Biden’s infrastructure proposal is the Green New Deal.\n",
      "The two plans share some common approaches, but a spending plan inspired by the Green New Deal is about four times larger than the Biden plan. The Green New Deal also advocates broader social goals that are absent from the White House infrastructure proposal.\n",
      "We rate this claim Mostly False.\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "\n",
    "# Set URL to scrape\n",
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "\n",
    "# Scrape HTML with requests and beautifulsoup\n",
    "html = requests.get(url) \n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "explanation = soup.find('article', class_='m-textblock').get_text() # identify this class from looking at HTML\n",
    "\n",
    "import re\n",
    "explanation = re.sub(r\"\\n+\", \"\\n\", explanation)\n",
    "\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the output from this focused, site-specific scraping approach with that from the blacklist method above. <br/>\n",
    "**Which method gives the cleaner output? Which method is more extensible?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling broadly with `Scrapy` <a id='scrapy'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a _Twisted_ application, Scrapy is event-driven, asynchronous, and is virtually multi-threaded (while using only one thread). While other programs cause _blocks_ when they access files or the web, spawn new processes, or do system operations, Scrapy instead waits until a resource is available, solves the immediate problem, and then calls another task. In short, Scrapy is fast, flexible, and scalable. It offers one of the most user-friendly ways to write crawling programs that can move across heterogeneous swaths of the internet, download stuff, and not break. \n",
    "\n",
    "To grasp the intuition behind Scrapy, imagine a bank where tellers (threads) are available to see customers (processes), who need to fill out forms before they're done. Such a situation could be configured in these ways:\n",
    "\n",
    "- _Blocking_ operation with a _single_ thread: Here there is 1 teller trying to help 5 customers. When customer 1 needs time to fill out a form, then teller 1 is occupied waiting for customer 1--and all the other customers are stuck in line.\n",
    "- _Blocking_ operation with _multiple_ threads: Now there are still 5 customers, but there are 3 tellers. When customer 1 needs time to fill out a form, then teller 1 is occupied. Customer 2 may have access to teller 2 and customer 3 to teller 3, but then all the tellers are monopolized while people fill out forms, which means customers 4 and 5 are still stuck waiting in line. \n",
    "- _Non-blocking_ operation with a _single_ thread: Here again we have 1 teller and 5 customers. When customer 1 needs time to fill out a form, they stand aside so the single teller can help customer 2. When customer 1 is finished, they wait until customer 2 is done or has something to do, then customer 1 is called back to continue being helped. If customers 1 and 2 both have forms to complete, they can do that on the side and the single teller can see customer 3, and so on. \n",
    "\n",
    "You can see this last situation is way more efficient than the previous two. This is the Scrapy default; when _multiple_ threads are available for a _non-blocking_ operation (like when multiple spiders work together), this is even better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start scraping, you will have to set up a new Scrapy project. Enter a directory where you’d like to store your code and run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ scrapy startproject schools\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a `schools` directory with the following contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "schools/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "    tutorial/             # project's Python module, you'll import your code from here\n",
    "        __init__.py\n",
    "        items.py          # project items definition file\n",
    "        middlewares.py    # project middlewares file\n",
    "        pipelines.py      # project pipelines file\n",
    "        settings.py       # project settings file\n",
    "        spiders/          # a directory where you'll later put your spiders\n",
    "            __init__.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple (narrow) spider <a id='simple'> </a>\n",
    "\n",
    "Spiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass `scrapy.Spider` and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data.\n",
    "\n",
    "This is the code for our first Spider. Save it in a file named `simple_spider.py` under the `schools/spiders` directory in your project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import scrapy\n",
    "\n",
    "class SimpleSpider(scrapy.Spider):\n",
    "    name = \"simple\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our Spider subclasses `scrapy.Spider` and defines some attributes and methods:\n",
    "\n",
    "-`name`: identifies the Spider. It must be unique within a project, that is, you can’t set the same name for different Spiders.\n",
    "\n",
    "-`start_urls`: a list of URLs to provide the initial requests for the crawler. Armed with this list alone, the spider will download HTML from the webpages specified, much as a web browser does. But it won't extract anything from the pages--that's why we need to define the `parse()` method.\n",
    "\n",
    "-`parse()`: a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of `TextResponse` that holds the page content and has further helpful methods to handle it.\n",
    "\n",
    "The `parse()` method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (`Request`) from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put our spider to work, go to the project’s top level directory and run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ scrapy crawl simple\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command runs the spider with name `simple` that we’ve just added, that will send some requests for the `quotes.toscrape.com` domain. You will get an output similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "...\n",
    "2019-03-19 15:58:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2019-03-19 15:58:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] INFO: Spider opened\n",
    "2019-03-19 15:58:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2019-03-19 15:58:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
    "2019-03-19 15:58:49 [simple] DEBUG: Saved file quotes-1.html\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2019-03-19 15:58:50 [simple] DEBUG: Saved file quotes-2.html\n",
    "2019-03-19 15:58:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2019-03-19 15:58:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 678,\n",
    " 'downloader/request_count': 3,\n",
    " ...}\n",
    "2019-03-19 15:58:50 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the files in the current directory. You should notice that two new files have been created: `quotes-1.html` and `quotes-2.html`, with the content for the respective URLs, as our `parse` method instructs.\n",
    "\n",
    "How did this work? Scrapy schedules the `scrapy.Request` objects returned by the `start_requests` method of the Spider. Upon receiving a response for each one, it instantiates `Response` objects and calls the callback method associated with the request (in this case, the `parse` method) passing the response as argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Modify and run the spider script above to scrape this short list of `start_urls`: \n",
    "```python\n",
    "['http://www.baylessk12.org/', 'https://crcc.doniphanr1.k12.mo.us/', 'https://www.hazelwoodschools.org/southeastmiddle']\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data\n",
    "The best way to learn how to extract data with Scrapy is trying selectors using the shell [Scrapy shell](https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell). Remember to always enclose URLs in quotes (double quotes for Windows) when running Scrapy shell from command-line, otherwise urls containing arguments (ie. `&` character) will not work. Run:\n",
    "\n",
    "```shell\n",
    "$ scrapy shell 'http://quotes.toscrape.com'\n",
    "```\n",
    "You will see something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "2019-03-19 20:00:05 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: tutorial)\n",
    "2019-03-19 20:00:05 [scrapy.utils.log] INFO: Versions: lxml 4.3.1.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 17.9.0, Python 3.6.7 (default, Oct 22 2018, 11:32:17) - [GCC 8.2.0], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Linux-4.15.0-45-generic-x86_64-with-Ubuntu-18.04-bionic\n",
    "2019-03-19 20:00:05 [scrapy.crawler] INFO: Overridden settings: {...}\n",
    "2019-03-19 20:00:05 [scrapy.extensions.telnet] INFO: Telnet Password: 030319d194e7f6b0\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage']\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    "...]\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    "...]\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2019-03-19 20:00:05 [scrapy.core.engine] INFO: Spider opened\n",
    "2019-03-19 20:00:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2019-03-19 20:00:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f07993c02b0>\n",
    "... \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the shell, you can try selecting elements using CSS with the response object:\n",
    "```shell\n",
    ">>> response.css('title')\n",
    "```\n",
    "The result of running `response.css('title')` is a list-like object called SelectorList, which represents a list of Selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data.\n",
    "\n",
    "To extract the text from the title above, you can do:\n",
    "```shell\n",
    ">>> response.css('title::text').getall()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to note here: one is that we’ve added `::text` to the CSS query, to mean we want to select only the text elements directly inside `<title>` element. If we don’t specify `::text`, we’d get the full title element, including its tags:\n",
    "```shell\n",
    ">>> response.css('title').getall()\n",
    "```\n",
    "The other thing is that the result of calling `.getall()` is a list: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do:\n",
    "```shell\n",
    ">>> response.css('title::text').get()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the `getall()` and `get()` methods, you can also use the `re()` method to extract using regular expressions:\n",
    "```shell\n",
    ">>> response.css('title::text').re(r'Quotes.*')\n",
    ">>> response.css('title::text').re(r'Q\\w+')\n",
    ">>> response.css('title::text').re(r'(\\w+) to (\\w+)')\n",
    "```\n",
    "In order to find the proper CSS selectors to use, you can use your browser developer tools (e.g., in Chrome, right click > `Inspect`) to inspect the HTML and come up with a selector (for more info, see [Using your browser’s Developer Tools for scraping](https://docs.scrapy.org/en/latest/topics/developer-tools.html)). You can also try opening the response page from the shell in your web browser using `view(response)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Inspect [quotes.toscrape.com](quotes.toscrape.com) for the selectors associated with quotes. Use this information to display the text of one of the quotes in the scrapy shell. <br>\n",
    "**Hint 1:** If you need help getting a better sense of website structure, use the HTML tree below (under \"Extracting quotes and authors\") as a visual guide.<br>\n",
    "**Hint 2:** You can subset within selectors by using periods and spaces. For instance, the following produces a SelectorList for the class2 of each type2 within the class1 of each type1:\n",
    "```shell\n",
    "response.css('type1.class1 type2.class2')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "```shell\n",
    "response.css(div.quote span.text::text).get()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining items\n",
    "\n",
    "Within the project directory just created, there’s an `items.py` file. Items add structure to our scraping results and are used by spiders.\n",
    "\n",
    "Here you can add class fields such as url, images, or locations. These fields can be filled by pipelines (a more advanced topic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from scrapy.item import Item, Field\n",
    "\n",
    "class SchoolsItem(Item):\n",
    "    text = Field()\n",
    "    author = Field()\n",
    "    tags = Field()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting quotes and authors\n",
    "Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page.\n",
    "\n",
    "Each quote in http://quotes.toscrape.com is represented by HTML elements that look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<div class=\"quote\">\n",
    "    <span class=\"text\">“The world as we have created it is a process of our\n",
    "    thinking. It cannot be changed without changing our thinking.”</span>\n",
    "    <span>\n",
    "        by <small class=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "    </span>\n",
    "    <div class=\"tags\">\n",
    "        Tags:\n",
    "        <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "        <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "        <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "        <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "    </div>\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we extract the data we want? To start, we get a list of selectors for the quote HTML elements with:\n",
    "\n",
    "```shell\n",
    ">>> response.css(\"div.quote\")\n",
    "```\n",
    "\n",
    "Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:\n",
    "```shell\n",
    ">>> quote = response.css(\"div.quote\")[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s extract title, author and the tags from that quote using the quote object we just created:\n",
    "\n",
    "```shell\n",
    ">>> title = quote.css(\"span.text::text\").get()\n",
    ">>> title\n",
    ">>> author = quote.css(\"small.author::text\").get()\n",
    ">>> author\n",
    "```\n",
    "\n",
    "Given that the tags are a list of strings, we can use the .getall() method to get all of them:\n",
    "```shell\n",
    ">>> tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    ">>> tags\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary. Copy and paste each of these subsequent lines into scrapy shell (or type them in via split-screen):\n",
    "```shell\n",
    ">>> for quote in response.css(\"div.quote\"):\n",
    "...     text = quote.css(\"span.text::text\").get()\n",
    "...     author = quote.css(\"small.author::text\").get()\n",
    "...     tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "...     print(dict(text=text, author=author, tags=tags))\n",
    ">>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data in our spider\n",
    "Let’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider.\n",
    "\n",
    "A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the `yield` Python keyword in the callback, as you can see below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class SimpleSpider(scrapy.Spider):\n",
    "    name = \"simple\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this spider, it will output the extracted data with the log:\n",
    "```shell\n",
    "2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
    "{'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}\n",
    "2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
    "{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the scraped data\n",
    "The simplest way to store the scraped data is by using Feed exports, with the following command:\n",
    "```shell\n",
    "$ scrapy crawl quotes -o quotes.json\n",
    "```\n",
    "\n",
    "That will generate an quotes.json file containing all scraped items, serialized in JSON.\n",
    "\n",
    "For historic reasons, Scrapy appends to a given file instead of overwriting its contents. If you run this command twice without removing the file before the second time, you’ll end up with a long JSON file--actually, a broken JSON file, which cannot be read.\n",
    "\n",
    "You can also use other formats, like JSON Lines:\n",
    "```shell\n",
    "$ scrapy crawl quotes -o quotes.jl\n",
    "```\n",
    "\n",
    "The JSON Lines format is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory.\n",
    "\n",
    "In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in `schools/pipelines.py`. Though you don’t need to implement any item pipelines if you just want to store the scraped items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link extraction in a (broad) spider <a id='linkextraction'> </a>\n",
    "\n",
    "Let’s say, instead of just scraping the stuff from the first two pages from http://quotes.toscrape.com, you want quotes from all the pages in the website. We could do this by adding more URLs to the `start_urls` field, but this sounds like work. We could also read them in via a text file, but that doesn't sound any easier.\n",
    "\n",
    "The best way would be to find the URLs of within-domain links from the website itself, extract data from these pages, go one level still further down, and so on. We could do this by identifying and extracting links using their CSS selectors, and this would let us scrape all the quotes across all the pages across this website (there are steps for this in [the `Scrapy` tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)). But CSS selectors are site-specific, so what about other websites? Could we use `Scrapy` to crawl all the links reliably across different kinds of websites?\n",
    "\n",
    "Yes! In fact, this kind of _broad crawling_ is (arguably) where `Scrapy` shines the most. Crawlers typically mix _horizontal crawling_, crawling pages at the same hierarchical level (for example, from `school.com/page1` to `school.com/page2`), with _vertical crawling_, moving from a higher hierarchical level (for example, `school.com/main`) to a lower one (for example, `school.com/main/about_us`). `Scrapy` makes doing both of these easy by providing the `CrawlSpider` class, from which we can borrow using the `genspider` command:\n",
    "\n",
    "```shell\n",
    "$ scrapy genspider -t crawl broad http://quotes.toscrape.com  # borrows from `crawl` spider template\n",
    "Created spider 'broad' using template 'crawl' in module:\n",
    "  schools.spiders.broad\n",
    "```\n",
    "Go ahead and execute this command in your terminal, and check out the resulting file in `schools/spiders/broad.py`. It should look like this:\n",
    "\n",
    "```python\n",
    "class BroadSpider(CrawlSpider):\n",
    "    name = 'broad'\n",
    "    allowed_domains = ['http://quotes.toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        item = {}\n",
    "        ...\n",
    "        return item\n",
    "```\n",
    "It's worth explaining this a bit. `CrawlSpider` provides an implementation of the `parse()` method that uses the `rules` variable to allow easy two-direction (both horizontal and vertical) crawling. For instance, `Scrapy` by default avoids duplicate requests. \n",
    "\n",
    "Unless a callback is set, a Rule will follow the extracted URLs, which means that it will scan target pages for extra links and follow them. If a callback is set, the Rule won't follow the links from target pages. If you would like it to follow links, you should either return/yield them from your callback method, or set the follow argument of Rule() to true (which is what we will do). \n",
    "\n",
    "How does this spider find links to follow? As their name implies, LinkExtractors are specialized in extracting links, so by default, they are looking for the `a` (and `area`) `href` HTML tags or attributes. Links are ordered using by  \"last in, first out\", meaning that it scrapes a page and all its sublinks before visiting the next page.\n",
    "\n",
    "Now let's use adapt this template to scrape from all the pages on our sample website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Adapt the `CrawlSpider` in `broad.py` to scrape the text, author, and tag for each quote across all the page on `http://quotes.toscrape.com`. Edit the script first, then run it via your terminal, then check the output to make sure.\n",
    "\n",
    "```python\n",
    "# Your solution here\n",
    "```\n",
    "\n",
    "```python\n",
    "class BroadSpider(CrawlSpider):\n",
    "    name = 'broad'\n",
    "    allowed_domains = ['http://quotes.toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(), \n",
    "             callback='parse_item', \n",
    "             follow=True),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "```\n",
    "\n",
    "Call it like so:\n",
    "```shell\n",
    "scrapy crawl broad -o quotes_broad.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the parse method, we can define items.\n",
    "\n",
    "```python\n",
    "item = PropertiesItem()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item properties can then be set with the response we get from parsing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def parse(self, response):\n",
    "    item = PropertiesItem()      \n",
    "    item['image_urls'] = response.xpath(\n",
    "         '//*[@itemprop=\"image\"][1]/@src').extract()\n",
    "    return item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse and finally create these items, run the spider with `scrapy crawl broad` in the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-direction crawling with a spider\n",
    "\n",
    "We can crawl both horizontally and vertically in the same parse method, using Request. \n",
    "\n",
    "This can be done manually with two calls Request. However, since this is done commonly, there is a crawling template we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ scrapy genspider -t crawl SPIDER_NAME web # Creates the “crawl” template.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the default rule with two rules, one for horizontal and one for vertical crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = (\n",
    "    Rule(LinkExtractor(restrict_xpaths='//*[contains(@class,\"next\")]')),\n",
    "    Rule(LinkExtractor(restrict_xpaths='//*[@itemprop=\"url\"]'),\n",
    "         callback='parse_item')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LinkExtractor()` by default searches for `a`, `area`, and `href` HTML tags or attributes.\n",
    "\n",
    "What does the callback argument do in a rule? By default, if not explicitly set, it follows/crawls links based on the rule. With it set, you have to explicitly return links from your callback if you still want to follow links.\n",
    "\n",
    "Behind the scenes Scrapy uses last in, first out for processing requests (depth first crawl). This means we visit a page and all of its sub-links, before visiting the next page. Furthermore, Scrapy avoids duplicate requests. These default behaviors can be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring LinkExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrawlSpider, LinkExtractor, Rule\n",
    "from scrapy.spider import CrawlSpider, Rule\n",
    "from scrapy.linkextractor import LinkExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CrawlSpider, LinkExtractors, and Rule are classes in scrapy.\n",
    "- CrawlSpider is a derived class of Spider, with more methods and functions.\n",
    "   - CrawlSpider follows links depending on given Rules.\n",
    "   - The design principle of the Spider class is to crawl only the URL in start_urls, while the CrawlSpider class defines some rules to provide a convenient mechanism to follow up the links, which is obtained from the crawled webpage links and continue to crawl.\n",
    "- LinkExtractor is used to extract links\n",
    "   - Links extracted from LinkExtractor represented by scrapy.link.Link object\n",
    "      - url parameter most useful (maybe also text)\n",
    "- Rule represents the rules for crawling.\n",
    "   - Rules are set by creating a list of rules as static class property.\n",
    "   - Contains a collection of one or more Rule objects. Each Rule defines specific rules for crawling websites\n",
    "   - If multiple Rules match the same link, the first one will be used according to the order in which they are defined in this object.\n",
    "- CrawlSpider also provides a callback function: parse_start_url(response)\n",
    "   - This function is called when the request of start_url returns. This function analyzes the initial return value and must return an Item object or a Request object or a repeatable object containing both Item object and Request object.  It’s  useful because the CrawlSpider may not parse the initial start urls by default.\n",
    "   - Since CrawlSpider uses the \"parse\" function to implement its logic, if you override the \"parse\" function, CrawlSpider will fail.\n",
    "- extract_links(response) method returns list of links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LinkExtractor](https://docs.scrapy.org/en/latest/topics/link-extractors.html)\n",
    "\n",
    "```python\n",
    "class scrapy.linkextractors.LinkExtractor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinkExtractor is an object that extracts links that will be followed from a crawled webpage (scrapy.http.Response).\n",
    "\n",
    "```python\n",
    "class scrapy.linkextractors.LinkExtractor(\n",
    "    allow = (),\n",
    "    deny = (),\n",
    "    allow_domains = (),\n",
    "    deny_domains = (),\n",
    "    deny_extensions = None,\n",
    "    restrict_xpaths = (),\n",
    "    tags = ('a','area'),\n",
    "    attrs = ('href'),\n",
    "    canonicalize = True,\n",
    "    unique = True,\n",
    "    process_value = None\n",
    ")\n",
    "```\n",
    "\n",
    "The main LinkExtractor parameters:\n",
    "- allow: The value that satisfies the \"regular expression\" in the brackets will be extracted, and if it is empty, all match.\n",
    "- deny: URLs that do not match this regular expression (or regular expression list) must not be extracted\n",
    "- allow_domains: connected domains that will be extracted\n",
    "- deny_domains: The domains of the link must not be extracted.\n",
    "- restrict_xpaths: Use xpath expressions to filter links together with allow.\n",
    "\n",
    "How to set these parameters for our generic web-crawler:\n",
    "- allow: possible alternative to allowed_domains; use regex to indicate that if link starts with root URL, keep going\n",
    "- deny: use this to exclude anything with “calendar” in URL\n",
    "- canonicalize = False: this set to True helps with duplicate checking, but may make link following less robust. Let’s use False for now. Later on we can check the number of pages successfully scraped, and compare the effectiveness of setting this to True vs. False\n",
    "- unique = True: filter duplicates to avoid scraping something already scraped\n",
    "- follow = True: I think this may be True by default, but let’s be explicit we want to follow each link\n",
    "- callback = self.parse: Will this allow recursive scraping (gather links from child URLs, their children, etc.)? If not, default behavior (don’t set a callback at all) may be better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Rule - Crawling rules](https://docs.scrapy.org/en/latest/topics/spiders.html#crawling-rules)\n",
    "\n",
    "Several parameters are used to create Rules.\n",
    "```python\n",
    "class scrapy.contrib.spiders.Rule(\n",
    "    link_extractor,\n",
    "    callback=None,\n",
    "    cb_kwargs=None,\n",
    "    follow=None,\n",
    "    process_links=None,\n",
    "    process_request=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- link_extractor: is a Link Extractor object. It defines how to extract links from crawled web pages.\n",
    "- callback: is a callable or string (the function of the same name in the Spider will be called). This function will be called every time a link is obtained from link_extractor. The callback function receives a response as its first parameter and returns a list containing Item and Request objects (or subclasses of both).\n",
    "- cb_kwargs: A dictionary containing the keyword arguments passed to the callback function.\n",
    "- follow: is a boolean value that specifies whether the link extracted from the response according to this rule needs to be followed. If callback is None, follow defaults to True, otherwise defaults to False.\n",
    "- process_links: is a callable or string (the function of the same name in the Spider will be called). This function will be called when the link list is obtained from link_extrator. This method is mainly used for filtering.\n",
    "- process_request: is a callable or string (all functions with the same name in the spider will be called). This function will be called for every request extracted by this rule. The function must return a request or None. Used to filter requests.\n",
    "- errback: is a callable or a string that is called if any exception is raised.\n",
    "\n",
    "```python\n",
    "from scrapy.spiders.crawl import Rule, CrawlSpider\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class DoubanSpider(CrawlSpider):\n",
    "    name = \"test\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = ['https://example.com/uselinks']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=('subject/\\d+/$',)),\n",
    "        callback='parse_items'),\n",
    "    \t)\n",
    "\n",
    "    def parse_items(self, response):\n",
    "        # Yield items or return items\n",
    "        pass\n",
    "```\n",
    "\n",
    "- Scrapy requests start_urls and gets the response\n",
    "- Use the allow content in LinkExtractors to match the response and get the URL\n",
    "- Request this URL, give the response, and handle the function pointed to by the callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following links\n",
    "Let’s say, instead of just scraping the stuff from the first two pages from http://quotes.toscrape.com, you want quotes from all the pages in the website.\n",
    "\n",
    "Now that you know how to extract data from pages, let’s see how to follow links from them.\n",
    "\n",
    "First thing is to extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup:\n",
    "\n",
    "```shell\n",
    "<ul class=\"pager\">\n",
    "    <li class=\"next\">\n",
    "        <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n",
    "    </li>\n",
    "</ul>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try extracting it in the shell:\n",
    "```shell\n",
    ">>> response.css('li.next a').get()\n",
    "```\n",
    "\n",
    "This gets the anchor element, but we want the attribute href. For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this:\n",
    "```shell\n",
    ">>> response.css('li.next a::attr(href)').get()\n",
    "```\n",
    "There is also an attrib property available (see [Selecting element attributes](https://docs.scrapy.org/en/latest/topics/selectors.html#selecting-attributes) for more):\n",
    "```shell\n",
    ">>> response.css('li.next a').attrib['href']\n",
    "```\n",
    "Let’s modify our spider to recursively follow the link to the next page and extract data from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "            \n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after extracting the data, the `parse()` method looks for the link to the next page and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages.\n",
    "\n",
    "What you see here is Scrapy’s mechanism of following links: when you yield a `Request` in a callback method (as `response.follow` does), Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes.\n",
    "\n",
    "Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting.\n",
    "\n",
    "In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with pagination. Note that even if pages refer to one another, we don’t need to worry about visiting a given page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. In other words, you don't need to hard-code duplicate page handling yourself--this is one example of the built-in functionality of scrapy that saves you a lot of work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "- **Easy:** Use the spider you just created to scrape quotes with another tag, such as 'inspirational' or 'books'. Examine the output.\n",
    "- **NOT so easy:** Complete the following `author` spider so it extracts the name, birthdate, and description for each author and stores the resulting dict in a CSV file. Remember to save the script to the `spiders` folder so Scrapy knows where to look when you call it on the command-line. (Note that Scrapy will by default only scrape each author's page once.) <br> **Hints:** How does the `author` spider know what to extract from each page? Notice what's missing from the `parse_author()` method that the previous spiders have. Use the `extract_with_css()` method in your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = 'author'\n",
    "\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # follow links to author pages\n",
    "        for href in response.css('.author + a::attr(href)'):\n",
    "            yield response.follow(href, self.parse_author)\n",
    "\n",
    "        # follow pagination links\n",
    "        for href in response.css('li.next a::attr(href)'):\n",
    "            yield response.follow(href, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        def extract_with_css(query):\n",
    "            return response.css(query).get(default='').strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy template: A Recursive Text Spider <a id='recursive'> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "import tldextract\n",
    "import csv\n",
    "from bs4 import BeautifulSoup # BS reads and parses even poorly/unreliably coded HTML \n",
    "from bs4.element import Comment # helps with detecting inline/junk tags when parsing with BS\n",
    "import lxml # fast bs4 parser\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import Rule, CrawlSpider\n",
    "from scrapy.exceptions import NotSupported\n",
    "\n",
    "# The following are required for parsing File text\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile\n",
    "import textract\n",
    "from itertools import chain\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inline tags for cleaning out HTML\n",
    "inline_tags = [\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\", \"kbd\", \n",
    "               \"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\", \"span\", \"sub\", \"sup\", \"head\", \n",
    "               \"title\", \"[document]\", \"script\", \"style\", \"meta\", \"noscript\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveTextSpider(CrawlSpider):\n",
    "    name = 'textspider'\n",
    "    rules = [\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                canonicalize=False,\n",
    "                unique=True\n",
    "            ),\n",
    "            follow=True,\n",
    "            callback=\"parse_items\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    def __init__(self, domain_list=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Overrides default constructor to set custom\n",
    "        instance attributes.\n",
    "        \n",
    "        Parameters:\n",
    "        - domain_list: csv or tsv format\n",
    "            List of entities containing string domains and unique identifiers.\n",
    "            \n",
    "        Attributes:\n",
    "        \n",
    "        - start_urls:\n",
    "            Used by scrapy.spiders.Spider. A list of URLs where the\n",
    "            spider will begin to crawl.\n",
    "\n",
    "        - allowed_domains:\n",
    "            Used by scrapy.spiders.Spider. An optional list of\n",
    "            strings containing domains that this spider is allowed\n",
    "            to crawl.\n",
    "\n",
    "        - domain_to_id:\n",
    "            A custom attribute used to map a string domain to\n",
    "            a number representing the unique id defined by\n",
    "            csv_input.\n",
    "        \"\"\"\n",
    "        super(RecursiveTextSpider, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = []\n",
    "        self.allowed_domains = []\n",
    "        self.rules = (Rule(CustomLinkExtractor(allow_domains = self.allowed_domains), follow=True, callback=\"parse_items\"),)\n",
    "        self.domain_to_id = {}\n",
    "        self.init_from_domain_list(domain_list)\n",
    "    \n",
    "    \n",
    "    # note: make sure we ignore robot.txt\n",
    "    # Method for parsing items\n",
    "    def parse_items(self, response):\n",
    "        \n",
    "        item = CharterItem()\n",
    "        item['url'] = response.url\n",
    "        item['text'] = self.get_text(response)\n",
    "        domain = self.get_domain(response.url)    \n",
    "\n",
    "        item['unique_id'] = self.domain_to_id[domain]\n",
    "        item['depth'] = response.request.meta['depth'] # uses DepthMiddleware\n",
    "        print(\"Depth: \", item['depth'])\n",
    "\n",
    "        yield item    \n",
    "        \n",
    "        \n",
    "    def init_from_domain_list(self, domain_list):\n",
    "        \"\"\"\n",
    "        Generate's this spider's instance attributes\n",
    "        from the input domain list, formatted as a CSV or TSV.\n",
    "        \n",
    "        Domain List's format:\n",
    "        1. The first row is meta data that is ignored.\n",
    "        2. Rows in the csv are 1d arrays with one element.\n",
    "        ex: row == ['3.70014E+11,http://www.charlottesecondary.org/'].\n",
    "        \n",
    "        Note: start_requests() isn't used since it doesn't work\n",
    "        well with CrawlSpider Rules.\n",
    "        \n",
    "        Args:\n",
    "            domain_list: Is the path string to this file.\n",
    "        Returns:\n",
    "            Nothing is returned. However, start_urls,\n",
    "            allowed_domains, and domain_to_id are initialized.\n",
    "        \"\"\"\n",
    "        if not domain_list:\n",
    "            return\n",
    "        with open(domain_list, 'r') as f:\n",
    "            delim = \",\" if \"csv\" in domain_list else \"\\t\"\n",
    "            reader = csv.reader(f, delimiter=delim, quoting=csv.QUOTE_NONE)\n",
    "            first_row = True\n",
    "            for raw_row in reader:\n",
    "                if first_row:\n",
    "                    first_row = False\n",
    "                    continue\n",
    "                \n",
    "                unique_id, url = raw_row\n",
    "\n",
    "                domain = self.get_domain(url, True)\n",
    "                # set instance attributes\n",
    "                self.start_urls.append(url)\n",
    "                self.allowed_domains.append(domain)\n",
    "                # note: float('3.70014E+11') == 370014000000.0\n",
    "                self.domain_to_id[domain] = float(unique_id)\n",
    "\n",
    "    \n",
    "    def get_domain(self, url, init = False):\n",
    "        \"\"\"\n",
    "        Given the url, gets the top level domain using the\n",
    "        tldextract library.\n",
    "        \n",
    "        Args:\n",
    "            init (Boolean): True if this function is called while initializing the Spider, else False\n",
    "        Ex:\n",
    "        >>> get_domain('http://www.charlottesecondary.org/')\n",
    "        charlottesecondary.org\n",
    "        >>> get_domain('https://www.socratesacademy.us/our-school')\n",
    "        socratesacademy.us\n",
    "        \"\"\"\n",
    "        extracted = tldextract.extract(url)\n",
    "        permissive_domain = f'{extracted.domain}.{extracted.suffix}' # gets top level domain: very permissive crawling\n",
    "        #specific_domain = re.sub(r'https?\\:\\/\\/', '', url) # full URL without http\n",
    "        specific_domain = re.sub(r'https?\\:\\/\\/w{0,3}\\.?', '', url) # full URL without http and www. to compare w/ permissive\n",
    "        print(\"Permissive:\", permissive_domain)\n",
    "        print(\"Specific:\", specific_domain)\n",
    "        top_level = len(specific_domain.replace(\"/\", \"\")) == len(permissive_domain) # compare specific and permissive domain\n",
    "        \n",
    "        if init: # Check if this is the initialization period for the Spider.\n",
    "            if top_level:\n",
    "                return permissive_domain\n",
    "            else:\n",
    "                return specific_domain\n",
    "        \n",
    "        # secondary round\n",
    "        if permissive_domain in self.allowed_domains:\n",
    "            return permissive_domain\n",
    "        \n",
    "        #implement dictionary for if specific domain is used in original allowed_domains; key is specific_domain?\n",
    "        \n",
    "    \n",
    "    def get_text(self, response):\n",
    "        \"\"\"\n",
    "        Gets the readable text from a website's body and filters it.\n",
    "        Ex:\n",
    "        if response.body == \"\\u00a0OUR \\tSCHOOL\\t\\t\\tPARENTSACADEMICSSUPPORT \\u200b\\u200bOur Mission\"\n",
    "        >>> get_text(response)\n",
    "        'OUR SCHOOL PARENTSACADEMICSSUPPORT Our Mission'\n",
    "        \n",
    "        For another example, see filter_text_ex.txt\n",
    "        \n",
    "        More options for cleaning HTML: \n",
    "        https://stackoverflow.com/questions/699468/remove-html-tags-not-on-an-allowed-list-from-a-python-string/812785#812785\n",
    "        Especially consider: `from lxml.html.clean import clean_html`\n",
    "        \"\"\"\n",
    "        # Load HTML into BeautifulSoup, extract text\n",
    "        soup = BeautifulSoup(response.body, 'html5lib') # slower but more accurate parser for messy HTML # lxml faster\n",
    "        # Remove non-visible tags from soup\n",
    "        [s.decompose() for s in soup(inline_tags)] # quick method for BS\n",
    "        # Extract text, remove <p> tags\n",
    "        visible_text = soup.get_text(strip = False) # get text from each chunk, leave unicode spacing (e.g., `\\xa0`) for now to avoid globbing words\n",
    "        \n",
    "        # Remove ascii (such as \"\\u00\")\n",
    "        filtered_text = visible_text.encode('ascii', 'ignore').decode('ascii')\n",
    "        \n",
    "        # Remove ad junk\n",
    "        filtered_text = re.sub(r'\\b\\S*pic.twitter.com\\/\\S*', '', filtered_text) \n",
    "        filtered_text = re.sub(r'\\b\\S*cnxps\\.cmd\\.push\\(.+\\)\\;', '', filtered_text) \n",
    "        # Replace all consecutive spaces (including in unicode), tabs, or \"|\"s with a single space\n",
    "        filtered_text = regex.sub(r\"[ \\t\\h\\|]+\", \" \", filtered_text)\n",
    "        # Replace any consecutive linebreaks with a single newline\n",
    "        filtered_text = regex.sub(r\"[\\n\\r\\f\\v]+\", \"\\n\", filtered_text)\n",
    "        # Remove json strings: https://stackoverflow.com/questions/21994677/find-json-strings-in-a-string\n",
    "        filtered_text = regex.sub(r\"{(?:[^{}]*|(?R))*}\", \" \", filtered_text)\n",
    "\n",
    "        # Remove white spaces at beginning and end of string; return\n",
    "        return filtered_text.strip()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
