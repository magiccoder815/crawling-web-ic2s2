{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and crawling the web\n",
    "\n",
    "This second day's workshop gives you practice scraping and crawling with modern Python tools.\n",
    "\n",
    "To review from yesterday, web-scraping means “programatically going over a collection of web pages and extracting data” and is a powerful tool for working with data on the web. Scraping has two core steps. First, you find web pages and download them via web requests (often called *web-crawling*). Then you extract and parse information from these pages (often called *web-scraping*). These two steps often happen together and recursively: you crawl some stuff, but upon scraping it you realize you got the wrong websites, so you go back to crawling, which changes your scraping approach, and so on.\n",
    "\n",
    "### Scrapy has \"batteries included\"\n",
    "\n",
    "You can build a scraper from scratch using low-level modules or libraries, but then you have to deal with some potential headaches as your scraper grows more complex. For example, you'll need to handle concurrency so you can crawl more than one page at a time. You'll probably want to figure out how to transform your scraped data into different formats like CSV, XML, or JSON. And you'll sometimes have to deal with sites that require specific settings and access patterns.\n",
    "\n",
    "You'll have better luck if you build your scraper on top of an existing library that handles those issues for you. For this tutorial, we will build some intuition for web-scraping by working with low-level approaches, using the `Requests` and `BeautifulSoup` libraries to make requests and parse the result. Then we will build a scraper with *Scrapy*,which is one of the most popular, flexible, and powerful Python scraping libraries. Scrapy takes a \"batteries included\" approach to scraping, meaning that it handles a lot of the common functionality that all scrapers need. This prevents you from reinventing the wheel--or worse, the flat tire!\n",
    "\n",
    "The focus here is on applying Scrapy. You can also read more about the [basics of scrapy](https://docs.scrapy.org/en/latest/intro/overview.html), its [architecture](https://docs.scrapy.org/en/latest/topics/architecture.html), or see [the FAQ](https://docs.scrapy.org/en/latest/faq.html). And if you need a refresher on scraping with Beautiful Soup, here's a [good tutorial](https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3).\n",
    "\n",
    "### What kind of crawling?\n",
    "\n",
    "A flexible tool like Scrapy can be used in many different ways depending on the task at hand.\n",
    "\n",
    "What I call _narrow crawling_ means focusing on a limited set of pre-defined domains--that is, studying their HTML and CSS structures and exploiting these to extract specific information repeatedly. This maximizes precision in scraping while sacrificing _extensibility_: the ability to incorporate new domains or be resilient to changes in website structure. This is what people usually mean when they say \"web-scraping\". It may or may not expand beyond the initial set of websites, but it may crawl more websites within this set (_vertical crawling_).\n",
    "\n",
    "What I call _broad crawling_ makes the opposite tradeoff, collecting information on a range of websites and promoting flexibility in its scraping algorithm (way of extracting website information) at the expense of generally less clean output. It may identify websites to scrape by google search (what do people click on most?), network analysis (what websites tend to link to one another?), or _link extraction_: finding all within-domain links on a given webpage, then all within-domain links on its children links, and so on to a specified depth. The messier output from broad crawling can present challenges for data cleaning and analysis (remember \"garbage in, garbage out\"?), but this depends on the application.\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    "* [Narrow crawling with `Requests` and `BeautifulSoup`](#narrow)\n",
    "    - [Making requests](#request)\n",
    "    - [Parsing HTML](#parsing)\n",
    "       - [Getting human-readable text](#readable)\n",
    "* [Broad crawling with `Scrapy`](#scrapy)\n",
    "    - [A simple Scrapy spider](#simple)\n",
    "    - [Link extraction](#linkextraction)\n",
    "* [Scrapy template: A Recursive Text Spider](#recursive)\n",
    "\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "* *narrow crawling (less extensible)*: \n",
    "    * Scraping a limited set of pre-defined domains: studying their HTML and CSS structures and exploiting these to extract specific information repeatedly. This maximizes precision in scraping while sacrificing extensibility (ability to incorporate new domains or changes in website structure). What people usually mean when they say \"web-scraping\". \n",
    "* *broad crawling (more extensible)*: \n",
    "    * Collecting information on a range of websites and promoting flexibility in its scraping algorithm (way of extracting website information) at the expense of generally less clean output. It may identify websites to scrape by google search (what do people click on most?), network analysis (what websites tend to link to one another?), or link extraction.\n",
    "* *extensibility*:\n",
    "    * Ability for a scraping approach to incorporate new domains or be resilient to changes in website structure. Generally higher for broad crawls than narrow crawls, at the expense of precision. \n",
    "* *link extraction*:\n",
    "    * Finding all within-domain links on a given webpage, then all within-domain links on its children links, and so on to a specified depth. \n",
    "* *horizontal crawling*: \n",
    "    * Crawling on the same hierarchical level as the input domain, such as going from the first to the second page of google results.\n",
    "* *vertical crawling*:\n",
    "    * Crawling at a higher or lower level from the input domain, such as navigating to the \"About Us\" page directly linked from a home page. \n",
    "\n",
    "**__________________________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow crawling with `Requests` and `BeautifulSoup` <a id='narrow'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making requests<a id='request'></a>\n",
    "\n",
    "The first step in web-scraping is getting the HTML of the website we want to scrape. The [requests](http://docs.python-requests.org/en/master/) library is the easiest way to do this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Canberra'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it looks like everything worked! Let's see our beautiful HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, that's weird. Doesn't look like HTML to me.\n",
    "\n",
    "What the `requests.get` function returned (and the thing in our `response` variable) was a Response object. It itself isn't the HTML that we wanted, but rather a collection of metadata about the request/response interaction between your computer and the Wikipedia server.\n",
    "\n",
    "For example, it knows whether the response was successful or not (`response.ok`), how long the whole interaction took (`response.elapsed`), what time the request took place (`response.headers['Date']`) and a whole bunch of other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tue, 27 Apr 2021 13:33:49 GMT'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, what we really care about is the HTML content. We can get that from the `Response` object with `response.text`. What we get back is a string of HTML, exactly the contents of the HTML file at the URL that we requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Canberra - Wikipedia</title>\n",
      "<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"6f2886de-da31-4c7e-bdcb-8de02a761b4f\",\"wgCSPNonce\":!1,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"Canberra\",\"wgTitle\":\"Canberra\",\"wgCurRevisionId\":1017188895,\"wgRevisionId\":1017188895,\"wgArticleId\":51983,\"wgIsArticle\":!0,\"wgIsRedirect\":!1,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1: Julian–Gregorian uncertainty\",\"Australian Statistical Geography Standard 2016 ID different from Wikidata\",\"Australian Statistical Geography Standard 2016 ID same as Wikidata\",\"Aus\n"
     ]
    }
   ],
   "source": [
    "html = response.text\n",
    "print(html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Get the HTML for [the Wikipedia page about HTML](https://en.wikipedia.org/wiki/HTML). \n",
    "Print out the first 1000 characters and compare it to the HTML you see when you view the source HTML in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>HTML - Wikipedia</title>\\n<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"4f84a59d-603f-42f7-bf6e-c3e276d6cb7e\",\"wgCSPNonce\":!1,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"HTML\",\"wgTitle\":\"HTML\",\"wgCurRevisionId\":1016919858,\"wgRevisionId\":1016919858,\"wgArticleId\":13191,\"wgIsArticle\":!0,\"wgIsRedirect\":!1,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 errors: missing periodical\",\"Webarchive template wayback links\",\"Wikipedia pages semi-protected against vandalism\",\"Articles with short description\",\"Short description is different from Wikida'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "url = 'https://en.wikipedia.org/wiki/HTML'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "html[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML<a id='parsing'></a>\n",
    "\n",
    "The second step in web scraping is parsing HTML. This is where things can get a little tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "View the source HTML of [the page listing all departments](http://guide.berkeley.edu/courses/), and see if you can find the part of the HTML where the departments are listed. There's a lot of other stuff in the file that we don't care too much about. You could try `Crtl-F`ing for the name of a department you can see on the webpage.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Solution**\n",
    "\n",
    "You should see something like this:\n",
    "\n",
    "\n",
    "```\n",
    "<div id=\"atozindex\">\n",
    "<h2 class=\"letternav-head\" id='A'><a name='A'>A</a></h2>\n",
    "<ul>\n",
    "<li><a href=\"/courses/aerospc/\">Aerospace Studies (AEROSPC)</a></li>\n",
    "<li><a href=\"/courses/africam/\">African American Studies (AFRICAM)</a></li>\n",
    "<li><a href=\"/courses/a,resec/\">Agricultural and Resource Economics (A,RESEC)</a></li>\n",
    "<li><a href=\"/courses/amerstd/\">American Studies (AMERSTD)</a></li>\n",
    "<li><a href=\"/courses/ahma/\">Ancient History and Mediterranean Archaeology (AHMA)</a></li>\n",
    "<li><a href=\"/courses/anthro/\">Anthropology (ANTHRO)</a></li>\n",
    "<li><a href=\"/courses/ast/\">Applied Science and Technology (AST)</a></li>\n",
    "<li><a href=\"/courses/arabic/\">Arabic (ARABIC)</a></li>\n",
    "<li><a href=\"/courses/arch/\">Architecture (ARCH)</a></li>\n",
    "```\n",
    "\n",
    "This is HTML. HTML uses \"tags\", code that surrounds the raw text which indicates the structure of the content. The tags are enclosed in `<` and `>` symbols. The `<li>` says \"this is a new thing in a list and `</li>` says \"that's the end of that new thing in the list\". Similarly, the `<a ...>` and the `</a>` say, \"everything between us is a hyperlink\". In this HTML file, each department is listed in a list with `<li>...</li>` and is also linked to its own page using `<a>...</a>`. In our browser, if we click on the name of the department, it takes us to that department's own page. The way the browser knows where to go is because the `<a>...</a>` tag tells it what page to go to. You'll see inside the `<a>` bit, there's a `href=...`. That tells us the (relative) location of the page it's linked to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Look at HTML source of [the page for the Aerospace Studies department](http://guide.berkeley.edu/courses/aerospc/), and try to find the part of the file where the information on each course is. Again, try searching for it using `Crtl-F`.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "\n",
    "```\n",
    "<div class=\"courseblock\">\n",
    "\n",
    "<button class=\"btn_toggleCoursebody\" aria-expanded=\"false\" aria-controls=\"cb_aerospc1a\" data-toggle=\"#cb_aerospc1a\">\n",
    "\n",
    "<a name=\"spanaerospc1aspanspanfoundationsoftheu.s.airforcespanspan1unitspan\"></a>\n",
    "<h3 class=\"courseblocktitle\">\n",
    "<span class=\"code\">AEROSPC 1A</span> \n",
    "<span class=\"title\">Foundations of the U.S. Air Force</span> \n",
    "<span class=\"hours\">1 Unit</span>\n",
    "</h3>\n",
    "```\n",
    "\n",
    "The content that we care about is enclosed within HTML tags. It looks like the course code is enclosed in a `span` tag, which has a `class` attribute with the value `\"code\"`. What we'll have to do is extract out the information we care about by specifying what tag it's enclosed in.\n",
    "\n",
    "But first, we're going to need to get the HTML of the first page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Get the HTML content of `http://guide.berkeley.edu/courses/` and store it in a variable called `academic_guide_html`. You can use the `get_html` function you wrote before.\n",
    "\n",
    "Print the first 500 characters to see what we got back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "academic_guide_url = 'http://guide.berkeley.edu/courses/'\n",
    "academic_guide_html = get_html(academic_guide_url)\n",
    "print(academic_guide_html[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we've got the HTML contents of the Academic Guide site we want to scrape. Now we can parse it. [\"Parsing\"](https://en.wikipedia.org/wiki/Parsing) means to turn a string of data into a structured representation. When we're parsing HTML, we're taking the Python string and turning it into a tree. The Python package `BeautifulSoup` does all our HTML parsing for us. We give it our HTML as a string and it returns a parsed HTML tree. Here, we're also telling BeautifulSoup to use the `lxml` parser behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "academic_guide_soup = BeautifulSoup(academic_guide_html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said before that all the departments were listed on the Academic Guide page with links to their departmental page, where the actual courses are listed. So we can find all the departments by looking in our parsed HTML for all the links. Remember that the links are represented in the HTML with the `<a>...</a>` tag, so we ask our `academic_guide_soup` to find us all the tags called `a`. What we get back is a list of all the `a` elements in the HTML page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = academic_guide_soup.find_all('a')\n",
    "# print a random link element\n",
    "links[48]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a list of `a` elements, each one represents a link on the Academic Guide page. But there are other links on this page in addition to the ones we care about, for example, a link back to the UC Berkeley home page. How can we filter out all the links we don't care about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Working with BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "Here's a basic introduction to beautifulsoup on how to get text from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the requests module to request data from the website\n",
    "r  = requests.get('http://www.k12northstar.org/chinook')\n",
    "data = r.text # it gives you raw data of the website --> more like html and css code but not really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a soup object that gives you a formatted website code in HTML/CSS, JS\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "print(soup.prettify())  # you can uncomment this to check the code, but it's too long so I just comment it for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example here: find_all('a') means find all <a> tag in html\n",
    "# uncomment to see the result\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = soup.find_all('p')  \n",
    "# p tag in html is where paragraphs or texts is placed, \n",
    "# so we need to extract them from the soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paragraph  # check what is being extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get plain text from all the p tag stuff\n",
    "for p in paragraph:\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use BeautifulSoup to get all the paragraphs from reddit.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "r  = requests.get('http://www.reddit.com')\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "paragraph = soup.find_all('p')\n",
    "for p in paragraph:\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get human-readable text from HTML<a id='readable'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags offer good information for NLP...sometimes. \n",
    "Other times, they offer extraneous characters, words, and in some cases entire scripts. \n",
    "\n",
    "Cases for including tags:\n",
    "- Identifying sections via headers\n",
    "- Identifying URL links (a href...)\n",
    "- Identifying images (img tags)\n",
    "\n",
    "Cases where tags overcomplicate things:\n",
    "- Paragraphs (floating p tags)\n",
    "- Scripts (nonsense javascript characters)\n",
    "- Styles (inline style properties)\n",
    "\n",
    "Reference: https://stackoverflow.com/questions/12959308/remove-all-inline-styles-using-beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods to remove tags\n",
    "\n",
    "*Simplest way*: `get_text()`\n",
    "\n",
    "- returns all the text in a document or beneath a tag, as a single Unicode string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI linked to example.com\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = '<a href=\"http://example.com/\">\\nI linked to <i>example.com</i>\\n</a>'\n",
    "soup = BeautifulSoup(markup, 'html.parser')\n",
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the specific soup attributes for which you want text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example.com'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.i.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell Beautiful Soup to strip whitespace from the beginning and end of each bit of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I linked toexample.com'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remove and return tags from soup:* `extract()`\n",
    "\n",
    "- Useful if we want to keep the contents of a tag and analyze them further, such as links or images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted tag: \n",
      "<i>example.com</i>\n",
      "\n",
      "Cleaned soup: \n",
      "<a href=\"http://example.com/\">\n",
      "I linked to \n",
      "</a>\n",
      "\n",
      "Text from cleaned soup:\n",
      "I linked to \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "markup = '<a href=\"http://example.com/\">\\nI linked to <i>example.com</i>\\n</a>'\n",
    "soup = BeautifulSoup(markup, 'html.parser')\n",
    "i_tag = soup.i.extract()\n",
    "\n",
    "print(f\"Extracted tag: \\n{i_tag}\")\n",
    "print()\n",
    "print(f'Cleaned soup: \\n{soup}')\n",
    "print()\n",
    "print(f'Text from cleaned soup:{soup.get_text()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Obliterate tags:* `decompose()`\n",
    "\n",
    "- Useful for tags we don't want, such as scripts\n",
    "- Better memory management by destroying the tag, as opposed to simply extracting to nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>some  text</p>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "markup = BeautifulSoup('<p>some <i>italicized</i> text</p>', 'html.parser')\n",
    "markup.i.decompose()\n",
    "markup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample usage for Decompose to remove script and style tags:\n",
    "for s in soup([\"script\", \"style\"]):\n",
    "\ts.decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Removing attributes](https://stackoverflow.com/questions/12959308/remove-all-inline-styles-using-beautifulsoup)\n",
    "\n",
    "Sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = [\"href\"] # keep text from within these tags\n",
    "for tag in soup.findAll(True):\n",
    "    for attribute in [attribute for attribute in tag.attrs if attribute not in whitelist]:\n",
    "        del tag[attribute]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Use BeautifulSoup to clean the following HTML and return its text. \n",
    "\n",
    "Keep the text in the `href` and `src` attributes, but completely remove the `script`, `style`, `meta`, and `noscript` attributes. Anything not in these attributes, keep all of it. \n",
    "\n",
    "([source](https://stackoverflow.com/questions/30565404/remove-all-style-scripts-and-html-tags-from-an-html-page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup = '<a href=\"http://example.com/\">\\nI linked to <i>example.com</i>\\n</a>'\n",
    "attribute_whitelist = [\"href\", \"src\"]\n",
    "tag_remove_list = [\"script\", \"style\", \"meta\", \"noscript\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned soup: \n",
      "<a href=\"http://example.com/\">\n",
      "I linked to <i>example.com</i>\n",
      "</a>\n",
      "\n",
      "Text from cleaned soup: \n",
      "I linked to example.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "soup = BeautifulSoup(markup, \"html.parser\")\n",
    "\n",
    "for s in soup(tag_remove_list):\n",
    "\ts.decompose()\n",
    "    \n",
    "for tag in soup.findAll(True):\n",
    "\tfor attribute in [attribute for attribute in tag.attrs if attribute not in attribute_whitelist]:\n",
    "\t\tdel tag[attribute]\n",
    "\n",
    "print(f'Cleaned soup: \\n{soup}')\n",
    "print()\n",
    "print(f'Text from cleaned soup: {soup.get_text()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broad crawling with `Scrapy` <a id='scrapy'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about crawling with Scrapy, we will explore [quotes.toscrape.com](quotes.toscrape.com), a scraping-friendly website that lists quotes from famous authors. By the end of this section, you’ll have a fully functional Python web scraper that walks through a series of pages and extracts data from each page. The scraper will be easily expandable so you can tinker around with it and use it as a foundation for your own projects scraping data from the web.\n",
    "\n",
    "As a _Twisted_ application, Scrapy is event-driven, asynchronous, and is virtually multi-threaded (while using only one thread). While other programs cause _blocks_ when they access files or the web, spawn new processes, or do system operations, Scrapy instead waits until a resource is available, solves the immediate problem, and then calls another task. In short, Scrapy is fast, flexible, and scalable. It offers one of the most user-friendly ways to write crawling programs that can move across heterogeneous swaths of the internet, download stuff, and not break. \n",
    "\n",
    "To grasp the intuition behind Scrapy, imagine a bank where tellers (threads) are available to see customers (processes), who need to fill out forms before they're done. Such a situation could be configured in these ways:\n",
    "\n",
    "- _Blocking_ operation with a _single_ thread: Here there is 1 teller trying to help 5 customers. When customer 1 needs time to fill out a form, then teller 1 is occupied waiting for customer 1--and all the other customers are stuck in line.\n",
    "- _Blocking_ operation with _multiple_ threads: Now there are still 5 customers, but there are 3 tellers. When customer 1 needs time to fill out a form, then teller 1 is occupied. Customer 2 may have access to teller 2 and customer 3 to teller 3, but then all the tellers are monopolized while people fill out forms, which means customers 4 and 5 are still stuck waiting in line. \n",
    "- _Non-blocking_ operation with a _single_ thread: Here again we have 1 teller and 5 customers. When customer 1 needs time to fill out a form, they stand aside so the single teller can help customer 2. When customer 1 is finished, they wait until customer 2 is done or has something to do, then customer 1 is called back to continue being helped. If customers 1 and 2 both have forms to complete, they can do that on the side and the single teller can see customer 3, and so on. \n",
    "\n",
    "You can see this last situation is way more efficient than the previous two. This is the Scrapy default; when _multiple_ threads are available for a _non-blocking_ operation (like when multiple spiders work together), this is even better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start scraping, you will have to set up a new Scrapy project. Enter a directory where you’d like to store your code and run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ scrapy startproject tutorial\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a tutorial directory with the following contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tutorial/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "    tutorial/             # project's Python module, you'll import your code from here\n",
    "        __init__.py\n",
    "        items.py          # project items definition file\n",
    "        middlewares.py    # project middlewares file\n",
    "        pipelines.py      # project pipelines file\n",
    "        settings.py       # project settings file\n",
    "        spiders/          # a directory where you'll later put your spiders\n",
    "            __init__.py\n",
    "            ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple scrapy spider <a id='simple'> </a>\n",
    "\n",
    "Spiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass scrapy.Spider and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data.\n",
    "\n",
    "This is the code for our first Spider. Save it in a file named `quotes_spider.py` under the `tutorial/spiders` directory in your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our Spider subclasses scrapy.Spider and defines some attributes and methods:\n",
    "\n",
    "-`name`: identifies the Spider. It must be unique within a project, that is, you can’t set the same name for different Spiders.\n",
    "\n",
    "-`start_urls`: a list of URLs to provide the initial requests for the crawler. Armed with this list alone, the spider will download HTML from the webpages specified, much as a web browser does. But it won't extract anything from the pages--that's why we need to define the `parse()` method.\n",
    "\n",
    "-`parse()`: a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of `TextResponse` that holds the page content and has further helpful methods to handle it.\n",
    "\n",
    "The `parse()` method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (`Request`) from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put our spider to work, go to the project’s top level directory and run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ scrapy crawl quotes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command runs the spider with name `quotes` that we’ve just added, that will send some requests for the `quotes.toscrape.com` domain. You will get an output similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "...\n",
    "2019-03-19 15:58:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2019-03-19 15:58:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] INFO: Spider opened\n",
    "2019-03-19 15:58:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2019-03-19 15:58:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
    "2019-03-19 15:58:49 [quotes] DEBUG: Saved file quotes-1.html\n",
    "2019-03-19 15:58:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2019-03-19 15:58:50 [quotes] DEBUG: Saved file quotes-2.html\n",
    "2019-03-19 15:58:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2019-03-19 15:58:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 678,\n",
    " 'downloader/request_count': 3,\n",
    " ...}\n",
    "2019-03-19 15:58:50 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the files in the current directory. You should notice that two new files have been created: `quotes-1.html` and `quotes-2.html`, with the content for the respective URLs, as our `parse` method instructs.\n",
    "\n",
    "How did this work? Scrapy schedules the `scrapy.Request` objects returned by the `start_requests` method of the Spider. Upon receiving a response for each one, it instantiates `Response` objects and calls the callback method associated with the request (in this case, the `parse` method) passing the response as argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Modify and run the spider script above to scrape this short list of `start_urls`: \n",
    "```python\n",
    "['http://brickset.com/sets/year-2016']\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data\n",
    "The best way to learn how to extract data with Scrapy is trying selectors using the shell [Scrapy shell](https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell). Remember to always enclose URLs in quotes (double quotes for Windows) when running Scrapy shell from command-line, otherwise urls containing arguments (ie. `&` character) will not work. Run:\n",
    "\n",
    "```shell\n",
    "$ scrapy shell 'http://quotes.toscrape.com'\n",
    "```\n",
    "You will see something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "2019-03-19 20:00:05 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: tutorial)\n",
    "2019-03-19 20:00:05 [scrapy.utils.log] INFO: Versions: lxml 4.3.1.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 17.9.0, Python 3.6.7 (default, Oct 22 2018, 11:32:17) - [GCC 8.2.0], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Linux-4.15.0-45-generic-x86_64-with-Ubuntu-18.04-bionic\n",
    "2019-03-19 20:00:05 [scrapy.crawler] INFO: Overridden settings: {...}\n",
    "2019-03-19 20:00:05 [scrapy.extensions.telnet] INFO: Telnet Password: 030319d194e7f6b0\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage']\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    "...]\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    "...]\n",
    "2019-03-19 20:00:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2019-03-19 20:00:05 [scrapy.core.engine] INFO: Spider opened\n",
    "2019-03-19 20:00:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2019-03-19 20:00:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f07993c02b0>\n",
    "... \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the shell, you can try selecting elements using CSS with the response object:\n",
    "```shell\n",
    ">>> response.css('title')\n",
    "```\n",
    "The result of running `response.css('title')` is a list-like object called SelectorList, which represents a list of Selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data.\n",
    "\n",
    "To extract the text from the title above, you can do:\n",
    "```shell\n",
    ">>> response.css('title::text').getall()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to note here: one is that we’ve added `::text` to the CSS query, to mean we want to select only the text elements directly inside `<title>` element. If we don’t specify `::text`, we’d get the full title element, including its tags:\n",
    "```shell\n",
    ">>> response.css('title').getall()\n",
    "```\n",
    "The other thing is that the result of calling `.getall()` is a list: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do:\n",
    "```shell\n",
    ">>> response.css('title::text').get()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the `getall()` and `get()` methods, you can also use the `re()` method to extract using regular expressions:\n",
    "```shell\n",
    ">>> response.css('title::text').re(r'Quotes.*')\n",
    ">>> response.css('title::text').re(r'Q\\w+')\n",
    ">>> response.css('title::text').re(r'(\\w+) to (\\w+)')\n",
    "```\n",
    "In order to find the proper CSS selectors to use, you can use your browser developer tools (e.g., in Chrome, right click > `Inspect`) to inspect the HTML and come up with a selector (for more info, see [Using your browser’s Developer Tools for scraping](https://docs.scrapy.org/en/latest/topics/developer-tools.html)). You can also try opening the response page from the shell in your web browser using `view(response)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Inspect [quotes.toscrape.com](quotes.toscrape.com) for the selectors associated with quotes. Use this information to display the text of one of the quotes in the scrapy shell. <br>\n",
    "**Hint 1:** If you need help getting a better sense of website structure, use the HTML tree below as a visual guide.<br>\n",
    "**Hint 2:** You can subset within selectors by using periods and spaces. For instance, the following produces a SelectorList for the class2 of each type2 within the class1 of each type1:\n",
    "```shell\n",
    "response.css('type1.class1 type2.class2')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining items\n",
    "\n",
    "Within the project directory just created, there’s an `items.py` file. Items add structure to our scraping results and are used spiders.\n",
    "\n",
    "Here you can add class fields such as url, images or locations. These fields can be filled by pipelines (a more advanced topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.item import Item, Field\n",
    "\n",
    "class PropertiesItem(Item):\n",
    "    # Primary fields\n",
    "    title = Field()\n",
    "    price = Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing spiders\n",
    "\n",
    "Spiders element the scraping process defined in the previous sub-chapter, “The fundamental scraping process”.\n",
    "\n",
    "You can create a spider from a template using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ scrapy genspider SPIDER_NAME web\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spiders inherit from scrapy.Spider. Note the special function parse(self, response).\n",
    "The response object is the same object we found in the Scrapy shell.\n",
    "\n",
    "Within the parse method, we can define items."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "item = PropertiesItem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item properties can then be set with the response we get from parsing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    item = PropertiesItem()      \n",
    "    item['image_urls'] = response.xpath(\n",
    "         '//*[@itemprop=\"image\"][1]/@src').extract()\n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to parse and finally create these items, run the spider with  scrapy crawl basic in the project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting quotes and authors\n",
    "Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page.\n",
    "\n",
    "Each quote in http://quotes.toscrape.com is represented by HTML elements that look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<div class=\"quote\">\n",
    "    <span class=\"text\">“The world as we have created it is a process of our\n",
    "    thinking. It cannot be changed without changing our thinking.”</span>\n",
    "    <span>\n",
    "        by <small class=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "    </span>\n",
    "    <div class=\"tags\">\n",
    "        Tags:\n",
    "        <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "        <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "        <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "        <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "    </div>\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we extract the data we want? To start, we get a list of selectors for the quote HTML elements with:\n",
    "\n",
    "```shell\n",
    ">>> response.css(\"div.quote\")\n",
    "```\n",
    "\n",
    "Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:\n",
    "```shell\n",
    ">>> quote = response.css(\"div.quote\")[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s extract title, author and the tags from that quote using the quote object we just created:\n",
    "\n",
    "```shell\n",
    ">>> title = quote.css(\"span.text::text\").get()\n",
    ">>> title\n",
    ">>> author = quote.css(\"small.author::text\").get()\n",
    ">>> author\n",
    "```\n",
    "\n",
    "Given that the tags are a list of strings, we can use the .getall() method to get all of them:\n",
    "```shell\n",
    ">>> tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    ">>> tags\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary. Copy and paste each of these subsequent lines into scrapy shell:\n",
    "```shell\n",
    ">>> for quote in response.css(\"div.quote\"):\n",
    "...     text = quote.css(\"span.text::text\").get()\n",
    "...     author = quote.css(\"small.author::text\").get()\n",
    "...     tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "...     print(dict(text=text, author=author, tags=tags))\n",
    ">>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data in our spider\n",
    "Let’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider.\n",
    "\n",
    "A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the `yield` Python keyword in the callback, as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this spider, it will output the extracted data with the log:\n",
    "```shell\n",
    "2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
    "{'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}\n",
    "2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
    "{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the scraped data\n",
    "The simplest way to store the scraped data is by using Feed exports, with the following command:\n",
    "```shell\n",
    "$ scrapy crawl quotes -o quotes.json\n",
    "```\n",
    "\n",
    "That will generate an quotes.json file containing all scraped items, serialized in JSON.\n",
    "\n",
    "For historic reasons, Scrapy appends to a given file instead of overwriting its contents. If you run this command twice without removing the file before the second time, you’ll end up with a long JSON file--actually, a broken JSON file, which cannot be read.\n",
    "\n",
    "You can also use other formats, like JSON Lines:\n",
    "```shell\n",
    "$ scrapy crawl quotes -o quotes.jl\n",
    "```\n",
    "\n",
    "The JSON Lines format is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory.\n",
    "\n",
    "In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in `tutorial/pipelines.py`. Though you don’t need to implement any item pipelines if you just want to store the scraped items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following links\n",
    "Let’s say, instead of just scraping the stuff from the first two pages from http://quotes.toscrape.com, you want quotes from all the pages in the website.\n",
    "\n",
    "Now that you know how to extract data from pages, let’s see how to follow links from them.\n",
    "\n",
    "First thing is to extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup:\n",
    "\n",
    "```shell\n",
    "<ul class=\"pager\">\n",
    "    <li class=\"next\">\n",
    "        <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n",
    "    </li>\n",
    "</ul>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try extracting it in the shell:\n",
    "```shell\n",
    ">>> response.css('li.next a').get()\n",
    "```\n",
    "\n",
    "This gets the anchor element, but we want the attribute href. For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this:\n",
    "```shell\n",
    ">>> response.css('li.next a::attr(href)').get()\n",
    "```\n",
    "There is also an attrib property available (see [Selecting element attributes](https://docs.scrapy.org/en/latest/topics/selectors.html#selecting-attributes) for more):\n",
    "```shell\n",
    ">>> response.css('li.next a').attrib['href']\n",
    "```\n",
    "Let’s modify our spider to recursively follow the link to the next page and extract data from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "            \n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after extracting the data, the `parse()` method looks for the link to the next page and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages.\n",
    "\n",
    "What you see here is Scrapy’s mechanism of following links: when you yield a `Request` in a callback method (as `response.follow` does), Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes.\n",
    "\n",
    "Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting.\n",
    "\n",
    "In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with pagination. Note that even if pages refer to one another, we don’t need to worry about visiting a given page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. In other words, you don't need to hard-code duplicate page handling yourself--this is one example of the built-in functionality of scrapy that saves you a lot of work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spider arguments\n",
    "Rather than hard-coding file handling into your spider (as did the simple spider above), you can give your scrapy spiders a filename following the `-o` command line argument. You can also provide your spiders with other arguments by using the `-a` option when running them, such as indicating which tag you want to scrape:\n",
    "```shell\n",
    "$ scrapy crawl quotes -o quotes-love.json -a tag=love\n",
    "```\n",
    "These arguments are passed to the Spider’s `__init__` method and become spider attributes by default. If you want your spider to use this attribute intelligently, you need to hard-code this behavior. \n",
    "\n",
    "In this example, the value provided for the `tag` argument will be available via `self.tag`. You can use this to make your spider fetch only quotes with a specific tag, building the URL based on the argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = 'http://quotes.toscrape.com/'\n",
    "        tag = getattr(self, 'tag', None)\n",
    "        if tag is not None:\n",
    "            url = url + 'tag/' + tag\n",
    "        yield scrapy.Request(url, self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass the `tag=love` argument to this spider, you’ll notice that it will only visit URLs from the humor tag, such as http://quotes.toscrape.com/tag/love.\n",
    "\n",
    "You can learn more about handling spider arguments [here](https://docs.scrapy.org/en/latest/topics/spiders.html#spiderargs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "- **Easy:** Use the spider you just created to scrape quotes with another tag, such as 'inspirational' or 'books'. Examine the output.\n",
    "- **NOT so easy:** Complete the following `author` spider so it extracts the name, birthdate, and description for each author and stores the resulting dict in a CSV file. Remember to save the script to the `spiders` folder so Scrapy knows where to look when you call it on the command-line. (Note that Scrapy will by default only scrape each author's page once.) <br> **Hints:** How does the `author` spider know what to extract from each page? Notice what's missing from the `parse_author()` method that the previous spiders have. Use the `extract_with_css()` method in your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = 'author'\n",
    "\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # follow links to author pages\n",
    "        for href in response.css('.author + a::attr(href)'):\n",
    "            yield response.follow(href, self.parse_author)\n",
    "\n",
    "        # follow pagination links\n",
    "        for href in response.css('li.next a::attr(href)'):\n",
    "            yield response.follow(href, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        def extract_with_css(query):\n",
    "            return response.css(query).get(default='').strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Extraction <a id='linkextraction'> </a>\n",
    "\n",
    "You can either add more URL strings to your parse method’s start_urls field. Or, you can read a text file as a URL source.\n",
    "\n",
    "We could also use URLs defined within the pages we scrape.\n",
    "\n",
    "We can extract the next links using xpath or other selectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-direction crawling with a spider\n",
    "\n",
    "We can crawl both horizontally and vertically in the same parse method, using Request. \n",
    "\n",
    "This can be done manually with two calls Request. However, since this is done commonly, there is a crawling template we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ scrapy genspider -t crawl SPIDER_NAME web # Creates the “crawl” template.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasySpider(CrawlSpider):\n",
    "    name = 'NAME OF SPIDER'\n",
    "    allowed_domains = ['web']\n",
    "    start_urls = ['http://www.web/']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        #...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the default rule with two rules, one for horizontal and one for vertical crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = (\n",
    "    Rule(LinkExtractor(restrict_xpaths='//*[contains(@class,\"next\")]')),\n",
    "    Rule(LinkExtractor(restrict_xpaths='//*[@itemprop=\"url\"]'),\n",
    "         callback='parse_item')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LinkExtractor()` by default searches for `a`, `area`, and `href` HTML tags or attributes.\n",
    "\n",
    "What does the callback argument do in a rule? By default, if not explicitly set, it follows/crawls links based on the rule. With it set, you have to explicitly return links from your callback if you still want to follow links.\n",
    "\n",
    "Behind the scenes Scrapy uses last in, first out for processing requests (depth first crawl). This means we visit a page and all of its sub-links, before visiting the next page. Furthermore, Scrapy avoids duplicate requests. These default behaviors can be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring LinkExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrawlSpider, LinkExtractor, Rule\n",
    "from scrapy.spider import CrawlSpider, Rule\n",
    "from scrapy.linkextractor import LinkExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CrawlSpider, LinkExtractors, and Rule are classes in scrapy.\n",
    "- CrawlSpider is a derived class of Spider, with more methods and functions.\n",
    "   - CrawlSpider follows links depending on given Rules.\n",
    "   - The design principle of the Spider class is to crawl only the URL in start_urls, while the CrawlSpider class defines some rules to provide a convenient mechanism to follow up the links, which is obtained from the crawled webpage links and continue to crawl.\n",
    "- LinkExtractor is used to extract links\n",
    "   - Links extracted from LinkExtractor represented by scrapy.link.Link object\n",
    "      - url parameter most useful (maybe also text)\n",
    "- Rule represents the rules for crawling.\n",
    "   - Rules are set by creating a list of rules as static class property.\n",
    "   - Contains a collection of one or more Rule objects. Each Rule defines specific rules for crawling websites\n",
    "   - If multiple Rules match the same link, the first one will be used according to the order in which they are defined in this object.\n",
    "- CrawlSpider also provides a callback function: parse_start_url(response)\n",
    "   - This function is called when the request of start_url returns. This function analyzes the initial return value and must return an Item object or a Request object or a repeatable object containing both Item object and Request object.  It’s  useful because the CrawlSpider may not parse the initial start urls by default.\n",
    "   - Since CrawlSpider uses the \"parse\" function to implement its logic, if you override the \"parse\" function, CrawlSpider will fail.\n",
    "- extract_links(response) method returns list of links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LinkExtractor](https://docs.scrapy.org/en/latest/topics/link-extractors.html)\n",
    "\n",
    "```python\n",
    "class scrapy.linkextractors.LinkExtractor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinkExtractor is an object that extracts links that will be followed from a crawled webpage (scrapy.http.Response).\n",
    "\n",
    "```python\n",
    "class scrapy.linkextractors.LinkExtractor(\n",
    "    allow = (),\n",
    "    deny = (),\n",
    "    allow_domains = (),\n",
    "    deny_domains = (),\n",
    "    deny_extensions = None,\n",
    "    restrict_xpaths = (),\n",
    "    tags = ('a','area'),\n",
    "    attrs = ('href'),\n",
    "    canonicalize = True,\n",
    "    unique = True,\n",
    "    process_value = None\n",
    ")\n",
    "```\n",
    "\n",
    "The main LinkExtractor parameters:\n",
    "- allow: The value that satisfies the \"regular expression\" in the brackets will be extracted, and if it is empty, all match.\n",
    "- deny: URLs that do not match this regular expression (or regular expression list) must not be extracted\n",
    "- allow_domains: connected domains that will be extracted\n",
    "- deny_domains: The domains of the link must not be extracted.\n",
    "- restrict_xpaths: Use xpath expressions to filter links together with allow.\n",
    "\n",
    "How to set these parameters for our generic web-crawler:\n",
    "- allow: possible alternative to allowed_domains; use regex to indicate that if link starts with root URL, keep going\n",
    "- deny: use this to exclude anything with “calendar” in URL\n",
    "- canonicalize = False: this set to True helps with duplicate checking, but may make link following less robust. Let’s use False for now. Later on we can check the number of pages successfully scraped, and compare the effectiveness of setting this to True vs. False\n",
    "- unique = True: filter duplicates to avoid scraping something already scraped\n",
    "- follow = True: I think this may be True by default, but let’s be explicit we want to follow each link\n",
    "- callback = self.parse: Will this allow recursive scraping (gather links from child URLs, their children, etc.)? If not, default behavior (don’t set a callback at all) may be better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Rule - Crawling rules](https://docs.scrapy.org/en/latest/topics/spiders.html#crawling-rules)\n",
    "\n",
    "Several parameters are used to create Rules.\n",
    "```python\n",
    "class scrapy.contrib.spiders.Rule(\n",
    "    link_extractor,\n",
    "    callback=None,\n",
    "    cb_kwargs=None,\n",
    "    follow=None,\n",
    "    process_links=None,\n",
    "    process_request=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- link_extractor: is a Link Extractor object. It defines how to extract links from crawled web pages.\n",
    "- callback: is a callable or string (the function of the same name in the Spider will be called). This function will be called every time a link is obtained from link_extractor. The callback function receives a response as its first parameter and returns a list containing Item and Request objects (or subclasses of both).\n",
    "- cb_kwargs: A dictionary containing the keyword arguments passed to the callback function.\n",
    "- follow: is a boolean value that specifies whether the link extracted from the response according to this rule needs to be followed. If callback is None, follow defaults to True, otherwise defaults to False.\n",
    "- process_links: is a callable or string (the function of the same name in the Spider will be called). This function will be called when the link list is obtained from link_extrator. This method is mainly used for filtering.\n",
    "- process_request: is a callable or string (all functions with the same name in the spider will be called). This function will be called for every request extracted by this rule. The function must return a request or None. Used to filter requests.\n",
    "- errback: is a callable or a string that is called if any exception is raised.\n",
    "\n",
    "```python\n",
    "from scrapy.spiders.crawl import Rule, CrawlSpider\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class DoubanSpider(CrawlSpider):\n",
    "    name = \"test\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = ['https://example.com/uselinks']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=('subject/\\d+/$',)),\n",
    "        callback='parse_items'),\n",
    "    \t)\n",
    "\n",
    "    def parse_items(self, response):\n",
    "        # Yield items or return items\n",
    "        pass\n",
    "```\n",
    "\n",
    "- Scrapy requests start_urls and gets the response\n",
    "- Use the allow content in LinkExtractors to match the response and get the URL\n",
    "- Request this URL, give the response, and handle the function pointed to by the callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy template: A Recursive Text Spider <a id='recursive'> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "import tldextract\n",
    "import csv\n",
    "from bs4 import BeautifulSoup # BS reads and parses even poorly/unreliably coded HTML \n",
    "from bs4.element import Comment # helps with detecting inline/junk tags when parsing with BS\n",
    "import lxml # fast bs4 parser\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import Rule, CrawlSpider\n",
    "from scrapy.exceptions import NotSupported\n",
    "\n",
    "# The following are required for parsing File text\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile\n",
    "import textract\n",
    "from itertools import chain\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inline tags for cleaning out HTML\n",
    "inline_tags = [\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\", \"kbd\", \n",
    "               \"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\", \"span\", \"sub\", \"sup\", \"head\", \n",
    "               \"title\", \"[document]\", \"script\", \"style\", \"meta\", \"noscript\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveTextSpider(CrawlSpider):\n",
    "    name = 'textspider'\n",
    "    rules = [\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                canonicalize=False,\n",
    "                unique=True\n",
    "            ),\n",
    "            follow=True,\n",
    "            callback=\"parse_items\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    def __init__(self, domain_list=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Overrides default constructor to set custom\n",
    "        instance attributes.\n",
    "        \n",
    "        Parameters:\n",
    "        - domain_list: csv or tsv format\n",
    "            List of entities containing string domains and unique identifiers.\n",
    "            \n",
    "        Attributes:\n",
    "        \n",
    "        - start_urls:\n",
    "            Used by scrapy.spiders.Spider. A list of URLs where the\n",
    "            spider will begin to crawl.\n",
    "\n",
    "        - allowed_domains:\n",
    "            Used by scrapy.spiders.Spider. An optional list of\n",
    "            strings containing domains that this spider is allowed\n",
    "            to crawl.\n",
    "\n",
    "        - domain_to_id:\n",
    "            A custom attribute used to map a string domain to\n",
    "            a number representing the unique id defined by\n",
    "            csv_input.\n",
    "        \"\"\"\n",
    "        super(RecursiveTextSpider, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = []\n",
    "        self.allowed_domains = []\n",
    "        self.rules = (Rule(CustomLinkExtractor(allow_domains = self.allowed_domains), follow=True, callback=\"parse_items\"),)\n",
    "        self.domain_to_id = {}\n",
    "        self.init_from_domain_list(domain_list)\n",
    "    \n",
    "    \n",
    "    # note: make sure we ignore robot.txt\n",
    "    # Method for parsing items\n",
    "    def parse_items(self, response):\n",
    "        \n",
    "        item = CharterItem()\n",
    "        item['url'] = response.url\n",
    "        item['text'] = self.get_text(response)\n",
    "        domain = self.get_domain(response.url)    \n",
    "\n",
    "        item['unique_id'] = self.domain_to_id[domain]\n",
    "        item['depth'] = response.request.meta['depth'] # uses DepthMiddleware\n",
    "        print(\"Depth: \", item['depth'])\n",
    "\n",
    "        yield item    \n",
    "        \n",
    "        \n",
    "    def init_from_domain_list(self, domain_list):\n",
    "        \"\"\"\n",
    "        Generate's this spider's instance attributes\n",
    "        from the input domain list, formatted as a CSV or TSV.\n",
    "        \n",
    "        Domain List's format:\n",
    "        1. The first row is meta data that is ignored.\n",
    "        2. Rows in the csv are 1d arrays with one element.\n",
    "        ex: row == ['3.70014E+11,http://www.charlottesecondary.org/'].\n",
    "        \n",
    "        Note: start_requests() isn't used since it doesn't work\n",
    "        well with CrawlSpider Rules.\n",
    "        \n",
    "        Args:\n",
    "            domain_list: Is the path string to this file.\n",
    "        Returns:\n",
    "            Nothing is returned. However, start_urls,\n",
    "            allowed_domains, and domain_to_id are initialized.\n",
    "        \"\"\"\n",
    "        if not domain_list:\n",
    "            return\n",
    "        with open(domain_list, 'r') as f:\n",
    "            delim = \",\" if \"csv\" in domain_list else \"\\t\"\n",
    "            reader = csv.reader(f, delimiter=delim, quoting=csv.QUOTE_NONE)\n",
    "            first_row = True\n",
    "            for raw_row in reader:\n",
    "                if first_row:\n",
    "                    first_row = False\n",
    "                    continue\n",
    "                \n",
    "                unique_id, url = raw_row\n",
    "\n",
    "                domain = self.get_domain(url, True)\n",
    "                # set instance attributes\n",
    "                self.start_urls.append(url)\n",
    "                self.allowed_domains.append(domain)\n",
    "                # note: float('3.70014E+11') == 370014000000.0\n",
    "                self.domain_to_id[domain] = float(unique_id)\n",
    "\n",
    "    \n",
    "    def get_domain(self, url, init = False):\n",
    "        \"\"\"\n",
    "        Given the url, gets the top level domain using the\n",
    "        tldextract library.\n",
    "        \n",
    "        Args:\n",
    "            init (Boolean): True if this function is called while initializing the Spider, else False\n",
    "        Ex:\n",
    "        >>> get_domain('http://www.charlottesecondary.org/')\n",
    "        charlottesecondary.org\n",
    "        >>> get_domain('https://www.socratesacademy.us/our-school')\n",
    "        socratesacademy.us\n",
    "        \"\"\"\n",
    "        extracted = tldextract.extract(url)\n",
    "        permissive_domain = f'{extracted.domain}.{extracted.suffix}' # gets top level domain: very permissive crawling\n",
    "        #specific_domain = re.sub(r'https?\\:\\/\\/', '', url) # full URL without http\n",
    "        specific_domain = re.sub(r'https?\\:\\/\\/w{0,3}\\.?', '', url) # full URL without http and www. to compare w/ permissive\n",
    "        print(\"Permissive:\", permissive_domain)\n",
    "        print(\"Specific:\", specific_domain)\n",
    "        top_level = len(specific_domain.replace(\"/\", \"\")) == len(permissive_domain) # compare specific and permissive domain\n",
    "        \n",
    "        if init: # Check if this is the initialization period for the Spider.\n",
    "            if top_level:\n",
    "                return permissive_domain\n",
    "            else:\n",
    "                return specific_domain\n",
    "        \n",
    "        # secondary round\n",
    "        if permissive_domain in self.allowed_domains:\n",
    "            return permissive_domain\n",
    "        \n",
    "        #implement dictionary for if specific domain is used in original allowed_domains; key is specific_domain?\n",
    "        \n",
    "    \n",
    "    def get_text(self, response):\n",
    "        \"\"\"\n",
    "        Gets the readable text from a website's body and filters it.\n",
    "        Ex:\n",
    "        if response.body == \"\\u00a0OUR \\tSCHOOL\\t\\t\\tPARENTSACADEMICSSUPPORT \\u200b\\u200bOur Mission\"\n",
    "        >>> get_text(response)\n",
    "        'OUR SCHOOL PARENTSACADEMICSSUPPORT Our Mission'\n",
    "        \n",
    "        For another example, see filter_text_ex.txt\n",
    "        \n",
    "        More options for cleaning HTML: \n",
    "        https://stackoverflow.com/questions/699468/remove-html-tags-not-on-an-allowed-list-from-a-python-string/812785#812785\n",
    "        Especially consider: `from lxml.html.clean import clean_html`\n",
    "        \"\"\"\n",
    "        # Load HTML into BeautifulSoup, extract text\n",
    "        soup = BeautifulSoup(response.body, 'html5lib') # slower but more accurate parser for messy HTML # lxml faster\n",
    "        # Remove non-visible tags from soup\n",
    "        [s.decompose() for s in soup(inline_tags)] # quick method for BS\n",
    "        # Extract text, remove <p> tags\n",
    "        visible_text = soup.get_text(strip = False) # get text from each chunk, leave unicode spacing (e.g., `\\xa0`) for now to avoid globbing words\n",
    "        \n",
    "        # Remove ascii (such as \"\\u00\")\n",
    "        filtered_text = visible_text.encode('ascii', 'ignore').decode('ascii')\n",
    "        \n",
    "        # Remove ad junk\n",
    "        filtered_text = re.sub(r'\\b\\S*pic.twitter.com\\/\\S*', '', filtered_text) \n",
    "        filtered_text = re.sub(r'\\b\\S*cnxps\\.cmd\\.push\\(.+\\)\\;', '', filtered_text) \n",
    "        # Replace all consecutive spaces (including in unicode), tabs, or \"|\"s with a single space\n",
    "        filtered_text = regex.sub(r\"[ \\t\\h\\|]+\", \" \", filtered_text)\n",
    "        # Replace any consecutive linebreaks with a single newline\n",
    "        filtered_text = regex.sub(r\"[\\n\\r\\f\\v]+\", \"\\n\", filtered_text)\n",
    "        # Remove json strings: https://stackoverflow.com/questions/21994677/find-json-strings-in-a-string\n",
    "        filtered_text = regex.sub(r\"{(?:[^{}]*|(?R))*}\", \" \", filtered_text)\n",
    "\n",
    "        # Remove white spaces at beginning and end of string; return\n",
    "        return filtered_text.strip()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
