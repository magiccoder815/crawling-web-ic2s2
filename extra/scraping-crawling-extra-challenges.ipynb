{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and crawling the web: Extra challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges: Making requests\n",
    "\n",
    "1. Write a function called `get_html` that takes a URL as an argument and returns the HTML contents as a string. Test your function on the page for [Sir Tim Berners-Lee](https://en.wikipedia.org/wiki/Tim_Berners-Lee).\n",
    "2. What happens if the request doesn't go so smoothly? Add a defensive measure to your function to check that the response recieved was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Parsing HTML\n",
    "\n",
    "Scrape a clean claim review explanation in [this claim review by FactCheck.org](https://www.factcheck.org/2019/10/trumps-claims-about-hunter-biden-in-china/). As usual, use your browser to inspect this website's HTML and identify any unique types and/or classes that enclose the explanation (and nothing else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Challenges: Extended parsing case study\n",
    "\n",
    "Imagine you're in the field of education, in fact your specialty is studying higher education institutions. You're wondering how different disciplines change over time. Is it true that disciplines are incorporating more computational techniques as the years go on? Is that true for all disciplines or only some? Can we spot emerging themes across a whole university?\n",
    "\n",
    "To answer these questions, we're going to need data. We're going to collect a dataset of all courses registered at UC Berkeley, not just those being taught this semester but all courses currently approved to be taught. These are listed on [this page](http://guide.berkeley.edu/courses/), called the Academic Guide. Well, actually they're not directly listed on that page. That page lists the departments/programs/units that teach currently approved courses. If we click on each department (for the sake of brevity, I'm just going to call them all \"departments\"), we can see the list of all courses they're approved to teach. For example, [here's](http://guide.berkeley.edu/courses/aerospc/) the page for Aerospace Studies. We'll call these pages departmental pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "View the source HTML of [the page listing all departments](http://guide.berkeley.edu/courses/), and see if you can find the part of the HTML where the departments are listed. There's a lot of other stuff in the file that we don't care too much about. You could try `Crtl-F`ing for the name of a department you can see on the webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Look at HTML source of [the page for the Aerospace Studies department](http://guide.berkeley.edu/courses/aerospc/), and try to find the part of the file where the information on each course is. Again, try searching for it using `Crtl-F`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Get the HTML content of `http://guide.berkeley.edu/courses/` and store it in a variable called `academic_guide_html`. You can use the `get_html` function you wrote before.\n",
    "\n",
    "Print the first 500 characters to see what we got back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said before that all the departments were listed on the Academic Guide page with links to their departmental page, where the actual courses are listed. So we can find all the departments by looking in our parsed HTML for all the links. Remember that the links are represented in the HTML with the `<a>...</a>` tag, so we ask our `academic_guide_soup` to find us all the tags called `a`. What we get back is a list of all the `a` elements in the HTML page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "academic_guide_soup = BeautifulSoup(academic_guide_html, 'lxml')\n",
    "\n",
    "links = academic_guide_soup.find_all('a')\n",
    "# print a random link element\n",
    "links[48]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a list of `a` elements, each one represents a link on the Academic Guide page. But there are other links on this page in addition to the ones we care about, for example, a link back to the UC Berkeley home page. How can we filter out all the links we don't care about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Look through the list `links`, or the HTML source, and figure out how we can identify just the links that we care about, namely the links to departmental pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our new `is_departmental_page` function to filter out the links we don't care about. How many departments do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departmental_page_links = [link for link in links if is_departmental_page(link)]\n",
    "len(departmental_page_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each link in our `departmental_page_links` list contains a HTML element representing a link. Each element contains not only the relative location of the link but also the text that is linked (i.e. the words on the page that are underlined and you can click on to go to the linked page). In BeautifulSoup, we can get that text by asking for it with `element.text`, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departmental_page_links[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "From the `departmental_page_links`, we can extract out the name and the code for each department. Try doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each link in our `departmental_page_links` list, we can get the relative link that it points to like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departmental_page_links[0].attrs['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Write a function that extracts out the relative link of a link element.\n",
    "\n",
    "*Hint: This has a similar solution to our `is_departmental_page` function from before.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right! Now we've identified all the departmental links on the Academic Guide page, we've found their name and code, and we know the relative link they point to. Next, we can use this relative link to construct the full URL they point to, which we'll then use to scrape the HTML for each departmental page.\n",
    "\n",
    "Let's write a function that takes a departmental link and returns the absolute URL of its departmental page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_absolute_url(departmental_link):\n",
    "    relative_link = extract_relative_link(departmental_link)\n",
    "    return academic_guide_url + relative_link\n",
    "\n",
    "construct_absolute_url(departmental_page_links[37])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize so far, we've gone from the URL of the Academic Guide website, found all the departments that offer approved courses, identified their name and code and the link to their departmental page which lists all the courses they teach. \n",
    "\n",
    "Now we want to find the get the HTML for each departmental page and scrape it for all the courses they offer. Let's focus on one page for now, the Aerospace Studies page. Once we select the link, we use our functions from above to: i) get the name (I guess we already know it's Aerospace, but whatever) and code, get the full URL, get the HTML for that URL and then parse the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aerospace_link = departmental_page_links[0]\n",
    "aerospace_name, aerospace_code = extract_department_name_and_code(aerospace_link)\n",
    "aerospace_url = construct_absolute_url(aerospace_link)\n",
    "aerospace_html = get_html(aerospace_url)\n",
    "aerospace_soup = BeautifulSoup(aerospace_html, 'lxml')\n",
    "print(aerospace_html[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right at the start of this section on parsing HTML, we saw the HTML for a departmental page. Here it is again.\n",
    "\n",
    "```\n",
    "<div class=\"courseblock\">\n",
    "\n",
    "<button class=\"btn_toggleCoursebody\" aria-expanded=\"false\" aria-controls=\"cb_aerospc1a\" data-toggle=\"#cb_aerospc1a\">\n",
    "\n",
    "<a name=\"spanaerospc1aspanspanfoundationsoftheu.s.airforcespanspan1unitspan\"></a>\n",
    "<h3 class=\"courseblocktitle\">\n",
    "<span class=\"code\">AEROSPC 1A</span> \n",
    "<span class=\"title\">Foundations of the U.S. Air Force</span> \n",
    "<span class=\"hours\">1 Unit</span>\n",
    "</h3>\n",
    "```\n",
    "\n",
    "It looks like each course is listed in a `div` element that has a `class` attribute with value `\"courseblock\"`. We can use this information to identify all the courses on a page and then extract out the information from them. You've seen how to do this before, here it is again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aerospace_courseblocks = aerospace_soup.find_all(class_='courseblock')\n",
    "len(aerospace_courseblocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the Aerospace department has seven current courses they're approved to teach (at the time of writing). Looking at the page in our browser, that looks right to me! So now we have a list called `aerospace_courseblocks` that holds seven elements that each refer to one course taught by the Aerospace department. Now we can extract out any information we care about. We just have to look at the page in our browser, decide what information we care about, then look at the HTML source to see where that information is kept in the HTML structure. Finally, we write a function for each piece of information we want to extract out of a course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Write functions to take a courseblock and extract:\n",
    "- The course code (e.g. AEROSPC 1A)\n",
    "- The coure name\n",
    "- The number of units\n",
    "- The textual description of the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to scrape these four pieces of information from every course from every department and save it as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_one_department(department_link):\n",
    "    department_name, department_code = extract_department_name_and_code(department_link)\n",
    "    department_url = construct_absolute_url(department_link)\n",
    "    department_html = get_html(department_url)\n",
    "    department_soup = BeautifulSoup(department_html, 'lxml')\n",
    "    department_courseblocks = department_soup.find_all(class_='courseblock')\n",
    "    result = []\n",
    "    for courseblock in department_courseblocks:\n",
    "        course = extract_one_course(courseblock)\n",
    "        course['department_name'] = department_name\n",
    "        course['department_code'] = department_code\n",
    "        result.append(course)\n",
    "    return result\n",
    "\n",
    "aerospace_courses = scrape_one_department(aerospace_link)\n",
    "for value in aerospace_courses[0].values():\n",
    "    print(value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def scrape_all_departments(be_nice=True):\n",
    "    academic_guide_url = 'http://guide.berkeley.edu/courses/'\n",
    "    academic_guide_html = get_html(academic_guide_url)\n",
    "    academic_guide_soup = BeautifulSoup(academic_guide_html, 'lxml')\n",
    "    links = academic_guide_soup.find_all('a')\n",
    "    departmental_page_links = [link for link in links if is_departmental_page(link)]\n",
    "    \n",
    "    result = []\n",
    "    for departmental_page_link in departmental_page_links:\n",
    "        department_result = scrape_one_department(departmental_page_link)\n",
    "        result.extend(department_result)\n",
    "        if be_nice:\n",
    "            time.sleep(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = scrape_all_departments(be_nice=False)\n",
    "df = pd.DataFrame(result)\n",
    "print(str(len(df)) + ' courses scraped')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9842 courses scraped! (At the time of writing). Wow, that was a lot easier than doing it by hand!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
